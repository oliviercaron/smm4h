preprocess_text <- function(df) {
df %>%
mutate(text_clean = text) %>%
# Convert to lowercase
mutate(text_clean = str_to_lower(text_clean)) %>%
# Remove user mentions
mutate(text_clean = str_remove_all(text_clean, "@\\w+")) %>%
# Remove HTML-like tags and <user> placeholders
mutate(text_clean = str_remove_all(text_clean, "<.*?>")) %>%
# Replace unicode sequences (like emojis)
mutate(text_clean = replace_non_ascii(text_clean)) %>%
# Remove punctuation
mutate(text_clean = str_remove_all(text_clean, "[[:punct:]]")) %>%
# Remove numbers
mutate(text_clean = str_remove_all(text_clean, "\\d+")) %>%
# Remove extra whitespace
mutate(text_clean = str_squish(text_clean)) %>%
# Optionally: remove stopwords (language-specific, handled later)
mutate(text_tokens = str_split(text_clean, "\\s+"))
}
# Application sur tes données
dev_data_cleaned <- preprocess_text(dev_data_resultats)
# ------------------------------------------------------------
# 1. Revised Prompt for Multilingual ADE Detection (Task 1)
# ------------------------------------------------------------
prompt_final_task1_multilingual <- "
Read the following social media post. If it mentions a harmful or unwanted effect clearly caused by a medication, respond with '1'. Otherwise, respond with '0'. Only return one digit: '1' or '0'.
prompt_final_task1_multilingual <- "
Read the following social media post. If it mentions a harmful or unwanted effect clearly caused by a medication, respond with '1'. Otherwise, respond with '0'. Only return one digit: '1' or '0'.
prompt_final_task1_multilingual <- "
Read the following social media post. If it mentions a harmful or unwanted effect clearly caused by a medication, respond with '1'. Otherwise, respond with '0'. Only return one digit: '1' or '0'.
Examples of '1':
- 'I felt dizzy after taking the drug.'
- 'My child became aggressive after using the medicine.'
Examples of '0':
- 'I started a new treatment today.'
- 'Does this drug have side effects?'
Now classify this post:
"
dev_data_sample <- sample_n(dev_data, 500) # Sample 5 rows for illustration
dev_data_sample <- preprocess_text(dev_data_sample)
resultats_task1 <- dev_data_sample |>
llm_custom(text, prompt = prompt_final_task1_multilingual)
View(resultats_task1)
library(purrr) # Optionnel mais pratique pour la manipulation de listes (tokens)
preprocess_text <- function(df, text_col = "text") {
# Vérifier si la colonne de texte existe
if (!text_col %in% names(df)) {
stop(paste("La colonne spécifiée '", text_col, "' n'existe pas dans le dataframe."))
}
df %>%
# Créer une nouvelle colonne pour le nettoyage, en gardant l'originale
# Utilise !!sym() pour que la fonction accepte le nom de colonne comme argument
mutate(text_clean = !!sym(text_col)) %>%
# 1. Supprimer les mentions utilisateur (ex: @utilisateur)
mutate(text_clean = str_remove_all(text_clean, "@\\w+")) %>%
# 2. Supprimer les URLs
mutate(text_clean = str_remove_all(text_clean, "http[s]?://\\S+")) %>% # URLs http/https
mutate(text_clean = str_remove_all(text_clean, "www\\.\\S+")) %>%      # URLs commençant par www.
# 3. Supprimer les balises type HTML et placeholders (ex: <tag>, <user>)
mutate(text_clean = str_remove_all(text_clean, "<.*?>")) %>%
# 4. Convertir en minuscules
# C'est souvent mieux après avoir supprimé les éléments sensibles à la casse comme les URLs/mentions,
# mais ici l'ordre est acceptable.
mutate(text_clean = str_to_lower(text_clean)) %>%
# 5. Remplacer les caractères non-ASCII (comme les emojis, certains accents si non désirés) par un espace
# Remplacer par un espace évite de fusionner des mots après suppression.
# Alternative: utiliser iconv pour translittérer (ex: é -> e) si vous voulez garder les accents de base
# mutate(text_clean = iconv(text_clean, from = "UTF-8", to = "ASCII//TRANSLIT"))
mutate(text_clean = str_replace_all(text_clean, "[^\\x00-\\x7F]", " ")) %>% # Remplace par espace
# 6. Supprimer la ponctuation
# Attention: ceci supprime aussi les apostrophes (ex: "l'arbre" -> "larbre") et les traits d'union.
# Adaptez le pattern si vous voulez conserver certains signes.
mutate(text_clean = str_remove_all(text_clean, "[[:punct:]]")) %>%
# 7. Supprimer les nombres
mutate(text_clean = str_remove_all(text_clean, "\\d+")) %>%
# 8. Supprimer les espaces superflus (début/fin, multiples espaces internes réduits à un seul)
# Étape cruciale après de multiples suppressions/remplacements.
mutate(text_clean = str_squish(text_clean)) %>%
# 9. Tokeniser le texte nettoyé (séparer en mots) dans une colonne de type liste
# Sépare sur la base d'un ou plusieurs espaces.
# Filtre les chaînes vides qui pourraient résulter de la séparation si le texte devient vide.
mutate(
text_tokens = str_split(text_clean, "\\s+"),
# Utilise purrr::map pour appliquer une fonction à chaque élément de la liste de tokens
text_tokens = map(text_tokens, ~ .x[.x != ""])
# Alternative sans purrr:
# text_tokens = lapply(text_tokens, function(tokens) tokens[tokens != ""])
)
# --- Optionnel: Suppression des mots vides (stopwords) ---
# À faire typiquement sur la colonne 'text_tokens'
# Nécessite une liste de stopwords pour la langue cible (ex: français)
# Exemple avec le package 'tm' (à installer : install.packages("tm"))
# library(tm)
# french_stopwords <- stopwords("french")
# mutate(text_tokens = map(text_tokens, ~ .x[! .x %in% french_stopwords]))
}
# Application sur tes données
dev_data_cleaned <- preprocess_text(dev_data_resultats)
View(dev_data_cleaned)
# Fonction de preprocessing améliorée
# Fonction de preprocessing MISE À JOUR (conserve les caractères non-ASCII)
preprocess_text_multilingual <- function(df, text_col = "text") {
# Vérifier si la colonne de texte existe
if (!text_col %in% names(df)) {
stop(paste("La colonne spécifiée '", text_col, "' n'existe pas dans le dataframe."))
}
df %>%
mutate(text_clean = !!sym(text_col)) %>%
# 1. Supprimer les mentions utilisateur (ex: @utilisateur)
mutate(text_clean = str_remove_all(text_clean, "@\\w+")) %>%
# 2. Supprimer les URLs
mutate(text_clean = str_remove_all(text_clean, "http[s]?://\\S+")) %>%
mutate(text_clean = str_remove_all(text_clean, "www\\.\\S+")) %>%
# 3. Supprimer les balises type HTML et placeholders (ex: <tag>, <user>, <pi>, <doctor>)
# Rend le pattern un peu plus générique pour les placeholders courants
mutate(text_clean = str_remove_all(text_clean, "<[/]?\\w+>")) %>% # <tag>, </tag>, <user>, etc.
# Alternative plus large si vous avez des placeholders non standards :
# mutate(text_clean = str_remove_all(text_clean, "<.*?>")) %>%
# 4. Convertir en minuscules
# Fonctionne généralement bien pour Latin, Cyrillique, Grec, etc.
mutate(text_clean = str_to_lower(text_clean)) %>%
# --- ÉTAPE SUPPRIMÉE ---
# 5. L'étape qui supprimait les caractères non-ASCII a été retirée
# pour conserver les alphabets comme le cyrillique, les accents, etc.
# Si vous voulez spécifiquement supprimer les EMOJIS, c'est plus complexe
# et nécessiterait une regex ciblant les plages Unicode des emojis.
# 6. Supprimer la ponctuation
# ATTENTION : '[[:punct:]]' peut avoir un comportement différent selon
# la locale système et peut supprimer des caractères utiles dans certaines langues.
# Vous pourriez avoir besoin d'une liste de ponctuations plus ciblée si nécessaire.
# Par exemple, pour garder les traits d'union internes : str_replace_all(text_clean, "[^\\w\\s-]|(?<!\\w)-|-\\B", " ")
mutate(text_clean = str_remove_all(text_clean, "[[:punct:]]")) %>%
# 7. Supprimer les nombres
mutate(text_clean = str_remove_all(text_clean, "\\d+")) %>%
# 8. Supprimer les espaces superflus
mutate(text_clean = str_squish(text_clean)) %>%
# 9. Tokeniser le texte nettoyé
mutate(
text_tokens = str_split(text_clean, "\\s+"),
text_tokens = map(text_tokens, ~ .x[.x != ""])
)
# --- Optionnel: Suppression des mots vides (stopwords) ---
# !! IMPORTANT POUR LE MULTILINGUE !!
# Cette étape DOIT être adaptée à la langue de CHAQUE texte.
# Vous auriez besoin de détecter la langue (ex: avec cld2 ou cld3)
# puis d'appliquer la liste de stopwords appropriée.
# L'exemple ci-dessous ne fonctionnerait QUE pour le français.
# library(tm)
# # Exemple pour une seule langue (ex: français)
# # french_stopwords <- stopwords("french")
# # mutate(text_tokens = map(text_tokens, ~ .x[! .x %in% french_stopwords]))
#
# # Approche multilingue simplifiée (si vous avez une colonne 'language'):
# # stopwords_list <- list(
# #   en = stopwords("en"),
# #   fr = stopwords("fr"),
# #   de = stopwords("de"),
# #   ru = stopwords("ru") # nécessite un package ou une liste pour le russe
# # )
# # mutate(text_tokens = map2(text_tokens, language,
# #                           ~ .x[! .x %in% stopwords_list[[.y]] ] ))
}
# Application sur tes données
dev_data_cleaned <- preprocess_text(dev_data_resultats)
# ------------------------------------------------------------
# 0. Configure mall (Adapt to your exact model name)
# ------------------------------------------------------------
# Ensure Ollama is running and the model 'gemma3' (or the correct name) is available
# Using 'gemma3' as you did, but double-check with 'ollama list'
# Low temperature is preferred for strict instruction following.
llm_use(backend = "ollama", model = "gemma3", temperature = 0.1) # Seed might be useful for reproducibility
# ------------------------------------------------------------
# 1. Revised Prompt for Multilingual ADE Detection (Task 1)
# ------------------------------------------------------------
prompt_final_task1_multilingual <- "
Read the following social media post. If it mentions a negative medical side effect (adverse drug events), respond with '1', if it mentions something close but not related to drug or medicines respond with '0'  Only return one digit: '1' or '0'.
Examples of '1':
- 'I felt dizzy after taking the drug.'
- 'My child became aggressive after using the medicine.'
Examples of '0':
- 'I started a new treatment today.'
- 'Does this drug have side effects?'
Now classify this post:
"
dev_data_sample <- sample_n(dev_data, 500) # Sample 5 rows for illustration
dev_data_sample <- preprocess_text(dev_data_sample)
# ------------------------------------------------------------
# 2. Apply the prompt to the dev_data dataframe
# ------------------------------------------------------------
resultats_task1 <- dev_data_sample |>
llm_custom(text, prompt = prompt_final_task1_multilingual)
dev_data_resultats <- resultats_task1 |>
mutate(.pred = str_trim(.pred)) |>
mutate(pred_label = if_else(.pred %in% c("0", "1"), .pred, NA_character_)) |>
select(-.pred)  # Optional: remove intermediate column
# ------------------------------------------------------------
# 3. Display results
# ------------------------------------------------------------
print("Cleaned predictions with 'pred_label' column:")
print(dev_data_resultats)
#test_translate <- llm_translate(dev_data_resultats, text, "french")
```
#test_translate <- llm_translate(dev_data_resultats, text, "french")
table(dev_data_resultats$pred_label)
table(dev_data_resultats$label)
# ------------------------------------------------------------
# 0. Configure mall (Adapt to your exact model name)
# ------------------------------------------------------------
# Ensure Ollama is running and the model 'gemma3' (or the correct name) is available
# Using 'gemma3' as you did, but double-check with 'ollama list'
# Low temperature is preferred for strict instruction following.
llm_use(backend = "ollama", model = "gemma3", temperature = 0.1) # Seed might be useful for reproducibility
# ------------------------------------------------------------
# 1. Revised Prompt for Multilingual ADE Detection (Task 1)
# ------------------------------------------------------------
prompt_final_task1_multilingual <- "
You are given a single social media post written in English, German, French, or Russian.
Your task is to classify the post as either:
- '1' if it mentions at least one Adverse Drug Event (ADE) — a negative medical side effect clearly associated with a drug.
- '0' if it does not mention any ADE.
Posts may include informal language, sarcasm, or ambiguity. Be conservative: only assign '1' when the ADE mention is clear and drug-related.
Output only the number: '1' or '0'. No explanation.
Examples:
German:
- \"ich hatte Müdigkeit als Auswirkung\" → 1
- \"Das heisst ja nicht, dass es bei dir genau so sein wird\" → 0
French:
- \"troubles de la vue, hypertension, saignements\" (caused by hormones) → 1
- \"je ne vais que très rarement chez eux [...] autopromotion\" → 0
Russian:
- \"у ребенка сильно изменилось поведение [...] отменили препарат\" → 1
- \"назначили Тонзилгон [...] горло почти зажило!\" → 0
English:
- \"trazodone [...] means dopey all day\" → 1
- \"Exelon stock [...] buy, sell or hold?\" → 0
Now classify this post:"
dev_data_sample <- sample_n(dev_data, 500) # Sample 5 rows for illustration
dev_data_sample <- preprocess_text(dev_data_sample)
# ------------------------------------------------------------
# 2. Apply the prompt to the dev_data dataframe
# ------------------------------------------------------------
resultats_task1 <- dev_data_sample |>
llm_custom(text, prompt = prompt_final_task1_multilingual)
dev_data_resultats <- resultats_task1 |>
mutate(.pred = str_trim(.pred)) |>
mutate(pred_label = if_else(.pred %in% c("0", "1"), .pred, NA_character_)) |>
select(-.pred)  # Optional: remove intermediate column
# ------------------------------------------------------------
# 3. Display results
# ------------------------------------------------------------
print("Cleaned predictions with 'pred_label' column:")
print(dev_data_resultats)
#test_translate <- llm_translate(dev_data_resultats, text, "french")
table(dev_data_resultats$pred_label)
table(dev_data_resultats$label)
library(caret)
evaluate_multilingual <- function(dev_data_results) {
languages <- unique(dev_data_results$language)
metrics <- data.frame()
for (lang in languages) {
lang_data <- dev_data_results %>% filter(language == lang)
true <- lang_data$label
pred <- lang_data$pred_label
conf <- confusionMatrix(factor(pred), factor(true), positive = "1")
precision <- conf$byClass["Precision"]
recall <- conf$byClass["Recall"]
f1 <- conf$byClass["F1"]
accuracy <- conf$overall["Accuracy"]
cat(sprintf("\nMetrics for language: %s\n", toupper(lang)))
cat(sprintf("  Precision-%s (Pos): %.4f\n", tolower(lang), precision))
cat(sprintf("  Recall-%s    (Pos): %.4f\n", tolower(lang), recall))
cat(sprintf("  F1-%s        (Pos): %.4f\n", tolower(lang), f1))
cat(sprintf("  Accuracy-%s:        %.4f\n", tolower(lang), accuracy))
metrics <- rbind(metrics, data.frame(
language = lang,
precision = precision,
recall = recall,
f1 = f1,
accuracy = accuracy
))
}
# Overall metrics (macro & micro averages)
overall_precision <- mean(metrics$precision, na.rm = TRUE)
overall_recall <- mean(metrics$recall, na.rm = TRUE)
overall_f1_macro <- mean(metrics$f1, na.rm = TRUE)
# Micro-averaged F1 (global across all data)
all_true <- dev_data_results$label
all_pred <- dev_data_results$pred_label
conf_all <- confusionMatrix(factor(all_pred), factor(all_true), positive = "1")
f1_micro <- conf_all$byClass["F1"]
acc_all <- conf_all$overall["Accuracy"]
cat("\n--- Overall Evaluation Summary (Positive Class Focus) ---\n")
cat(sprintf("F1-score across all languages (Positive Class): %.4f  <-- Primary Metric\n", f1_micro))
cat(sprintf("Macro F1-score across all languages (Pos Class):%.4f\n", overall_f1_macro))
cat(sprintf("Overall Precision (Positive Class):             %.4f\n", overall_precision))
cat(sprintf("Overall Recall (Positive Class):                %.4f\n", overall_recall))
cat(sprintf("Overall Accuracy across all languages:          %.4f\n", acc_all))
}
evaluate_multilingual <- function(dev_data_results) {
languages <- unique(dev_data_results$language)
metrics <- data.frame()
for (lang in languages) {
lang_data <- dev_data_results %>% filter(language == lang)
true <- lang_data$label
pred <- lang_data$pred_label
conf <- confusionMatrix(factor(pred), factor(true), positive = "1")
precision <- conf$byClass["Precision"]
recall <- conf$byClass["Recall"]
f1 <- conf$byClass["F1"]
accuracy <- conf$overall["Accuracy"]
cat(sprintf("\nMetrics for language: %s\n", toupper(lang)))
cat(sprintf("  Precision-%s (Pos): %.4f\n", tolower(lang), precision))
cat(sprintf("  Recall-%s    (Pos): %.4f\n", tolower(lang), recall))
cat(sprintf("  F1-%s        (Pos): %.4f\n", tolower(lang), f1))
cat(sprintf("  Accuracy-%s:        %.4f\n", tolower(lang), accuracy))
metrics <- rbind(metrics, data.frame(
language = lang,
precision = precision,
recall = recall,
f1 = f1,
accuracy = accuracy
))
}
# Overall metrics (macro & micro averages)
overall_precision <- mean(metrics$precision, na.rm = TRUE)
overall_recall <- mean(metrics$recall, na.rm = TRUE)
overall_f1_macro <- mean(metrics$f1, na.rm = TRUE)
# Micro-averaged F1 (global across all data)
all_true <- dev_data_results$label
all_pred <- dev_data_results$pred_label
conf_all <- confusionMatrix(factor(all_pred), factor(all_true), positive = "1")
f1_micro <- conf_all$byClass["F1"]
acc_all <- conf_all$overall["Accuracy"]
cat("\n--- Overall Evaluation Summary (Positive Class Focus) ---\n")
cat(sprintf("F1-score across all languages (Positive Class): %.4f  <-- Primary Metric\n", f1_micro))
cat(sprintf("Macro F1-score across all languages (Pos Class):%.4f\n", overall_f1_macro))
cat(sprintf("Overall Precision (Positive Class):             %.4f\n", overall_precision))
cat(sprintf("Overall Recall (Positive Class):                %.4f\n", overall_recall))
cat(sprintf("Overall Accuracy across all languages:          %.4f\n", acc_all))
# Plot F1-scores
ggplot(metrics, aes(x = language, y = f1)) +
geom_col(fill = "steelblue") +
geom_text(aes(label = sprintf("%.3f", f1)), vjust = -0.3, size = 4.5) +
labs(
title = "F1-score (Positive Class) per Language",
x = "Language",
y = "F1-score"
) +
ylim(0, 1) +
theme_minimal()
}
}
evaluate_multilingual <- function(dev_data_results) {
languages <- unique(dev_data_results$language)
metrics <- data.frame()
for (lang in languages) {
lang_data <- dev_data_results %>% filter(language == lang)
true <- lang_data$label
pred <- lang_data$pred_label
conf <- confusionMatrix(factor(pred), factor(true), positive = "1")
precision <- conf$byClass["Precision"]
recall <- conf$byClass["Recall"]
f1 <- conf$byClass["F1"]
accuracy <- conf$overall["Accuracy"]
cat(sprintf("\nMetrics for language: %s\n", toupper(lang)))
cat(sprintf("  Precision-%s (Pos): %.4f\n", tolower(lang), precision))
cat(sprintf("  Recall-%s    (Pos): %.4f\n", tolower(lang), recall))
cat(sprintf("  F1-%s        (Pos): %.4f\n", tolower(lang), f1))
cat(sprintf("  Accuracy-%s:        %.4f\n", tolower(lang), accuracy))
metrics <- rbind(metrics, data.frame(
language = lang,
precision = precision,
recall = recall,
f1 = f1,
accuracy = accuracy
))
}
# Overall metrics (macro & micro averages)
overall_precision <- mean(metrics$precision, na.rm = TRUE)
overall_recall <- mean(metrics$recall, na.rm = TRUE)
overall_f1_macro <- mean(metrics$f1, na.rm = TRUE)
# Micro-averaged F1 (global across all data)
all_true <- dev_data_results$label
all_pred <- dev_data_results$pred_label
conf_all <- confusionMatrix(factor(all_pred), factor(all_true), positive = "1")
f1_micro <- conf_all$byClass["F1"]
acc_all <- conf_all$overall["Accuracy"]
cat("\n--- Overall Evaluation Summary (Positive Class Focus) ---\n")
cat(sprintf("F1-score across all languages (Positive Class): %.4f  <-- Primary Metric\n", f1_micro))
cat(sprintf("Macro F1-score across all languages (Pos Class):%.4f\n", overall_f1_macro))
cat(sprintf("Overall Precision (Positive Class):             %.4f\n", overall_precision))
cat(sprintf("Overall Recall (Positive Class):                %.4f\n", overall_recall))
cat(sprintf("Overall Accuracy across all languages:          %.4f\n", acc_all))
# Plot F1-scores
ggplot(metrics, aes(x = language, y = f1)) +
geom_col(fill = "steelblue") +
geom_text(aes(label = sprintf("%.3f", f1)), vjust = -0.3, size = 4.5) +
labs(
title = "F1-score (Positive Class) per Language",
x = "Language",
y = "F1-score"
) +
ylim(0, 1) +
theme_minimal()
}
evaluate_multilingual(dev_data_resultats)
# ------------------------------------------------------------
# 0. Configure mall (Adapt to your exact model name)
# ------------------------------------------------------------
# Ensure Ollama is running and the model 'gemma3' (or the correct name) is available
# Using 'gemma3' as you did, but double-check with 'ollama list'
# Low temperature is preferred for strict instruction following.
llm_use(backend = "ollama", model = "gemma3:12b", temperature = 0.1) # Seed might be useful for reproducibility
# ------------------------------------------------------------
# 1. Revised Prompt for Multilingual ADE Detection (Task 1)
# ------------------------------------------------------------
prompt_final_task1_multilingual <- "
You are given a single social media post written in English, German, French, or Russian.
Your task is to classify the post as either:
- '1' if it mentions at least one Adverse Drug Event (ADE) — a negative medical side effect clearly associated with a drug.
- '0' if it does not mention any ADE.
Posts may include informal language, sarcasm, or ambiguity. Be conservative: only assign '1' when the ADE mention is clear and drug-related.
Output only the number: '1' or '0'. No explanation.
Examples:
German:
- \"ich hatte Müdigkeit als Auswirkung\" → 1
- \"Das heisst ja nicht, dass es bei dir genau so sein wird\" → 0
French:
- \"troubles de la vue, hypertension, saignements\" (caused by hormones) → 1
- \"je ne vais que très rarement chez eux [...] autopromotion\" → 0
Russian:
- \"у ребенка сильно изменилось поведение [...] отменили препарат\" → 1
- \"назначили Тонзилгон [...] горло почти зажило!\" → 0
English:
- \"trazodone [...] means dopey all day\" → 1
- \"Exelon stock [...] buy, sell or hold?\" → 0
Now classify this post:"
dev_data_sample <- sample_n(dev_data, 500) # Sample 5 rows for illustration
dev_data_sample <- preprocess_text(dev_data_sample)
# ------------------------------------------------------------
# 2. Apply the prompt to the dev_data dataframe
# ------------------------------------------------------------
resultats_task1 <- dev_data_sample |>
llm_custom(text, prompt = prompt_final_task1_multilingual)
dev_data_resultats <- resultats_task1 |>
mutate(.pred = str_trim(.pred)) |>
mutate(pred_label = if_else(.pred %in% c("0", "1"), .pred, NA_character_)) |>
select(-.pred)  # Optional: remove intermediate column
# ------------------------------------------------------------
# 3. Display results
# ------------------------------------------------------------
print("Cleaned predictions with 'pred_label' column:")
print(dev_data_resultats)
#test_translate <- llm_translate(dev_data_resultats, text, "french")
table(dev_data_resultats$pred_label)
table(dev_data_resultats$label)
evaluate_multilingual <- function(dev_data_results) {
languages <- unique(dev_data_results$language)
metrics <- data.frame()
for (lang in languages) {
lang_data <- dev_data_results %>% filter(language == lang)
true <- lang_data$label
pred <- lang_data$pred_label
conf <- confusionMatrix(factor(pred), factor(true), positive = "1")
precision <- conf$byClass["Precision"]
recall <- conf$byClass["Recall"]
f1 <- conf$byClass["F1"]
accuracy <- conf$overall["Accuracy"]
cat(sprintf("\nMetrics for language: %s\n", toupper(lang)))
cat(sprintf("  Precision-%s (Pos): %.4f\n", tolower(lang), precision))
cat(sprintf("  Recall-%s    (Pos): %.4f\n", tolower(lang), recall))
cat(sprintf("  F1-%s        (Pos): %.4f\n", tolower(lang), f1))
cat(sprintf("  Accuracy-%s:        %.4f\n", tolower(lang), accuracy))
metrics <- rbind(metrics, data.frame(
language = lang,
precision = precision,
recall = recall,
f1 = f1,
accuracy = accuracy
))
}
# Overall metrics (macro & micro averages)
overall_precision <- mean(metrics$precision, na.rm = TRUE)
overall_recall <- mean(metrics$recall, na.rm = TRUE)
overall_f1_macro <- mean(metrics$f1, na.rm = TRUE)
# Micro-averaged F1 (global across all data)
all_true <- dev_data_results$label
all_pred <- dev_data_results$pred_label
conf_all <- confusionMatrix(factor(all_pred), factor(all_true), positive = "1")
f1_micro <- conf_all$byClass["F1"]
acc_all <- conf_all$overall["Accuracy"]
cat("\n--- Overall Evaluation Summary (Positive Class Focus) ---\n")
cat(sprintf("F1-score across all languages (Positive Class): %.4f  <-- Primary Metric\n", f1_micro))
cat(sprintf("Macro F1-score across all languages (Pos Class):%.4f\n", overall_f1_macro))
cat(sprintf("Overall Precision (Positive Class):             %.4f\n", overall_precision))
cat(sprintf("Overall Recall (Positive Class):                %.4f\n", overall_recall))
cat(sprintf("Overall Accuracy across all languages:          %.4f\n", acc_all))
# Plot F1-scores
ggplot(metrics, aes(x = language, y = f1)) +
geom_col(fill = "steelblue") +
geom_text(aes(label = sprintf("%.3f", f1)), vjust = -0.3, size = 4.5) +
labs(
title = "F1-score (Positive Class) per Language",
x = "Language",
y = "F1-score"
) +
ylim(0, 1) +
theme_minimal()
}
evaluate_multilingual(dev_data_resultats)
View(train_data)
