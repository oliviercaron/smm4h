
Évaluation finale:
{'eval_loss': 0.47089266777038574, 'eval_positive_precision': 0.5882352941176471, 'eval_positive_recall': 0.5714285714285714, 'eval_positive_f1': 0.5797101449275363, 'eval_TP': 20, 'eval_FP': 14, 'eval_FN': 15, 'eval_runtime': 8.3089, 'eval_samples_per_second': 76.304, 'eval_steps_per_second': 4.814, 'epoch': 7.972354623450905}

Recherche du seuil optimal pour maximiser le F1 de la classe positive...

Meilleur seuil trouvé: 0.40, F1 score pour la classe positive: 0.5797

Top 5 des meilleurs seuils:
Seuil: 0.50, Precision: 0.5882, Recall: 0.5714, F1: 0.5797
Seuil: 0.53, Precision: 0.5882, Recall: 0.5714, F1: 0.5797
Seuil: 0.62, Precision: 0.5882, Recall: 0.5714, F1: 0.5797
Seuil: 0.61, Precision: 0.5882, Recall: 0.5714, F1: 0.5797
Seuil: 0.60, Precision: 0.5882, Recall: 0.5714, F1: 0.5797

Modèle et seuil optimal sauvegardés dans de_gbert_large_positive_f1
Distribution des classes dans les données d'entraînement originales:
Classe positive: 153 (9.86%)
Classe négative: 1399 (90.14%)

Distribution des classes après rééquilibrage:
Classe positive: 699 (33.32%)
Classe négative: 1399 (66.68%)
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\Olivier\AppData\Local\Temp\ipykernel_13456\2030998234.py:170: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `PositiveF1Trainer.__init__`. Use `processing_class` instead.
  trainer = PositiveF1Trainer(

Évaluation finale:
{'eval_loss': 0.3046858608722687, 'eval_positive_precision': 0.7777777777777778, 'eval_positive_recall': 0.6, 'eval_positive_f1': 0.6774193548387097, 'eval_TP': 21, 'eval_FP': 6, 'eval_FN': 14, 'eval_runtime': 2.8653, 'eval_samples_per_second': 221.269, 'eval_steps_per_second': 13.96, 'epoch': 7.972354623450905}

Recherche du seuil optimal pour maximiser le F1 de la classe positive...

Meilleur seuil trouvé: 0.14, F1 score pour la classe positive: 0.7385

Top 5 des meilleurs seuils:
Seuil: 0.27, Precision: 0.8000, Recall: 0.6857, F1: 0.7385
Seuil: 0.15, Precision: 0.8000, Recall: 0.6857, F1: 0.7385
Seuil: 0.26, Precision: 0.8000, Recall: 0.6857, F1: 0.7385
Seuil: 0.25, Precision: 0.8000, Recall: 0.6857, F1: 0.7385
Seuil: 0.24, Precision: 0.8000, Recall: 0.6857, F1: 0.7385

Modèle et seuil optimal sauvegardés dans de_gbert_large_positive_f1
Checkpoint checkpoint-4870: F1 positif = 0.6289120715350224
Checkpoint checkpoint-4870: F1 positif = 0.6740331491712708
Checkpoint checkpoint-4870: F1 positif = 0.6878980891719745
Checkpoint checkpoint-4870: F1 positif = 0.7181372549019608
Checkpoint checkpoint-4870: F1 positif = 0.7304347826086957
Checkpoint checkpoint-5844: F1 positif = 0.6289120715350224
Checkpoint checkpoint-5844: F1 positif = 0.6740331491712708
Checkpoint checkpoint-5844: F1 positif = 0.6878980891719745
Checkpoint checkpoint-5844: F1 positif = 0.7181372549019608
Checkpoint checkpoint-5844: F1 positif = 0.7304347826086957
Checkpoint checkpoint-5844: F1 positif = 0.7315270935960592

Meilleur checkpoint: checkpoint-5844 avec F1 positif = 0.7315270935960592
Aucune métrique d'évaluation trouvée dans les fichiers trainer_state.json
Aucune métrique d'évaluation trouvée dans les fichiers trainer_state.json
Checkpoint checkpoint-4870: F1 positif = 0.6289120715350224
Checkpoint checkpoint-4870: F1 positif = 0.6740331491712708
Checkpoint checkpoint-4870: F1 positif = 0.6878980891719745
Checkpoint checkpoint-4870: F1 positif = 0.7181372549019608
Checkpoint checkpoint-4870: F1 positif = 0.7304347826086957
Checkpoint checkpoint-5844: F1 positif = 0.6289120715350224
Checkpoint checkpoint-5844: F1 positif = 0.6740331491712708
Checkpoint checkpoint-5844: F1 positif = 0.6878980891719745
Checkpoint checkpoint-5844: F1 positif = 0.7181372549019608
Checkpoint checkpoint-5844: F1 positif = 0.7304347826086957
Checkpoint checkpoint-5844: F1 positif = 0.7315270935960592

Meilleur checkpoint: checkpoint-5844 avec F1 positif = 0.7315270935960592
Aucune métrique d'évaluation trouvée dans les fichiers trainer_state.json
Checkpoint checkpoint-700: F1 positif = 0.38666666666666666
Checkpoint checkpoint-700: F1 positif = 0.38823529411764707
Checkpoint checkpoint-700: F1 positif = 0.6756756756756757
Checkpoint checkpoint-700: F1 positif = 0.6329113924050633
Checkpoint checkpoint-700: F1 positif = 0.6170212765957447
Checkpoint checkpoint-700: F1 positif = 0.6567164179104478
Checkpoint checkpoint-700: F1 positif = 0.6774193548387097
Checkpoint checkpoint-2000: F1 positif = 0.38666666666666666
Checkpoint checkpoint-2000: F1 positif = 0.38823529411764707
Checkpoint checkpoint-2000: F1 positif = 0.6756756756756757
Checkpoint checkpoint-2000: F1 positif = 0.6329113924050633
Checkpoint checkpoint-2000: F1 positif = 0.6170212765957447
Checkpoint checkpoint-2000: F1 positif = 0.6567164179104478
Checkpoint checkpoint-2000: F1 positif = 0.6774193548387097
Checkpoint checkpoint-2000: F1 positif = 0.676470588235294
Checkpoint checkpoint-2000: F1 positif = 0.591549295774648
Checkpoint checkpoint-2000: F1 positif = 0.5862068965517241
Checkpoint checkpoint-2000: F1 positif = 0.5384615384615384
Checkpoint checkpoint-2000: F1 positif = 0.5862068965517241
Checkpoint checkpoint-2000: F1 positif = 0.576271186440678
Checkpoint checkpoint-2000: F1 positif = 0.576271186440678
Checkpoint checkpoint-2000: F1 positif = 0.48000000000000004
Checkpoint checkpoint-2000: F1 positif = 0.5517241379310345
Checkpoint checkpoint-2000: F1 positif = 0.5517241379310345
Checkpoint checkpoint-2000: F1 positif = 0.5517241379310345
Checkpoint checkpoint-2000: F1 positif = 0.5517241379310345
Checkpoint checkpoint-2000: F1 positif = 0.5517241379310345
Checkpoint checkpoint-2096: F1 positif = 0.38666666666666666
Checkpoint checkpoint-2096: F1 positif = 0.38823529411764707
Checkpoint checkpoint-2096: F1 positif = 0.6756756756756757
Checkpoint checkpoint-2096: F1 positif = 0.6329113924050633
Checkpoint checkpoint-2096: F1 positif = 0.6170212765957447
Checkpoint checkpoint-2096: F1 positif = 0.6567164179104478
Checkpoint checkpoint-2096: F1 positif = 0.6774193548387097
Checkpoint checkpoint-2096: F1 positif = 0.676470588235294
Checkpoint checkpoint-2096: F1 positif = 0.591549295774648
Checkpoint checkpoint-2096: F1 positif = 0.5862068965517241
Checkpoint checkpoint-2096: F1 positif = 0.5384615384615384
Checkpoint checkpoint-2096: F1 positif = 0.5862068965517241
Checkpoint checkpoint-2096: F1 positif = 0.576271186440678
Checkpoint checkpoint-2096: F1 positif = 0.576271186440678
Checkpoint checkpoint-2096: F1 positif = 0.48000000000000004
Checkpoint checkpoint-2096: F1 positif = 0.5517241379310345
Checkpoint checkpoint-2096: F1 positif = 0.5517241379310345
Checkpoint checkpoint-2096: F1 positif = 0.5517241379310345
Checkpoint checkpoint-2096: F1 positif = 0.5517241379310345
Checkpoint checkpoint-2096: F1 positif = 0.5517241379310345

Meilleur checkpoint: checkpoint-700 avec F1 positif = 0.6774193548387097
Distribution des classes dans les données d'entraînement originales:
Classe positive: 153 (9.86%)
Classe négative: 1399 (90.14%)

Distribution des classes après rééquilibrage:
Classe positive: 699 (33.32%)
Classe négative: 1399 (66.68%)
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Distribution des classes dans les données d'entraînement originales:
Classe positive: 153 (9.86%)
Classe négative: 1399 (90.14%)

Distribution des classes après rééquilibrage:
Classe positive: 699 (33.32%)
Classe négative: 1399 (66.68%)
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Distribution des classes dans les données d'entraînement originales:
Classe positive: 153 (9.86%)
Classe négative: 1399 (90.14%)

Distribution des classes après rééquilibrage:
Classe positive: 699 (33.32%)
Classe négative: 1399 (66.68%)
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\Olivier\AppData\Local\Temp\ipykernel_13456\1134661924.py:170: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `PositiveF1Trainer.__init__`. Use `processing_class` instead.
  trainer = PositiveF1Trainer(
Distribution des classes dans les données d'entraînement originales:
Classe positive: 153 (9.86%)
Classe négative: 1399 (90.14%)

Distribution des classes après rééquilibrage:
Classe positive: 699 (33.32%)
Classe négative: 1399 (66.68%)
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\Olivier\AppData\Local\Temp\ipykernel_13456\2497697336.py:170: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `PositiveF1Trainer.__init__`. Use `processing_class` instead.
  trainer = PositiveF1Trainer(
