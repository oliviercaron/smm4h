Loading data...
Loaded 31187 train and 4625 dev examples.
Data loaded.
ACTION REQUIRED: Please ensure you have cleared the Hugging Face datasets cache (e.g., ~/.cache/huggingface/datasets) before this run if you suspect caching issues.
Tokenizing data...
Using 6 processes for tokenization...
Tokenization complete.
Loading main tokenizer 'xlm-roberta-large' for the Trainer...
Cleaning dataset columns...
Columns after cleaning and rename: ['labels']
Computing class weights and determining label info...
Model will be configured with num_labels=2
Class Weights (for classes [0, 1]): [0.5426468 6.3620973]

Setting up QLoRA configuration...
Loading model 'xlm-roberta-large' (num_labels=2)...
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Preparing model for k-bit training...

Inspecting/casting classifier...
Casting classifier to float32...
Inspecting after cast:
  dense.weight                   | torch.float32
  dense.bias                     | torch.float32
  out_proj.weight                | torch.float32
  out_proj.bias                  | torch.float32

Setting up LoRA configuration...
Applying LoRA adapter...
LoRA applied successfully.
trainable params: 2,624,514 || all params: 562,516,996 || trainable%: 0.4666

Verifying dataset columns before Trainer initialization...
Train dataset columns: ['labels']
Validation dataset columns: ['labels']
ERROR: Training dataset is MISSING required columns! Expected: ['input_ids', 'attention_mask', 'labels']
