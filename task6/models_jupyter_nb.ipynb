{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle Deberta v3 small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New train_df distribution:\n",
      " label\n",
      "0    1097\n",
      "1     919\n",
      "Name: count, dtype: int64\n",
      "\n",
      "New valid_df distribution:\n",
      " label\n",
      "0    275\n",
      "1    230\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set distribution (old valid.csv):\n",
      " label\n",
      "0    420\n",
      "1    366\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 05:57 < 02:33, 2.46 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.266300</td>\n",
       "      <td>0.238762</td>\n",
       "      <td>0.907598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.226771</td>\n",
       "      <td>0.915114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>0.280186</td>\n",
       "      <td>0.939457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>0.471487</td>\n",
       "      <td>0.921529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.312394</td>\n",
       "      <td>0.940678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.357378</td>\n",
       "      <td>0.937238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.436695</td>\n",
       "      <td>0.926230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Best threshold found on validation set: 0.52 (F1 = 0.9427)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test F1 on old valid.csv using threshold 0.52 = 0.9455\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAHHCAYAAAC4M/EEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABEuklEQVR4nO3deVxU5f4H8M8MMuyDosKIAu4LbhgaTrkmgss1t36VmqKZFoGVXtd7VcCNrpaaiUtmol5JzdSSyn1X3FDKXEgQA2NzZQRlnfP7w8vkCOqMM8M4cz7vXueVc85zzvmekfryfZ7nnCMRBEEAERERWS2puQMgIiIi02KyJyIisnJM9kRERFaOyZ6IiMjKMdkTERFZOSZ7IiIiK8dkT0REZOWY7ImIiKwckz0REZGVY7InMqGcnBy88cYbqFmzJiQSCRYvXmz0c0gkEkRGRhr9uJZq5MiRqF+/vrnDIHqhMNmTQSQSiU7LwYMHDT7X/fv3ERkZqfexcnJyMHHiRDRv3hyOjo5wcnKCv78/5syZg7t37xoc19OMHz8eu3btwrRp07B+/Xr06tXLpOerSpGRkZBIJJBKpcjIyKiwXaVSwcHBARKJBOHh4Xof/3n/vomoomrmDoAs2/r167U+r1u3Dnv27KmwvkWLFgaf6/79+4iKigIAdOvWTad9Tp8+jT59+iA/Px/vvPMO/P39AQBnzpzBp59+isOHD2P37t0Gx/Yk+/fvR//+/TFx4kSTnePBgweoVs18/ynb2dnh22+/xeTJk7XWb9261aDjPs/fNwCsWrUKarXaoHMTWRsmezLIO++8o/X5xIkT2LNnT4X15nD37l0MHDgQNjY2OHfuHJo3b661fe7cuVi1apVJY8jNzUX16tVNeg57e3uTHv9Z+vTpU2myj4uLQ9++ffH9999XSRwFBQVwcnKCra1tlZyPyJKwG59MTq1WY/HixWjZsiXs7e3h4eGB999/H3fu3NFqd+bMGQQHB6NWrVpwcHBAgwYN8O677wIArl27htq1awMAoqKiNMMDTxurXrlyJf766y8sXLiwQqIHAA8PD0yfPl1r3bJly9CyZUvY2dnB09MTYWFhFbr6u3XrhlatWuHixYvo3r07HB0dUbduXcyfP1/TJjY2FhKJBIIgICYmRhMv8Hf39+PK97l27ZpO30m5yr6Hc+fOoXfv3pDL5XB2dkaPHj1w4sSJSs937NgxTJgwAbVr14aTkxMGDhyIGzduPPF7fdzQoUORlJSEy5cva9ZlZ2dj//79GDp0aIX2xcXFmDlzJvz9/eHq6gonJyd07twZBw4c0LR51t/3yJEj4ezsjNTUVPTp0wcuLi4YNmyYZtujY/YRERGQSqXYt2+fVhxjx46FTCbDr7/+qvO1ElkqVvZkcu+//z5iY2MxatQofPTRR0hLS8PSpUtx7tw5HDt2DLa2tsjNzUVQUBBq166NqVOnonr16rh27ZqmK7h27dpYvnw5QkNDMXDgQAwaNAgA0KZNmyee98cff4SDgwPeeOMNneKMjIxEVFQUAgMDERoaiuTkZCxfvhynT5/WxFnuzp076NWrFwYNGoQ333wTW7ZswZQpU9C6dWv07t0bXbp0wfr16zF8+HD07NkTI0aM0Pt7e9Z38iQXLlxA586dIZfLMXnyZNja2mLlypXo1q0bDh06hICAAK3248aNQ40aNRAREYFr165h8eLFCA8Px6ZNm3SKs0uXLqhXrx7i4uIwa9YsAMCmTZvg7OyMvn37VmivUqnw9ddfY8iQIRgzZgzu3buH1atXIzg4GKdOnYKfn59Of9+lpaUIDg5Gp06d8Nlnn8HR0bHS+KZPn44dO3Zg9OjROH/+PFxcXLBr1y6sWrUKs2fPRtu2bXW6TiKLJhAZUVhYmPDoj9WRI0cEAMKGDRu02u3cuVNr/bZt2wQAwunTp5947Bs3bggAhIiICJ1iqVGjhtC2bVud2ubm5goymUwICgoSysrKNOuXLl0qABC++eYbzbquXbsKAIR169Zp1hUVFQkKhUIYPHiw1nEBCGFhYVrrIiIihMr+01uzZo0AQEhLSxMEQbfvpPwcj34nAwYMEGQymZCamqpZl5mZKbi4uAhdunSpcL7AwEBBrVZr1o8fP16wsbER7t69+9Tzll/HjRs3hIkTJwqNGzfWbOvQoYMwatSoSr+D0tJSoaioSOtYd+7cETw8PIR3331Xs+5pf98hISECAGHq1KmVbvPx8dFad/78eUEmkwnvvfeecOfOHaFu3bpC+/bthZKSkqdeI5G1YDc+mdR3330HV1dX9OzZEzdv3tQs/v7+cHZ21nTdlo9rx8fHo6SkxCjnVqlUcHFx0ant3r17UVxcjE8++QRS6d//WYwZMwZyuRw//fSTVntnZ2eteQkymQwvv/wyrl69apTYgef7TsrKyrB7924MGDAADRs21KyvU6cOhg4diqNHj0KlUmntM3bsWK1hhc6dO6OsrAx//vmnzrEOHToUKSkpOH36tObflXXhA4CNjQ1kMhmAh0M8t2/fRmlpKdq3b4+zZ8/qfE4ACA0N1aldq1atEBUVha+//hrBwcG4efMm1q5da9aJjURVicmeTOrKlSvIy8uDu7s7ateurbXk5+cjNzcXANC1a1cMHjwYUVFRqFWrFvr37481a9agqKjouc8tl8tx7949ndqWJ7ZmzZpprZfJZGjYsGGFxFevXr0K4+41atSoMA/BEM/zndy4cQP379+vcB3Awzsi1Gp1hdvkvL29tT7XqFEDAPS6lnbt2qF58+aIi4vDhg0boFAo8Nprrz2x/dq1a9GmTRvY29ujZs2aqF27Nn766Sfk5eXpfM5q1aqhXr16OrefNGkS2rZti1OnTiEiIgK+vr4670tk6fhrLZmUWq2Gu7s7NmzYUOn28klYEokEW7ZswYkTJ7Bjxw7s2rUL7777Lj7//HOcOHECzs7Oep+7efPmSEpKQnFxsaaSNBYbG5tK1wuC8Mx9K5ucBzysyh9vZ+zvpDKGXMujhg4diuXLl8PFxQVvvfWWVg/Jo/773/9i5MiRGDBgACZNmgR3d3fY2NggOjoaqampOp/Pzs7uieeozNWrV3HlyhUAwPnz53Xej8gasLInk2rUqBFu3bqFV199FYGBgRWWxydHdezYEXPnzsWZM2ewYcMGXLhwARs3bgTw5CT5JP369cODBw90uvXLx8cHAJCcnKy1vri4GGlpaZrtxlBeOT8+y/9J3eZP+04eV7t2bTg6Ola4DgC4fPkypFIpvLy8DLuAJxg6dCiysrLwxx9/PLELHwC2bNmChg0bYuvWrRg+fDiCg4MRGBiIwsJCrXb6/n0/jVqtxsiRIyGXy/Gvf/0L3377rcHPASCyJEz2ZFJvvvkmysrKMHv27ArbSktLNQnvzp07FSpJPz8/ANB0W5fPttb1qXcffPAB6tSpg3/+85/4448/KmzPzc3FnDlzAACBgYGQyWRYsmSJVhyrV69GXl5epbPKn1ejRo0AAIcPH9asKygowNq1a7Xa6fKdPM7GxgZBQUH44YcftG7hy8nJQVxcHDp16gS5XG6Eq6ioUaNGWLx4MaKjo/Hyyy8/sV15T8Kj13by5EkkJCRotdP37/tpFi5ciOPHj+Orr77C7Nmz8corryA0NBQ3b940+NhEloDd+GRSXbt2xfvvv4/o6GgkJSUhKCgItra2uHLlCr777jt88cUXeOONN7B27VosW7YMAwcORKNGjXDv3j2sWrUKcrkcffr0AQA4ODjA19cXmzZtQtOmTeHm5oZWrVqhVatWlZ67Ro0a2LZtG/r06QM/Pz+tJ+idPXsW3377LZRKJYCHFfG0adMQFRWFXr164fXXX0dycjKWLVuGDh06GPUhQUFBQfD29sbo0aMxadIk2NjY4JtvvkHt2rWRnp6uaafLd1KZOXPmYM+ePejUqRM+/PBDVKtWDStXrkRRUZHWswBM4eOPP35mm3/84x/YunUrBg4ciL59+yItLQ0rVqyAr68v8vPzNe30/ft+kkuXLmHGjBkYOXIk+vXrB+DhMwb8/Pzw4YcfYvPmzfpdJJElMuetAGR9Hr/1rtxXX30l+Pv7Cw4ODoKLi4vQunVrYfLkyUJmZqYgCIJw9uxZYciQIYK3t7dgZ2cnuLu7C//4xz+EM2fOaB3n+PHjgr+/vyCTyXS+DS8zM1MYP3680LRpU8He3l5wdHQU/P39hblz5wp5eXlabZcuXSo0b95csLW1FTw8PITQ0FDhzp07Wm26du0qtGzZssJ5KrvlC5XceicIgpCYmCgEBAQIMplM8Pb2FhYuXFjh1jtdv5PKvoezZ88KwcHBgrOzs+Do6Ch0795dOH78uFab8vM9fmvfgQMHBADCgQMHKsT9qEdvvXuax78DtVotzJs3T/Dx8RHs7OyEdu3aCfHx8ZV+f0/6+w4JCRGcnJwqPd+jxyktLRU6dOgg1KtXr8KthF988YUAQNi0adNT4yeyBhJB0HMWDhEREVkUjtkTERFZOSZ7IiIiK8dkT0REZOWY7ImIiKwckz0REZGVY7InIiKychb9UB21Wo3MzEy4uLgY9dGaRERUNQRBwL179+Dp6anXuw70VVhYiOLiYoOPI5PJYG9vb4SIqpZFJ/vMzEyTPeebiIiqTkZGhl5vMdRHYWEhHFxqAqX3DT6WQqFAWlqaxSV8i0725e8ql/mGQGJj3LeaEb0o/jywwNwhEJnMvXsqNGngrfn/uSkUFxcDpfdh5xsCGJIryoqRfXEtiouLmeyrUnnXvcRGxmRPVstUL64hepFUyVBsNXuDcoUgsdxpbhad7ImIiHQmAWDILxUWPDWMyZ6IiMRBIn24GLK/hbLcyImIiEgnrOyJiEgcJBIDu/Ettx+fyZ6IiMSB3fhERERkrVjZExGROLAbn4iIyNoZ2I1vwZ3hlhs5ERER6YSVPRERiQO78YmIiKwcZ+MTERGRtWJlT0RE4sBufCIiIisn4m58JnsiIhIHEVf2lvtrChEREemElT0REYkDu/GJiIisnERiYLJnNz4RERG9oFjZExGROEglDxdD9rdQTPZERCQOIh6zt9zIiYiISCes7ImISBxEfJ89kz0REYkDu/GJiIjIWrGyJyIicWA3PhERkZUTcTc+kz0REYmDiCt7y/01hYiIiHTCyp6IiMSB3fhERERWjt34REREZK1Y2RMRkUgY2I1vwfUxkz0REYkDu/GJiIjIWrGyJyIicZBIDJyNb7mVPZM9ERGJg4hvvbPcyImIiEgnrOyJiEgcRDxBj8meiIjEgd34REREVq68sjdk0cPy5cvRpk0byOVyyOVyKJVK/PLLL5rt3bp1g0Qi0Vo++OADrWOkp6ejb9++cHR0hLu7OyZNmoTS0lK9L52VPRERkQnUq1cPn376KZo0aQJBELB27Vr0798f586dQ8uWLQEAY8aMwaxZszT7ODo6av5cVlaGvn37QqFQ4Pjx48jKysKIESNga2uLefPm6RULkz0REYlDFXfj9+vXT+vz3LlzsXz5cpw4cUKT7B0dHaFQKCrdf/fu3bh48SL27t0LDw8P+Pn5Yfbs2ZgyZQoiIyMhk8l0joXd+EREJA5V3I3/qLKyMmzcuBEFBQVQKpWa9Rs2bECtWrXQqlUrTJs2Dffv39dsS0hIQOvWreHh4aFZFxwcDJVKhQsXLuh1flb2REREelCpVFqf7ezsYGdnV2nb8+fPQ6lUorCwEM7Ozti2bRt8fX0BAEOHDoWPjw88PT3x22+/YcqUKUhOTsbWrVsBANnZ2VqJHoDmc3Z2tl4xM9kTEZEolE+CM+AAAAAvLy+t1REREYiMjKx0l2bNmiEpKQl5eXnYsmULQkJCcOjQIfj6+mLs2LGadq1bt0adOnXQo0cPpKamolGjRs8fZyWY7ImISBSMlewzMjIgl8s1q59U1QOATCZD48aNAQD+/v44ffo0vvjiC6xcubJC24CAAABASkoKGjVqBIVCgVOnTmm1ycnJAYAnjvM/CcfsiYiI9FB+K1358rRk/zi1Wo2ioqJKtyUlJQEA6tSpAwBQKpU4f/48cnNzNW327NkDuVyuGQrQFSt7IiISB8n/FkP218O0adPQu3dveHt74969e4iLi8PBgwexa9cupKamIi4uDn369EHNmjXx22+/Yfz48ejSpQvatGkDAAgKCoKvry+GDx+O+fPnIzs7G9OnT0dYWJhev2AATPZERCQSxurG11Vubi5GjBiBrKwsuLq6ok2bNti1axd69uyJjIwM7N27F4sXL0ZBQQG8vLwwePBgTJ8+XbO/jY0N4uPjERoaCqVSCScnJ4SEhGjdl68rJnsiIiITWL169RO3eXl54dChQ888ho+PD37++WeDY2GyJyIiUajqyv5FwmRPRESiwGRPRERk5cSc7HnrHRERkZVjZU9EROJQxbfevUiY7ImISBTYjU9ERERWi5U9ERGJwsO31BpS2RsvlqrGZE9ERKIggYHd+Bac7dmNT0REZOVY2RMRkSiIeYIekz0REYmDiG+9Yzc+ERGRlWNlT0RE4mBgN77AbnwiIqIXm6Fj9obN5DcvJnsiIhIFMSd7jtkTERFZOVb2REQkDiKejc9kT0REosBufCIiIrJarOyJiEgUxFzZM9kTEZEoiDnZsxufiIjIyrGyJyIiURBzZc9kT0RE4iDiW+/YjU9ERGTlWNkTEZEosBufiIjIyjHZExERWTkxJ3uO2RMREVk5VvZERCQOIp6Nz2RPRESiwG58IiIislqs7EXu3cGd8O7gzvCq4wYAuHw1GwtW/4K9xy8CAOrXrYXZHw9ER7+GkNlWw76ES5jy2Xe4cfue5hhtmtVD5LgBeMnXG2VlAn48kITpi75HwYNis1wT0bMsit2N+AO/4sqfObC3s8XLrRsgYlx/NPHx0LSJ3XYM3+86g1+TryO/oBBp+/4DVxdHM0ZNhmJlb2YxMTGoX78+7O3tERAQgFOnTpk7JNHIzL2LqKU/oPuI+XgtZAGOnPkDGz4bi+YNFXC0l2Hr0jAIENA/9Ev0fm8RZLY2+Hbh+5ofekUtV2yPGYe0jBsIHPUZ3vg4Bi0aKhATMdzMV0b0ZMfOpmD0/3XGrtX/xNYvw1BSVobB42JQ8KBI0+ZBYTF6KFtgwsieZoyUjEkCiSbhP9diwYP2Zq/sN23ahAkTJmDFihUICAjA4sWLERwcjOTkZLi7u5s7PKu388jvWp/nLN+Bdwd3QvtWDVCndnV416mJru/8B/cKCgEAH0auR9r++ejSoSkOnUpGcOdWKCktw8T5myEIAgBgQvQmHNv4LzSoVwtp129W+TURPcuWJR9qfY6Z+Q6aBv8Lv17KwCsvNQYAhA7pDgA4mnilyuMjMjazV/YLFy7EmDFjMGrUKPj6+mLFihVwdHTEN998Y+7QREcqlWBQT384Oshw+nwa7GTVIAgCiopLNW0Ki0uhVgvo2LYRAEBmWw0lpWWaRA8AD4oedt939GtUtRdA9JxU+Q9/ma3uym56a2ZQVW/gEIC5mTXZFxcXIzExEYGBgZp1UqkUgYGBSEhIMGNk4uLbyBMZhz5HzrHFWDjtLQyftArJadk4ff4a7hcWI3JcfzjY2cLRXobZHw9EtWo2UNSSAwCOnEmGe005xr3TA7bVbODq4oCI8P4AHnbxE73o1Go1/rXwewS0bQjfRp7mDodMSWKExUKZNdnfvHkTZWVl8PDw0Frv4eGB7OzsCu2LioqgUqm0FjLclT9z0GVYNAJHfYZvvj+KZZHD0ayBArfu5mPk1NXo1bkVrh/+HH8eWABXFwckXUqHWv2wkr98NRsfRq5H2Ds9kHlkIZJ3zkN65i3k3FJBrVab+cqInm3S/O9w6WoWvp4z0tyhkJVZvnw52rRpA7lcDrlcDqVSiV9++UWzvbCwEGFhYahZsyacnZ0xePBg5OTkaB0jPT0dffv2haOjI9zd3TFp0iSUlpY+fqpnMvuYvT6io6MRFRVl7jCsTklpmWZs/dfLGWjn640P3u6G8dEbceDkZbw0MApurk4oLVNDlf8Al3fOw7XdiZr9t+w6gy27zqC2mwvuPyiCIAAfDn0N1/66Za5LItLJ5AWbsevo7/hp5ceo61HD3OGQiVX1bPx69erh008/RZMmTSAIAtauXYv+/fvj3LlzaNmyJcaPH4+ffvoJ3333HVxdXREeHo5Bgwbh2LFjAICysjL07dsXCoUCx48fR1ZWFkaMGAFbW1vMmzdPr1jMWtnXqlULNjY2FX6TycnJgUKhqNB+2rRpyMvL0ywZGRlVFaqoSCUSyGTavwfeziuAKv8BOrdvito1nPHLkfMV9rtx+x4KHhRjYM+XUFhcggMnL1dVyER6EQQBkxdsxk8Hf8MPy8bBp24tc4dEVaCqx+z79euHPn36oEmTJmjatCnmzp0LZ2dnnDhxAnl5eVi9ejUWLlyI1157Df7+/lizZg2OHz+OEydOAAB2796Nixcv4r///S/8/PzQu3dvzJ49GzExMSgu1u/WZrMme5lMBn9/f+zbt0+zTq1WY9++fVAqlRXa29nZabpDyhcyzMyw1/FKu0bwquMG30aemBn2Ojr5N8F3v5wBAAzt1xHtW9VH/bq18GbvDoiNHo1l3x5Ayp+5mmOM+b8uaNOsHhp5u+O9/+uC+ZPfxKyYH6HKf2CuyyJ6qknzN2PzL2fw1ewQODvaI+emCjk3VXhQ+Pf/QHNuqnD+j+u4mnEDAHAxJRPn/7iOO3kF5gqbDCSRGL4AqDCcXFRU9PQT42GVvnHjRhQUFECpVCIxMRElJSVac9aaN28Ob29vzZy1hIQEtG7dWmuoOzg4GCqVChcuXNDr2s3ejT9hwgSEhISgffv2ePnll7F48WIUFBRg1KhR5g5NFGrVcMbyyBHwqCWHKr8QF1L+wuBxy3Dw1MOqvImPO2aGvY4ackekZ97G52t2YVncfq1jvNTSB1PH9oWTowxXruVgwrxvsemX0+a4HCKdfPP9UQBAvw+WaK1fOnMYhv6jIwBgzdajmP/13+Orfd//okIbEicvLy+tzxEREYiMjKy07fnz56FUKlFYWAhnZ2ds27YNvr6+SEpKgkwmQ/Xq1bXaPzpnLTs7u9I5beXb9GH2ZP/WW2/hxo0bmDlzJrKzs+Hn54edO3dWuEAyjY/mxD11e9TSHxG19MentgmNXG/MkIhM7vapL5/ZZurYPpg6tk8VRENV5WF1bsiY/cN/Z2RkaPUs29nZPXGfZs2aISkpCXl5ediyZQtCQkJw6NCh547heZk92QNAeHg4wsPDzR0GERFZs0e64p93fwB6DSPLZDI0bvzwQU3+/v44ffo0vvjiC7z11lsoLi7G3bt3tar7R+esKRSKCk+ULZ/jVtm8tqcx+0N1iIiIxEKtVqOoqAj+/v6wtbXVmrOWnJyM9PR0zZw1pVKJ8+fPIzf37zlSe/bsgVwuh6+vr17nfSEqeyIiIlOr6lvvpk2bht69e8Pb2xv37t1DXFwcDh48iF27dsHV1RWjR4/GhAkT4ObmBrlcjnHjxkGpVKJjx4dzQoKCguDr64vhw4dj/vz5yM7OxvTp0xEWFvbUoYPKMNkTEZEoSAzsxtd339zcXIwYMQJZWVlwdXVFmzZtsGvXLvTs+fDlSosWLYJUKsXgwYNRVFSE4OBgLFu2TLO/jY0N4uPjERoaCqVSCScnJ4SEhGDWrFl6x85kT0REZAKrV69+6nZ7e3vExMQgJibmiW18fHzw888/GxwLkz0REYmCVCqBVPr8pb1gwL7mxmRPRESiUNXd+C8SzsYnIiKycqzsiYhIFKp6Nv6LhMmeiIhEQczd+Ez2REQkCmKu7DlmT0REZOVY2RMRkSiIubJnsiciIlEQ85g9u/GJiIisHCt7IiISBQkM7MaH5Zb2TPZERCQK7MYnIiIiq8XKnoiIRIGz8YmIiKwcu/GJiIjIarGyJyIiUWA3PhERkZUTczc+kz0REYmCmCt7jtkTERFZOVb2REQkDgZ241vwA/SY7ImISBzYjU9ERERWi5U9ERGJAmfjExERWTl24xMREZHVYmVPRESiwG58IiIiK8dufCIiIrJarOyJiEgUxFzZM9kTEZEocMyeiIjIyom5sueYPRERkZVjZU9ERKLAbnwiIiIrx258IiIislqs7ImISBQkMLAb32iRVD0meyIiEgWpRAKpAdnekH3Njd34REREJhAdHY0OHTrAxcUF7u7uGDBgAJKTk7XadOvWTTOXoHz54IMPtNqkp6ejb9++cHR0hLu7OyZNmoTS0lK9YmFlT0REolDVs/EPHTqEsLAwdOjQAaWlpfjXv/6FoKAgXLx4EU5OTpp2Y8aMwaxZszSfHR0dNX8uKytD3759oVAocPz4cWRlZWHEiBGwtbXFvHnzdI6FyZ6IiEShqmfj79y5U+tzbGws3N3dkZiYiC5dumjWOzo6QqFQVHqM3bt34+LFi9i7dy88PDzg5+eH2bNnY8qUKYiMjIRMJtMpFnbjExGRKEglhi8AoFKptJaioiKdzp+XlwcAcHNz01q/YcMG1KpVC61atcK0adNw//59zbaEhAS0bt0aHh4emnXBwcFQqVS4cOGCztfOyp6IiEgPXl5eWp8jIiIQGRn51H3UajU++eQTvPrqq2jVqpVm/dChQ+Hj4wNPT0/89ttvmDJlCpKTk7F161YAQHZ2tlaiB6D5nJ2drXPMTPZERCQOEgMfjPO/XTMyMiCXyzWr7ezsnrlrWFgYfv/9dxw9elRr/dixYzV/bt26NerUqYMePXogNTUVjRo1ev5YH8NufCIiEoXyCXqGLAAgl8u1lmcl+/DwcMTHx+PAgQOoV6/eU9sGBAQAAFJSUgAACoUCOTk5Wm3KPz9pnL8yTPZEREQmIAgCwsPDsW3bNuzfvx8NGjR45j5JSUkAgDp16gAAlEolzp8/j9zcXE2bPXv2QC6Xw9fXV+dY2I1PRESiIPnfP4bsr4+wsDDExcXhhx9+gIuLi2aM3dXVFQ4ODkhNTUVcXBz69OmDmjVr4rfffsP48ePRpUsXtGnTBgAQFBQEX19fDB8+HPPnz0d2djamT5+OsLAwnYYPyjHZExGRKDw6o/5599fH8uXLATx8cM6j1qxZg5EjR0Imk2Hv3r1YvHgxCgoK4OXlhcGDB2P69OmatjY2NoiPj0doaCiUSiWcnJwQEhKidV++LpjsiYiITEAQhKdu9/LywqFDh555HB8fH/z8888GxcJkT0REoiDmV9zqlOx//PFHnQ/4+uuvP3cwREREplLVj8t9keiU7AcMGKDTwSQSCcrKygyJh4iIiIxMp2SvVqtNHQcREZFJifkVtwaN2RcWFsLe3t5YsRAREZmMmLvx9X6oTllZGWbPno26devC2dkZV69eBQDMmDEDq1evNnqARERExvD4e+OfZ7FUeif7uXPnIjY2FvPnz9d6tV6rVq3w9ddfGzU4IiIiMpzeyX7dunX46quvMGzYMNjY2GjWt23bFpcvXzZqcERERMZirGfjWyK9x+z/+usvNG7cuMJ6tVqNkpISowRFRERkbGKeoKd3Ze/r64sjR45UWL9lyxa0a9fOKEERERGR8ehd2c+cORMhISH466+/oFarsXXrViQnJ2PdunWIj483RYxEREQGkwAGvAbHsH3NTe/Kvn///tixYwf27t0LJycnzJw5E5cuXcKOHTvQs2dPU8RIRERkMDHPxn+u++w7d+6MPXv2GDsWIiIiMoHnfqjOmTNncOnSJQAPx/H9/f2NFhQREZGxVfUrbl8keif769evY8iQITh27BiqV68OALh79y5eeeUVbNy4EfXq1TN2jERERAYT81vv9B6zf++991BSUoJLly7h9u3buH37Ni5dugS1Wo333nvPFDESERGRAfSu7A8dOoTjx4+jWbNmmnXNmjXDl19+ic6dOxs1OCIiImOy4OLcIHoney8vr0ofnlNWVgZPT0+jBEVERGRs7MbXw4IFCzBu3DicOXNGs+7MmTP4+OOP8dlnnxk1OCIiImMpn6BnyGKpdKrsa9SoofUbTUFBAQICAlCt2sPdS0tLUa1aNbz77rsYMGCASQIlIiKi56NTsl+8eLGJwyAiIjItMXfj65TsQ0JCTB0HERGRSYn5cbnP/VAdACgsLERxcbHWOrlcblBAREREZFx6J/uCggJMmTIFmzdvxq1btypsLysrM0pgRERExsRX3Oph8uTJ2L9/P5YvXw47Ozt8/fXXiIqKgqenJ9atW2eKGImIiAwmkRi+WCq9K/sdO3Zg3bp16NatG0aNGoXOnTujcePG8PHxwYYNGzBs2DBTxElERETPSe/K/vbt22jYsCGAh+Pzt2/fBgB06tQJhw8fNm50RERERiLmV9zqnewbNmyItLQ0AEDz5s2xefNmAA8r/vIX4xAREb1oxNyNr3eyHzVqFH799VcAwNSpUxETEwN7e3uMHz8ekyZNMnqAREREZBi9x+zHjx+v+XNgYCAuX76MxMRENG7cGG3atDFqcERERMYi5tn4Bt1nDwA+Pj7w8fExRixEREQmY2hXvAXnet2S/ZIlS3Q+4EcfffTcwRAREZkKH5f7DIsWLdLpYBKJhMmeiIjoBaNTsi+fff+iSj/4GR/TS1ar5pA15g6ByGSEkgdVdi4pnmNW+mP7WyqDx+yJiIgsgZi78S35FxUiIiLSASt7IiISBYkEkIp0Nj4reyIiEgWpxPBFH9HR0ejQoQNcXFzg7u6OAQMGIDk5WatNYWEhwsLCULNmTTg7O2Pw4MHIycnRapOeno6+ffvC0dER7u7umDRpEkpLS/W7dv1CJyIiIl0cOnQIYWFhOHHiBPbs2YOSkhIEBQWhoKBA02b8+PHYsWMHvvvuOxw6dAiZmZkYNGiQZntZWRn69u2L4uJiHD9+HGvXrkVsbCxmzpypVyzP1Y1/5MgRrFy5EqmpqdiyZQvq1q2L9evXo0GDBujUqdPzHJKIiMikqnqC3s6dO7U+x8bGwt3dHYmJiejSpQvy8vKwevVqxMXF4bXXXgMArFmzBi1atMCJEyfQsWNH7N69GxcvXsTevXvh4eEBPz8/zJ49G1OmTEFkZCRkMplOsehd2X///fcIDg6Gg4MDzp07h6KiIgBAXl4e5s2bp+/hiIiIqkRVd+M/Li8vDwDg5uYGAEhMTERJSQkCAwM1bZo3bw5vb28kJCQAABISEtC6dWt4eHho2gQHB0OlUuHChQu6X7u+wc6ZMwcrVqzAqlWrYGtrq1n/6quv4uzZs/oejoiIyKKoVCqtpbzofRq1Wo1PPvkEr776Klq1agUAyM7Ohkwmq/DGWA8PD2RnZ2vaPJroy7eXb9OV3sk+OTkZXbp0qbDe1dUVd+/e1fdwREREVcJYr7j18vKCq6urZomOjn7mucPCwvD7779j48aNJr7Kyuk9Zq9QKJCSkoL69etrrT969CgaNmxorLiIiIiMylhvvcvIyNB6aqudnd1T9wsPD0d8fDwOHz6MevXqadYrFAoUFxfj7t27WtV9Tk4OFAqFps2pU6e0jlc+W7+8jU6x69zyf8aMGYOPP/4YJ0+ehEQiQWZmJjZs2ICJEyciNDRU38MRERFVCakRFgCQy+Vay5OSvSAICA8Px7Zt27B//340aNBAa7u/vz9sbW2xb98+zbrk5GSkp6dDqVQCAJRKJc6fP4/c3FxNmz179kAul8PX11fna9e7sp86dSrUajV69OiB+/fvo0uXLrCzs8PEiRMxbtw4fQ9HRERklcLCwhAXF4cffvgBLi4umjF2V1dXODg4wNXVFaNHj8aECRPg5uYGuVyOcePGQalUomPHjgCAoKAg+Pr6Yvjw4Zg/fz6ys7Mxffp0hIWFPbNH4VF6J3uJRIJ///vfmDRpElJSUpCfnw9fX184OzvreygiIqIqU9Xvs1++fDkAoFu3blrr16xZg5EjRwJ4+FZZqVSKwYMHo6ioCMHBwVi2bJmmrY2NDeLj4xEaGgqlUgknJyeEhIRg1qxZesXy3I/LlclkenUhEBERmZMUBo7ZQ799BUF4Zht7e3vExMQgJibmiW18fHzw888/63Xux+md7Lt37/7UBwvs37/foICIiIjIuPRO9n5+flqfS0pKkJSUhN9//x0hISHGiouIiMioqrob/0Wid7JftGhRpesjIyORn59vcEBERESmYOhT8Ax9gp45Ge1FOO+88w6++eYbYx2OiIiIjMRo77NPSEiAvb29sQ5HRERkVA/fZ2/Ii3CMGEwV0zvZP/rqPeDhbMOsrCycOXMGM2bMMFpgRERExsQxez24urpqfZZKpWjWrBlmzZqFoKAgowVGRERExqFXsi8rK8OoUaPQunVr1KhRw1QxERERGR0n6OnIxsYGQUFBfLsdERFZHIkR/rFUes/Gb9WqFa5evWqKWIiIiEymvLI3ZLFUeif7OXPmYOLEiYiPj0dWVhZUKpXWQkRERC8WncfsZ82ahX/+85/o06cPAOD111/XemyuIAiQSCQoKyszfpREREQGEvOYvc7JPioqCh988AEOHDhgyniIiIhMQiKRPPXdLrrsb6l0Tvblb+/p2rWryYIhIiIi49Pr1jtL/q2GiIjEjd34OmratOkzE/7t27cNCoiIiMgU+AQ9HUVFRVV4gh4RERG92PRK9m+//Tbc3d1NFQsREZHJSCUSg16EY8i+5qZzsud4PRERWTIxj9nr/FCd8tn4REREZFl0ruzVarUp4yAiIjItAyfoWfCj8fV/xS0REZElkkICqQEZ25B9zY3JnoiIREHMt97p/SIcIiIisiys7ImISBTEPBufyZ6IiERBzPfZsxufiIjIyrGyJyIiURDzBD0meyIiEgUpDOzGt+Bb79iNT0REZOVY2RMRkSiwG5+IiMjKSWFYd7Yld4VbcuxERESkA1b2REQkChKJxKDXtVvyq96Z7ImISBQkMOzFdZab6pnsiYhIJPgEPSIiIrJarOyJiEg0LLc2NwwreyIiEoXy++wNWfRx+PBh9OvXD56enpBIJNi+fbvW9pEjR2omDZYvvXr10mpz+/ZtDBs2DHK5HNWrV8fo0aORn5+v97Uz2RMREZlAQUEB2rZti5iYmCe26dWrF7KysjTLt99+q7V92LBhuHDhAvbs2YP4+HgcPnwYY8eO1TsWduMTEZEoVPWtd71790bv3r2f2sbOzg4KhaLSbZcuXcLOnTtx+vRptG/fHgDw5Zdfok+fPvjss8/g6empcyys7ImISBSkRlgAQKVSaS1FRUXPHdPBgwfh7u6OZs2aITQ0FLdu3dJsS0hIQPXq1TWJHgACAwMhlUpx8uRJvc7DZE9ERKQHLy8vuLq6apbo6OjnOk6vXr2wbt067Nu3D//5z39w6NAh9O7dG2VlZQCA7OxsuLu7a+1TrVo1uLm5ITs7W69zsRufiIhEwVjd+BkZGZDL5Zr1dnZ2z3W8t99+W/Pn1q1bo02bNmjUqBEOHjyIHj16PHeclWFlT0REoiAxwgIAcrlca3neZP+4hg0bolatWkhJSQEAKBQK5ObmarUpLS3F7du3nzjO/yRM9kRERC+A69ev49atW6hTpw4AQKlU4u7du0hMTNS02b9/P9RqNQICAvQ6NrvxiYhIFKp6Nn5+fr6mSgeAtLQ0JCUlwc3NDW5uboiKisLgwYOhUCiQmpqKyZMno3HjxggODgYAtGjRAr169cKYMWOwYsUKlJSUIDw8HG+//bZeM/EBVvZERCQSxpqNr6szZ86gXbt2aNeuHQBgwoQJaNeuHWbOnAkbGxv89ttveP3119G0aVOMHj0a/v7+OHLkiNawwIYNG9C8eXP06NEDffr0QadOnfDVV1/pfe2s7ImISBSqurLv1q0bBEF44vZdu3Y98xhubm6Ii4vT67yVYWVPRERk5VjZExGRKPB99kRERFbueV5m8/j+lord+ERERFaOlT0REYmCFBJIDeiMN2Rfc2OyJyIiUWA3PhEREVktVvZERCQKkv/9Y8j+lorJnoiIRIHd+ERERGS1WNkTEZEoSAycjc9ufCIiohecmLvxmeyJiEgUxJzsOWZPRERk5VjZExGRKPDWOyIiIisnlTxcDNnfUrEbn4iIyMqxsiciIlFgNz4REZGV42x8IiIislqs7ImISBQkMKwr3oILeyZ7IiISB87GJyIiIqvFyp4qOHY2BV+u34tfL6cj+6YK/10wBn27tdVsz72lQuSXP+DAyUvIu/cAr7RrjP9M+j808nY3Y9RElRsV2AyjejaHdy1nAMDl63exYGsS9v36V4W2m6b0RKBfPQz/fB9+PpMOABjSpTGWhnau9NjN3v8WN1WFpguejIqz8c3k8OHDWLBgARITE5GVlYVt27ZhwIAB5gyJANx/UIRWTevindeVGD55ldY2QRDwzqSvUK2aDTZ89j5cnOwRE7cfA8K+xInN0+HkYGemqIkql3n7PmZ9m4ir2SpIALzdpTH+O7EHuk37EcnX72rafdDbF4JQcf9tCWkVfjFYGtoJdrY2TPQWhrPxzaSgoABt27ZFTEyMOcOgx/R8tSWmh/bDP7q3rbAtNT0Xp89fw+dT3sZLLX3QpL4HFk59C4VFJfh+V6IZoiV6ul1nM7A36TquZquQmq3C3M1nUVBYivaNa2vatPJxQ1jfVvho5dEK+xeWlCE374FmKVOr0bllHWw4cKUqL4OMQGKExVKZtbLv3bs3evfubc4QSE9FJaUAAHu7v390pFIpZLbVcCIpFSMGvGKu0IieSSqRoH/H+nC0q4YzV3IBAA4yG3wV3hWT15xAbt6DZx7jrS6N8aCoFD+evGbiaImMx6LG7IuKilBUVKT5rFKpzBiNODWtr0A9RQ3MivkRi6YNgaODDMviDiAz9y5ybuWZOzyiSrXwqoGds/rC3tYGBYUlGLFwP5L/evjzOmd4AE79kYtfEtN1OtY73Zri++NXUVhSZsqQyQSkkEBqQF+81IJre4uajR8dHQ1XV1fN4uXlZe6QRMe2mg3Wzx+DlD9z0aDHZHh2noCjZ/5A4Cu+kEgs6seJRCQlMw/dpv6AoBnxWLM3GTGhndGsrit6+Xuhc8s6+Pe6kzodp32T2mhWrzr+yy58i8RufAsxbdo0TJgwQfNZpVIx4ZuBXwtvHImbhrz8BygpKUWtGi4IHLkAfi28zR0aUaVKytRIy7kHAPg17RbaNayFsb1aorCkFA08XHB19TCt9rHjuyPhcg76z96ptX5496b47dot/Jp2q8piJzIGi0r2dnZ2sLPjbO8XhauzA4CHk/bOXUrHvz74h5kjItKNVCqBna0U/9lyHuv3/6G17diCgZi+7hR2ns3QWu9kVw0DOjbA7I2ciGqxDC3PLbi0t6hkT1Uj/34R0jJuaD7/mXkL55Ovo7qrI7wUbti+9yxq1XBGPQ83XEzNxNTPt6Bv1zZ4rWMLM0ZNVLkZb/tjb9J1XL9ZAGcHW7zxakO82kKB//t0t2aG/eOu3ypA+o18rXUDlA1gYyPB5qOpVRU6GRnvszeT/Px8pKSkaD6npaUhKSkJbm5u8PZml7C5JF36E/0+WKL5/O9FWwEAQ/oGYFnkcOTcVOHfi7bixu178Kglx9t9AjDpvV7mCpfoqWrJ7bHsw87wqO4I1f1iXEy/g//7dDcOns/U6zjvdG+K+FN/QnW/2ESREpmORBAqe4xE1Th48CC6d+9eYX1ISAhiY2Ofub9KpYKrqytybuVBLpebIEIi86s5ZI25QyAyGaHkAR78EIa8PNP9f7w8V+xLSoezy/OfI/+eCj38vE0aq6mYtbLv1q0bzPi7BhERiYiIh+wt69Y7IiIi0h8n6BERkTiIuLRnsiciIlEQ82x8duMTEZEolL/1zpBFH4cPH0a/fv3g6ekJiUSC7du3a20XBAEzZ85EnTp14ODggMDAQFy5ov10xtu3b2PYsGGQy+WoXr06Ro8ejfx87dtCdcFkT0REZALPerPr/PnzsWTJEqxYsQInT56Ek5MTgoODUVj496uThw0bhgsXLmDPnj2Ij4/H4cOHMXbsWL1jYTc+ERGJQlUP2T/tza6CIGDx4sWYPn06+vfvDwBYt24dPDw8sH37drz99tu4dOkSdu7cidOnT6N9+/YAgC+//BJ9+vTBZ599Bk9PT51jYWVPRETiYKQ34ahUKq3l0bex6iotLQ3Z2dkIDAzUrHN1dUVAQAASEhIAAAkJCahevbom0QNAYGAgpFIpTp7U7eVN5ZjsiYiI9ODl5aX1Btbo6Gi9j5GdnQ0A8PDw0Frv4eGh2ZadnQ13d3et7dWqVYObm5umja7YjU9ERKJgrNn4GRkZWk/Qs4QXtLGyJyIiUTDWbHy5XK61PE+yVygUAICcnByt9Tk5OZptCoUCubm5WttLS0tx+/ZtTRtdMdkTERFVsQYNGkChUGDfvn2adSqVCidPnoRSqQQAKJVK3L17F4mJf79Wef/+/VCr1QgICNDrfOzGJyIiUajq2fjPerPrJ598gjlz5qBJkyZo0KABZsyYAU9PTwwYMAAA0KJFC/Tq1QtjxozBihUrUFJSgvDwcLz99tt6zcQHmOyJiEgsqjjbnzlzRuvNrhMmTADw95tdJ0+ejIKCAowdOxZ3795Fp06dsHPnTtjb22v22bBhA8LDw9GjRw9IpVIMHjwYS5YsqXCuZ4ZuzlfcGoqvuCUx4CtuyZpV5Stuj164bvArbju1rMdX3BIREb2oxPxsfCZ7IiIShed5vv3j+1sqJnsiIhIFEb/hlrfeERERWTtW9kREJA4iLu2Z7ImISBTEPEGP3fhERERWjpU9ERGJAmfjExERWTkRD9mzG5+IiMjasbInIiJxEHFpz2RPRESiwNn4REREZLVY2RMRkShwNj4REZGVE/GQPZM9ERGJhIizPcfsiYiIrBwreyIiEgUxz8ZnsiciInEwcIKeBed6duMTERFZO1b2REQkCiKen8dkT0REIiHibM9ufCIiIivHyp6IiESBs/GJiIisnJgfl8tufCIiIivHyp6IiERBxPPzmOyJiEgkRJztmeyJiEgUxDxBj2P2REREVo6VPRERiYIEBs7GN1okVY/JnoiIREHEQ/bsxiciIrJ2rOyJiEgUxPxQHSZ7IiISCfF25LMbn4iIyMqxsiciIlEQczc+K3siIhIFiREWfURGRkIikWgtzZs312wvLCxEWFgYatasCWdnZwwePBg5OTmGXeQTMNkTERGZSMuWLZGVlaVZjh49qtk2fvx47NixA9999x0OHTqEzMxMDBo0yCRxsBufiIhEwRzd+NWqVYNCoaiwPi8vD6tXr0ZcXBxee+01AMCaNWvQokULnDhxAh07dnz+QCvByp6IiERBYoR/9HXlyhV4enqiYcOGGDZsGNLT0wEAiYmJKCkpQWBgoKZt8+bN4e3tjYSEBKNdczlW9kREJA5GuvNOpVJprbazs4OdnV2F5gEBAYiNjUWzZs2QlZWFqKgodO7cGb///juys7Mhk8lQvXp1rX08PDyQnZ1tQJCVY7InIiLSg5eXl9bniIgIREZGVmjXu3dvzZ/btGmDgIAA+Pj4YPPmzXBwcDB1mFqY7ImISBSM9UidjIwMyOVyzfrKqvrKVK9eHU2bNkVKSgp69uyJ4uJi3L17V6u6z8nJqXSM31AcsyciIlEon6BnyAIAcrlca9E12efn5yM1NRV16tSBv78/bG1tsW/fPs325ORkpKenQ6lUGv3aWdkTERGZwMSJE9GvXz/4+PggMzMTERERsLGxwZAhQ+Dq6orRo0djwoQJcHNzg1wux7hx46BUKo0+Ex9gsiciIpF43hn1j+6vj+vXr2PIkCG4desWateujU6dOuHEiROoXbs2AGDRokWQSqUYPHgwioqKEBwcjGXLlj13fE/DZE9EROJQxe/B2bhx41O329vbIyYmBjExMQYEpRuO2RMREVk5VvZERCQK4n3BLZM9ERGJBN96R0RERFaLlT0REYmEYbPxLbkjn8meiIhEgd34REREZLWY7ImIiKwcu/GJiEgUxNyNz2RPRESiUNWPy32RsBufiIjIyrGyJyIiUWA3PhERkZUT8+Ny2Y1PRERk5VjZExGROIi4tGeyJyIiUeBsfCIiIrJarOyJiEgUOBufiIjIyol4yJ7JnoiIRELE2Z5j9kRERFaOlT0REYmCmGfjM9kTEZEocIKehRIEAQBwT6UycyREpiOUPDB3CEQmU/7zXf7/c1NSGZgrDN3fnCw62d+7dw8A0LiBl5kjISIiQ9y7dw+urq4mObZMJoNCoUATI+QKhUIBmUxmhKiqlkSoil+nTEStViMzMxMuLi6QWHL/igVRqVTw8vJCRkYG5HK5ucMhMir+fFc9QRBw7949eHp6Qio13ZzxwsJCFBcXG3wcmUwGe3t7I0RUtSy6spdKpahXr565wxAluVzO/xmS1eLPd9UyVUX/KHt7e4tM0sbCW++IiIisHJM9ERGRlWOyJ73Y2dkhIiICdnZ25g6FyOj4803WyqIn6BEREdGzsbInIiKyckz2REREVo7JnoiIyMox2RMREVk5JnvSWUxMDOrXrw97e3sEBATg1KlT5g6JyCgOHz6Mfv36wdPTExKJBNu3bzd3SERGxWRPOtm0aRMmTJiAiIgInD17Fm3btkVwcDByc3PNHRqRwQoKCtC2bVvExMSYOxQik+Ctd6STgIAAdOjQAUuXLgXw8L0EXl5eGDduHKZOnWrm6IiMRyKRYNu2bRgwYIC5QyEyGlb29EzFxcVITExEYGCgZp1UKkVgYCASEhLMGBkREemCyZ6e6ebNmygrK4OHh4fWeg8PD2RnZ5spKiIi0hWTPRERkZVjsqdnqlWrFmxsbJCTk6O1PicnBwqFwkxRERGRrpjs6ZlkMhn8/f2xb98+zTq1Wo19+/ZBqVSaMTIiItJFNXMHQJZhwoQJCAkJQfv27fHyyy9j8eLFKCgowKhRo8wdGpHB8vPzkZKSovmclpaGpKQkuLm5wdvb24yRERkHb70jnS1duhQLFixAdnY2/Pz8sGTJEgQEBJg7LCKDHTx4EN27d6+wPiQkBLGxsVUfEJGRMdkTERFZOY7ZExERWTkmeyIiIivHZE9ERGTlmOyJiIisHJM9ERGRlWOyJyIisnJM9kRERFaOyZ7IQCNHjtR693m3bt3wySefVHkcBw8ehEQiwd27d5/YRiKRYPv27TofMzIyEn5+fgbFde3aNUgkEiQlJRl0HCJ6fkz2ZJVGjhwJiUQCiUQCmUyGxo0bY9asWSgtLTX5ubdu3YrZs2fr1FaXBE1EZCg+G5+sVq9evbBmzRoUFRXh559/RlhYGGxtbTFt2rQKbYuLiyGTyYxyXjc3N6Mch4jIWFjZk9Wys7ODQqGAj48PQkNDERgYiB9//BHA313vc+fOhaenJ5o1awYAyMjIwJtvvonq1avDzc0N/fv3x7Vr1zTHLCsrw4QJE1C9enXUrFkTkydPxuNPnH68G7+oqAhTpkyBl5cX7Ozs0LhxY6xevRrXrl3TPI+9Ro0akEgkGDlyJICHbxWMjo5GgwYN4ODggLZt22LLli1a5/n555/RtGlTODg4oHv37lpx6mrKlClo2rQpHB0d0bBhQ8yYMQMlJSUV2q1cuRJeXl5wdHTEm2++iby8PK3tX3/9NVq0aAF7e3s0b94cy5Yt0zsWIjIdJnsSDQcHBxQXF2s+79u3D8nJydizZw/i4+NRUlKC4OBguLi44MiRIzh27BicnZ3Rq1cvzX6ff/45YmNj8c033+Do0aO4ffs2tm3b9tTzjhgxAt9++y2WLFmCS5cuYeXKlXB2doaXlxe+//57AEBycjKysrLwxRdfAACio6Oxbt06rFixAhcuXMD48ePxzjvv4NChQwAe/lIyaNAg9OvXD0lJSXjvvfcwdepUvb8TFxcXxMbG4uLFi/jiiy+watUqLFq0SKtNSkoKNm/ejB07dmDnzp04d+4cPvzwQ832DRs2YObMmZg7dy4uXbqEefPmYcaMGVi7dq3e8RCRiQhEVigkJETo37+/IAiCoFarhT179gh2dnbCxIkTNds9PDyEoqIizT7r168XmjVrJqjVas26oqIiwcHBQdi1a5cgCIJQp04dYf78+ZrtJSUlQr169TTnEgRB6Nq1q/Dxxx8LgiAIycnJAgBhz549lcZ54MABAYBw584dzbrCwkLB0dFROH78uFbb0aNHC0OGDBEEQRCmTZsm+Pr6am2fMmVKhWM9DoCwbdu2J25fsGCB4O/vr/kcEREh2NjYCNevX9es++WXXwSpVCpkZWUJgiAIjRo1EuLi4rSOM3v2bEGpVAqCIAhpaWkCAOHcuXNPPC8RmRbH7MlqxcfHw9nZGSUlJVCr1Rg6dCgiIyM121u3bq01Tv/rr78iJSUFLi4uWscpLCxEamoq8vLykJWVpfVa32rVqqF9+/YVuvLLJSUlwcbGBl27dtU57pSUFNy/fx89e/bUWl9cXIx27doBAC5dulTh9cJKpVLnc5TbtGkTlixZgtTUVOTn56O0tBRyuVyrjbe3N+rWrat1HrVajeTkZLi4uCA1NRWjR4/GmDFjNG1KS0vh6uqqdzxEZBpM9mS1unfvjuXLl0Mmk8HT0xPVqmn/uDs5OWl9zs/Ph7+/PzZs2FDhWLVr136uGBwcHPTeJz8/HwDw008/aSVZ4OE8BGNJSEjAsGHDEBUVheDgYLi6umLjxo34/PPP9Y511apVFX75sLGxMVqsRGQYJnuyWk5OTmjcuLHO7V966SVs2rQJ7u7uFarbcnXq1MHJkyfRpUsXAA8r2MTERLz00kuVtm/dujXUajUOHTqEwMDACtvLexbKyso063x9fWFnZ4f09PQn9gi0aNFCM9mw3IkTJ559kY84fvw4fHx88O9//1uz7s8//6zQLj09HZmZmfD09NScRyqVolmzZvDw8ICnpyeuXr2KYcOG6XV+Iqo6nKBH9D/Dhg1DrVq10L9/fxw5cgRpaWk4ePAgPvroI1y/fh0A8PHHH+PTTz/F9u3bcfnyZXz44YdPvUe+fv36CAkJwbvvvovt27drjrl582YAgI+PDyQSCeLj43Hjxg3k5+fDxcUFEydOxPjx47F27Vqkpqbi7Nmz+PLLLzWT3j744ANcuXIFkyZNQnJyMuLi4hAbG6vX9TZp0gTp6enYuHEjUlNTsWTJkkonG9rb2yMkJAS//vorjhw5go8++ghvvvkmFAoFACAqKgrR0dFYsmQJ/vjjD5w/fx5r1qzBwoUL9YqHiEyHyZ7ofxwdHXH48GF4e3tj0KBBaNGiBUaPHo3CwkJNpf/Pf/4Tw4cPR0hICJRKJVxcXDBw4MCnHnf58uV444038OGHH6J58+YYM2YMCgoKAAB169ZFVFQUpk6dCg8PD4SHhwMAZs+ejRkzZiA6OhotWrRAr1698NNPP6FBgwYAHo6jf//999i+fTvatm2LFStWYN68eXpd7+uvv47x48cjPDwcfn5+OH78OGbMmFGhXePGjTFo0CD06dMHQUFBaNOmjdatde+99x6+/vprrFmzBq1bt0bXrl0RGxuriZWIzE8iPGlmEREREVkFVvZERERWjsmeiIjIyjHZExERWTkmeyIiIivHZE9ERGTlmOyJiIisHJM9ERGRlWOyJyIisnJM9kRERFaOyZ6IiMjKMdkTERFZOSZ7IiIiK/f/6WixWMckEaQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.95      0.95      0.95       420\n",
      "     Class 1       0.94      0.95      0.95       366\n",
      "\n",
      "    accuracy                           0.95       786\n",
      "   macro avg       0.95      0.95      0.95       786\n",
      "weighted avg       0.95      0.95      0.95       786\n",
      "\n",
      "\n",
      "Training logs saved to training_log.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          Trainer, TrainingArguments, EarlyStoppingCallback)\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configurations identiques à votre script original\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Configuration mémoire\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"GPU Memory Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load the datasets\n",
    "# -------------------------------\n",
    "# train.csv = big dataset (2521 examples)\n",
    "# valid.csv = final test set (786 examples) => We'll rename it test_df in code\n",
    "\n",
    "train_full_df = pd.read_csv(\"data/train.csv\")   # 2521 examples\n",
    "test_df = pd.read_csv(\"data/valid.csv\")           # 786 examples (final sealed test)\n",
    "\n",
    "# Rename \"labels\" to \"label\" for consistency\n",
    "train_full_df = train_full_df.rename(columns={\"labels\": \"label\"})\n",
    "test_df = test_df.rename(columns={\"labels\": \"label\"})\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Split train.csv into train + valid subsets\n",
    "# -------------------------------\n",
    "# We take 80% for training, 20% for validation\n",
    "train_df, valid_df = train_test_split(\n",
    "    train_full_df,\n",
    "    test_size=0.2,  \n",
    "    stratify=train_full_df[\"label\"],  # maintain class distribution\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"New train_df distribution:\\n\", train_df[\"label\"].value_counts())\n",
    "print(\"\\nNew valid_df distribution:\\n\", valid_df[\"label\"].value_counts())\n",
    "print(\"\\nTest set distribution (old valid.csv):\\n\", test_df[\"label\"].value_counts())\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Create a custom Dataset\n",
    "# -------------------------------\n",
    "class VaccineDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=384):  # Changed max_length to 384\n",
    "        self.texts = dataframe['text'].tolist()\n",
    "        self.labels = dataframe['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text with truncation and padding to max_length\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Remove the batch dimension from each tensor\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Load the DeBERTa tokenizer and model\n",
    "# -------------------------------\n",
    "model_name = \"microsoft/deberta-v3-small\"  # Changed model to DeBERTa\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,\n",
    "    # Activation des techniques d'économie mémoire\n",
    "    device_map='auto'  # Répartition automatique sur le GPU\n",
    ")\n",
    "\n",
    "# Activation du gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Create datasets using the custom VaccineDataset class\n",
    "train_dataset = VaccineDataset(train_df, tokenizer, max_length=384)\n",
    "valid_dataset = VaccineDataset(valid_df, tokenizer, max_length=384)\n",
    "test_dataset = VaccineDataset(test_df, tokenizer, max_length=384)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Define the compute_metrics function for evaluation\n",
    "# -------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_score(labels, preds, pos_label=1)\n",
    "    return {\"f1\": f1}\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Define TrainingArguments with EarlyStopping\n",
    "# -------------------------------\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,  # Réduit de 16 à 8 pour la 3070\n",
    "    per_device_eval_batch_size=8,   # Idem\n",
    "    gradient_accumulation_steps=2,  # Simule un batch size de 16\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to=[],\n",
    "    fp16=True,  # Enable mixed precision for better memory usage\n",
    "    gradient_checkpointing=True  # Économie mémoire\n",
    ")\n",
    "# -------------------------------\n",
    "# 7. Create the Trainer with an EarlyStopping callback\n",
    "# -------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,  # used for early stopping and best model selection\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # stops if no improvement after 2 epochs\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# -------------------------------\n",
    "# 7b. Tune the best threshold on the validation dataset\n",
    "# -------------------------------\n",
    "# Get predictions on valid_df for threshold tuning\n",
    "valid_predictions = trainer.predict(valid_dataset)\n",
    "valid_logits = valid_predictions.predictions\n",
    "valid_labels = valid_predictions.label_ids\n",
    "\n",
    "# Compute probabilities for the positive class\n",
    "valid_probs = torch.softmax(torch.tensor(valid_logits), dim=1)[:, 1].numpy()\n",
    "\n",
    "# Search for the best threshold\n",
    "best_f1 = 0\n",
    "best_thresh = 0.5  # Default threshold\n",
    "for thresh in np.arange(0, 1.01, 0.01):\n",
    "    preds = (valid_probs >= thresh).astype(int)\n",
    "    score = f1_score(valid_labels, preds, pos_label=1)\n",
    "    if score > best_f1:\n",
    "        best_f1 = score\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(f\"\\n✅ Best threshold found on validation set: {best_thresh:.2f} (F1 = {best_f1:.4f})\")\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Evaluate on the final test set (old valid.csv) using the best threshold\n",
    "# -------------------------------\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "test_logits = test_predictions.predictions\n",
    "test_labels = test_predictions.label_ids\n",
    "\n",
    "# Apply the optimal threshold\n",
    "test_probs = torch.softmax(torch.tensor(test_logits), dim=1)[:, 1].numpy()\n",
    "test_preds = (test_probs >= best_thresh).astype(int)\n",
    "\n",
    "# Compute F1-score on the test set using the tuned threshold\n",
    "test_f1 = f1_score(test_labels, test_preds, pos_label=1)\n",
    "print(f\"\\nFinal Test F1 on old valid.csv using threshold {best_thresh:.2f} = {test_f1:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 8b. Display the Confusion Matrix on the Test Set\n",
    "# -------------------------------\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 8c. Print the Classification Report (precision, recall, F1 per class)\n",
    "# -------------------------------\n",
    "report = classification_report(test_labels, test_preds, target_names=[\"Class 0\", \"Class 1\"])\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# -------------------------------\n",
    "# 8d. Save training logs to CSV\n",
    "# -------------------------------\n",
    "logs_df = pd.DataFrame(trainer.state.log_history)\n",
    "logs_df.to_csv(\"training_log.csv\", index=False)\n",
    "print(\"\\nTraining logs saved to training_log.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Plot learning curves to check for overfitting\n",
    "# -------------------------------\n",
    "# Extract training and evaluation logs\n",
    "train_logs = [log for log in trainer.state.log_history if \"loss\" in log and \"eval_loss\" not in log]\n",
    "eval_logs = [log for log in trainer.state.log_history if \"eval_loss\" in log]\n",
    "\n",
    "# Build lists of epochs, train loss, eval loss, and eval f1\n",
    "train_epochs = [log[\"epoch\"] for log in train_logs]\n",
    "train_loss = [log[\"loss\"] for log in train_logs]\n",
    "\n",
    "eval_epochs = [log[\"epoch\"] for log in eval_logs]\n",
    "eval_loss = [log[\"eval_loss\"] for log in eval_logs]\n",
    "eval_f1 = [log[\"eval_f1\"] for log in eval_logs]\n",
    "\n",
    "# Plot: Training vs Validation Loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_epochs, train_loss, marker='o', label=\"Training Loss\")\n",
    "plt.plot(eval_epochs, eval_loss, marker='o', label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot: Validation F1 Score\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(eval_epochs, eval_f1, marker='o', color='green', label=\"Validation F1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Validation F1 over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deberta + moyenne de 3 loss + AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna + 3 loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New train_df distribution:\n",
      " label\n",
      "0    1097\n",
      "1     919\n",
      "Name: count, dtype: int64\n",
      "\n",
      "New valid_df distribution:\n",
      " label\n",
      "0    275\n",
      "1    230\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set distribution (old valid.csv):\n",
      " label\n",
      "0    420\n",
      "1    366\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-25 22:17:44,044] A new study created in memory with name: no-name-4d0cedc3-a480-4271-ac6b-8349c1034702\n",
      "C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_21744\\1546441541.py:162: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform(\"learning_rate\", 1e-6, 5e-5)\n",
      "C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_21744\\1546441541.py:163: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  weight_decay = trial.suggest_uniform(\"weight_decay\", 0.0, 0.1)\n",
      "C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_21744\\1546441541.py:164: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  alpha = trial.suggest_uniform(\"alpha\", 0.1, 1.0)\n",
      "C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_21744\\1546441541.py:165: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  beta = trial.suggest_uniform(\"beta\", 0.1, 1.0)\n",
      "C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_21744\\1546441541.py:166: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform(\"gamma\", 0.1, 1.0)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  23/1260 00:08 < 08:09, 2.53 it/s, Epoch 0.17/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-25 22:17:54,052] Trial 0 failed with parameters: {'learning_rate': 3.9999152944622774e-05, 'weight_decay': 0.09362751842441479, 'alpha': 0.5670094958572052, 'beta': 0.22800113580429945, 'gamma': 0.11020441217734267} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_21744\\1546441541.py\", line 210, in objective\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py\", line 2561, in _inner_training_loop\n",
      "    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-25 22:17:54,053] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 223\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# 11. Run the Optuna study to optimize hyperparameters\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m    222\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 223\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Increase n_trials for a more exhaustive search\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    226\u001b[0m trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[12], line 210\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    197\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[0;32m    198\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    199\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m     gamma\u001b[38;5;241m=\u001b[39mgamma\n\u001b[0;32m    207\u001b[0m )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Evaluate on the validation set\u001b[39;00m\n\u001b[0;32m    213\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(valid_dataset)\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2561\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2555\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2556\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2559\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2561\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2562\u001b[0m ):\n\u001b[0;32m   2563\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2564\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2565\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          Trainer, TrainingArguments, EarlyStoppingCallback)\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, fbeta_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Set random seeds for reproducibility\n",
    "# -------------------------------\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Function to print GPU memory (if available)\n",
    "# -------------------------------\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"GPU Memory Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load datasets\n",
    "# -------------------------------\n",
    "# train.csv: big dataset (2521 examples)\n",
    "# valid.csv: final test set (786 examples)\n",
    "train_full_df = pd.read_csv(\"data/train.csv\")   # 2521 examples\n",
    "test_df = pd.read_csv(\"data/valid.csv\")           # 786 examples (final sealed test)\n",
    "\n",
    "# Rename \"labels\" to \"label\" for consistency\n",
    "train_full_df = train_full_df.rename(columns={\"labels\": \"label\"})\n",
    "test_df = test_df.rename(columns={\"labels\": \"label\"})\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Split train_full_df into train and validation subsets (80/20 split)\n",
    "# -------------------------------\n",
    "train_df, valid_df = train_test_split(\n",
    "    train_full_df,\n",
    "    test_size=0.2,  \n",
    "    stratify=train_full_df[\"label\"],  # maintain class distribution\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"New train_df distribution:\\n\", train_df[\"label\"].value_counts())\n",
    "print(\"\\nNew valid_df distribution:\\n\", valid_df[\"label\"].value_counts())\n",
    "print(\"\\nTest set distribution (old valid.csv):\\n\", test_df[\"label\"].value_counts())\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Create a custom Dataset for tokenization\n",
    "# -------------------------------\n",
    "class VaccineDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=384):\n",
    "        self.texts = dataframe['text'].tolist()\n",
    "        self.labels = dataframe['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text with truncation and padding to max_length\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Remove the batch dimension from each tensor\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Load the DeBERTa tokenizer and set model name\n",
    "# -------------------------------\n",
    "model_name = \"roberta-base\"  # Using DeBERTa model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = VaccineDataset(train_df, tokenizer, max_length=384)\n",
    "valid_dataset = VaccineDataset(valid_df, tokenizer, max_length=384)\n",
    "test_dataset  = VaccineDataset(test_df, tokenizer, max_length=384)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Define custom loss functions\n",
    "# -------------------------------\n",
    "def focal_loss(logits, targets, alpha=1, gamma=2):\n",
    "    \"\"\"\n",
    "    Compute focal loss for multi-class classification.\n",
    "    \"\"\"\n",
    "    ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)  # p_t: probability of correct classification\n",
    "    focal_loss_val = alpha * ((1 - pt) ** gamma) * ce_loss\n",
    "    return focal_loss_val.mean()\n",
    "\n",
    "def dice_loss(logits, targets, smooth=1):\n",
    "    \"\"\"\n",
    "    Compute dice loss for multi-class classification.\n",
    "    \"\"\"\n",
    "    num_classes = logits.size(1)\n",
    "    # Convert targets to one-hot encoding\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=num_classes).float()\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    intersection = torch.sum(probs * targets_one_hot, dim=0)\n",
    "    union = torch.sum(probs, dim=0) + torch.sum(targets_one_hot, dim=0)\n",
    "    dice = (2 * intersection + smooth) / (union + smooth)\n",
    "    return (1 - dice).mean()\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Create a custom Trainer to use the combined loss\n",
    "# -------------------------------\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, alpha=0.33, beta=0.33, gamma=0.33, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        alpha, beta, gamma: weights for cross entropy, focal, and dice losses respectively.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha_weight = alpha\n",
    "        self.beta_weight = beta\n",
    "        self.gamma_weight = gamma\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Cross Entropy Loss\n",
    "        ce_loss = F.cross_entropy(logits, labels)\n",
    "        # Focal Loss\n",
    "        fl = focal_loss(logits, labels, alpha=1, gamma=2)\n",
    "        # Dice Loss\n",
    "        dl = dice_loss(logits, labels)\n",
    "        \n",
    "        # Combined Loss: weighted sum of individual losses\n",
    "        combined_loss = self.alpha_weight * ce_loss + self.beta_weight * fl + self.gamma_weight * dl\n",
    "        \n",
    "        return (combined_loss, outputs) if return_outputs else combined_loss\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Define compute_metrics function for evaluation using Fβ (beta=1.3)\n",
    "# -------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    fbeta = fbeta_score(labels, preds, beta=1.3, pos_label=1)\n",
    "    return {\"f1_beta\": fbeta}\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Define the objective function for Optuna\n",
    "# -------------------------------\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_uniform(\"learning_rate\", 1e-6, 5e-5)\n",
    "    weight_decay = trial.suggest_uniform(\"weight_decay\", 0.0, 0.1)\n",
    "    alpha = trial.suggest_uniform(\"alpha\", 0.1, 1.0)\n",
    "    beta = trial.suggest_uniform(\"beta\", 0.1, 1.0)\n",
    "    gamma = trial.suggest_uniform(\"gamma\", 0.1, 1.0)\n",
    "    \n",
    "    # Define TrainingArguments for this trial\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_beta\",\n",
    "        logging_steps=50,\n",
    "        seed=SEED,\n",
    "        report_to=[],\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True\n",
    "    )\n",
    "    \n",
    "    # Reload the model for each trial\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=2,\n",
    "        device_map='auto'\n",
    "    )\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Instantiate the custom trainer with the current hyperparameters\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        gamma=gamma\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    eval_result = trainer.evaluate(valid_dataset)\n",
    "    f1_beta = eval_result.get(\"eval_f1_beta\")\n",
    "    \n",
    "    # Report the metric to Optuna\n",
    "    return f1_beta\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Run the Optuna study to optimize hyperparameters\n",
    "# -------------------------------\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)  # Increase n_trials for a more exhaustive search\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  F1_beta: {:.4f}\".format(trial.value))\n",
    "print(\"  Best hyperparameters: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 12. Retrain final model using best hyperparameters and evaluate on the test set\n",
    "# -------------------------------\n",
    "\n",
    "# Use the best hyperparameters from Optuna\n",
    "best_lr = trial.params[\"learning_rate\"]\n",
    "best_wd = trial.params[\"weight_decay\"]\n",
    "best_alpha = trial.params[\"alpha\"]\n",
    "best_beta = trial.params[\"beta\"]\n",
    "best_gamma = trial.params[\"gamma\"]\n",
    "\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./final_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=best_lr,\n",
    "    weight_decay=best_wd,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_beta\",\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "# Load a fresh model\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    device_map='auto'\n",
    ")\n",
    "final_model.gradient_checkpointing_enable()\n",
    "\n",
    "final_trainer = CustomTrainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,  # You can also combine train and valid for final training if desired\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    alpha=best_alpha,\n",
    "    beta=best_beta,\n",
    "    gamma=best_gamma\n",
    ")\n",
    "\n",
    "# Train the final model\n",
    "final_trainer.train()\n",
    "\n",
    "# -------------------------------\n",
    "# 13. Tune the best threshold on the validation dataset using Fβ (beta=1.3)\n",
    "# -------------------------------\n",
    "valid_predictions = final_trainer.predict(valid_dataset)\n",
    "valid_logits = valid_predictions.predictions\n",
    "valid_labels = valid_predictions.label_ids\n",
    "\n",
    "# Compute probabilities for the positive class\n",
    "valid_probs = torch.softmax(torch.tensor(valid_logits), dim=1)[:, 1].numpy()\n",
    "\n",
    "best_fbeta = 0\n",
    "best_thresh = 0.5  # Default threshold\n",
    "for thresh in np.arange(0, 1.01, 0.01):\n",
    "    preds = (valid_probs >= thresh).astype(int)\n",
    "    score = fbeta_score(valid_labels, preds, beta=1.3, pos_label=1)\n",
    "    if score > best_fbeta:\n",
    "        best_fbeta = score\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(f\"\\n✅ Best threshold found on validation set: {best_thresh:.2f} (Fβ = {best_fbeta:.4f})\")\n",
    "\n",
    "# -------------------------------\n",
    "# 14. Evaluate on the final test set using the best threshold\n",
    "# -------------------------------\n",
    "test_predictions = final_trainer.predict(test_dataset)\n",
    "test_logits = test_predictions.predictions\n",
    "test_labels = test_predictions.label_ids\n",
    "\n",
    "test_probs = torch.softmax(torch.tensor(test_logits), dim=1)[:, 1].numpy()\n",
    "test_preds = (test_probs >= best_thresh).astype(int)\n",
    "\n",
    "test_f1 = f1_score(test_labels, test_preds, pos_label=1)\n",
    "print(f\"\\nFinal Test F1 on old valid.csv using threshold {best_thresh:.2f} = {test_f1:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 15. Display the Confusion Matrix on the Test Set\n",
    "# -------------------------------\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 16. Print the Classification Report\n",
    "# -------------------------------\n",
    "report = classification_report(test_labels, test_preds, target_names=[\"Class 0\", \"Class 1\"])\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# -------------------------------\n",
    "# 17. (Optional) Plot learning curves if desired\n",
    "# -------------------------------\n",
    "train_logs = [log for log in final_trainer.state.log_history if \"loss\" in log and \"eval_loss\" not in log]\n",
    "eval_logs = [log for log in final_trainer.state.log_history if \"eval_loss\" in log]\n",
    "\n",
    "train_epochs = [log[\"epoch\"] for log in train_logs]\n",
    "train_loss = [log[\"loss\"] for log in train_logs]\n",
    "\n",
    "eval_epochs = [log[\"epoch\"] for log in eval_logs]\n",
    "eval_loss = [log[\"eval_loss\"] for log in eval_logs]\n",
    "eval_f1 = [log[\"eval_f1_beta\"] for log in eval_logs]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_epochs, train_loss, marker='o', label=\"Training Loss\")\n",
    "plt.plot(eval_epochs, eval_loss, marker='o', label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(eval_epochs, eval_f1, marker='o', color='green', label=\"Validation F1 (β=1.3)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Validation F1 over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text = \"Took Shingrix shot 2 wks ago. About 4 days ago I woke up with 'bites' all over my back that itch...\"\n",
    "classes = [\"This is a personal vaccine adverse event\", \"This is not a personal vaccine adverse event\"]\n",
    "hypothesis_template = \"The post is about {}\"\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\")\n",
    "result = classifier(text, classes, hypothesis_template=hypothesis_template, multi_label=False)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna biomed_roberta_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New train_df distribution:\n",
      " label\n",
      "0    1097\n",
      "1     919\n",
      "Name: count, dtype: int64\n",
      "\n",
      "New valid_df distribution:\n",
      " label\n",
      "0    275\n",
      "1    230\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set distribution (old valid.csv):\n",
      " label\n",
      "0    420\n",
      "1    366\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 10:31:29,135] A new study created in memory with name: no-name-9b447904-c015-48ae-965b-020a0353808c\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 378/1260 02:12 < 05:10, 2.84 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>0.224218</td>\n",
       "      <td>0.906383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.161700</td>\n",
       "      <td>0.555325</td>\n",
       "      <td>0.865547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.432978</td>\n",
       "      <td>0.902314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 10:33:45,459] Trial 0 finished with value: 0.9063826629063383 and parameters: {'learning_rate': 2.0698779836659384e-05, 'weight_decay': 0.009948318492467757, 'dropout': 0.23431053489532608, 'batch_size': 16, 'optimizer': 'adafactor', 'alpha': 0.597361966731591, 'beta': 0.7752529118444957, 'gamma': 0.31009532090001946}. Best is trial 0 with value: 0.9063826629063383.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1260' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1260/1260 06:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.425249</td>\n",
       "      <td>0.856544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.272718</td>\n",
       "      <td>0.866638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.221397</td>\n",
       "      <td>0.891800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.269385</td>\n",
       "      <td>0.897060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.274431</td>\n",
       "      <td>0.902339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.279569</td>\n",
       "      <td>0.911662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.246802</td>\n",
       "      <td>0.921316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>0.252323</td>\n",
       "      <td>0.925487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.260392</td>\n",
       "      <td>0.928145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>0.253440</td>\n",
       "      <td>0.928288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 10:40:43,408] Trial 1 finished with value: 0.9282881792038747 and parameters: {'learning_rate': 3.009700608642199e-06, 'weight_decay': 0.054195791231730295, 'dropout': 0.17828510897651312, 'batch_size': 16, 'optimizer': 'adafactor', 'alpha': 0.853347501921317, 'beta': 0.8596416717331209, 'gamma': 0.5576260587234492}. Best is trial 1 with value: 0.9282881792038747.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 05:26 < 02:20, 2.70 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>0.341810</td>\n",
       "      <td>0.805558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.473614</td>\n",
       "      <td>0.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.487202</td>\n",
       "      <td>0.847681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.146100</td>\n",
       "      <td>0.508948</td>\n",
       "      <td>0.866765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.365936</td>\n",
       "      <td>0.908972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.108200</td>\n",
       "      <td>0.522104</td>\n",
       "      <td>0.881652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.530354</td>\n",
       "      <td>0.885454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [127/127 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 10:46:14,564] Trial 2 finished with value: 0.908971521322119 and parameters: {'learning_rate': 7.177808120969473e-06, 'weight_decay': 0.06121948539059389, 'dropout': 0.22713930389115525, 'batch_size': 4, 'optimizer': 'adafactor', 'alpha': 0.5971304618752904, 'beta': 0.5862500017035467, 'gamma': 0.22585692639078056}. Best is trial 1 with value: 0.9282881792038747.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1260' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1260/1260 06:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>0.579235</td>\n",
       "      <td>0.719670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.179300</td>\n",
       "      <td>0.640517</td>\n",
       "      <td>0.761290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0.705167</td>\n",
       "      <td>0.790469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.152800</td>\n",
       "      <td>0.753858</td>\n",
       "      <td>0.793510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.505713</td>\n",
       "      <td>0.852373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.112900</td>\n",
       "      <td>0.541724</td>\n",
       "      <td>0.853554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>0.639955</td>\n",
       "      <td>0.835041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.513702</td>\n",
       "      <td>0.867986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>0.577067</td>\n",
       "      <td>0.863122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.657604</td>\n",
       "      <td>0.850021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 10:52:50,057] Trial 3 finished with value: 0.8679864731576723 and parameters: {'learning_rate': 1.0838405213062654e-05, 'weight_decay': 0.07384593136119565, 'dropout': 0.2851351702731366, 'batch_size': 16, 'optimizer': 'adam', 'alpha': 0.5099988932169416, 'beta': 0.46602112475360447, 'gamma': 0.20378431228525462}. Best is trial 1 with value: 0.9282881792038747.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 630/1260 03:50 < 03:51, 2.73 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.377200</td>\n",
       "      <td>0.374619</td>\n",
       "      <td>0.604754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.374200</td>\n",
       "      <td>0.371658</td>\n",
       "      <td>0.653223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.355400</td>\n",
       "      <td>0.366242</td>\n",
       "      <td>0.735934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.279600</td>\n",
       "      <td>0.363183</td>\n",
       "      <td>0.717999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.365502</td>\n",
       "      <td>0.717999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [127/127 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 10:56:45,493] Trial 4 finished with value: 0.7359343404305936 and parameters: {'learning_rate': 6.616112390720269e-06, 'weight_decay': 0.07534765944627048, 'dropout': 0.39374629231962144, 'batch_size': 4, 'optimizer': 'adamw', 'alpha': 0.19814800252159454, 'beta': 0.49175242940733077, 'gamma': 0.8803400317423989}. Best is trial 1 with value: 0.9282881792038747.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 05:10 < 02:13, 2.84 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.208500</td>\n",
       "      <td>0.535514</td>\n",
       "      <td>0.841663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.436799</td>\n",
       "      <td>0.870439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.145500</td>\n",
       "      <td>0.459389</td>\n",
       "      <td>0.850213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.449949</td>\n",
       "      <td>0.890600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.190799</td>\n",
       "      <td>0.937851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.252658</td>\n",
       "      <td>0.935191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.279161</td>\n",
       "      <td>0.932198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:01:59,529] Trial 5 finished with value: 0.9378505381233895 and parameters: {'learning_rate': 4.549529213104616e-05, 'weight_decay': 0.0829683208215856, 'dropout': 0.2866630083160525, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.6249001681577615, 'beta': 0.20669767553013318, 'gamma': 0.9599323762619149}. Best is trial 5 with value: 0.9378505381233895.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 05:52 < 02:31, 2.49 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.446527</td>\n",
       "      <td>0.856951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.155400</td>\n",
       "      <td>0.542662</td>\n",
       "      <td>0.859353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.144900</td>\n",
       "      <td>0.485954</td>\n",
       "      <td>0.889288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.703225</td>\n",
       "      <td>0.865547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.294528</td>\n",
       "      <td>0.939469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.468369</td>\n",
       "      <td>0.928004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.396787</td>\n",
       "      <td>0.933773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [127/127 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:07:56,564] Trial 6 finished with value: 0.9394692694829954 and parameters: {'learning_rate': 3.787092008680818e-05, 'weight_decay': 0.056466871711220536, 'dropout': 0.28479637092923865, 'batch_size': 4, 'optimizer': 'adamw', 'alpha': 0.5538943118733821, 'beta': 0.7937839045911508, 'gamma': 0.3665039754563816}. Best is trial 6 with value: 0.9394692694829954.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 756/1260 04:05 < 02:44, 3.07 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.423300</td>\n",
       "      <td>0.417652</td>\n",
       "      <td>0.196664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.420100</td>\n",
       "      <td>0.416377</td>\n",
       "      <td>0.420575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.414100</td>\n",
       "      <td>0.415316</td>\n",
       "      <td>0.790574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.413890</td>\n",
       "      <td>0.816614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.407500</td>\n",
       "      <td>0.412429</td>\n",
       "      <td>0.768380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.397900</td>\n",
       "      <td>0.411377</td>\n",
       "      <td>0.722190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:12:05,983] Trial 7 finished with value: 0.8166137712156755 and parameters: {'learning_rate': 2.5165359315832724e-06, 'weight_decay': 0.06532160985002121, 'dropout': 0.3915486704357253, 'batch_size': 16, 'optimizer': 'adam', 'alpha': 0.4233933082617324, 'beta': 0.6523318905333129, 'gamma': 0.7930005915951991}. Best is trial 6 with value: 0.9394692694829954.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 504/1260 03:13 < 04:52, 2.59 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.411441</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.393133</td>\n",
       "      <td>0.784027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.285400</td>\n",
       "      <td>0.411966</td>\n",
       "      <td>0.732449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.457871</td>\n",
       "      <td>0.752036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [127/127 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:15:24,288] Trial 8 finished with value: 0.7840269823087693 and parameters: {'learning_rate': 4.927810163803123e-06, 'weight_decay': 0.056645539359289176, 'dropout': 0.33858582249138414, 'batch_size': 4, 'optimizer': 'adafactor', 'alpha': 0.6001185099055985, 'beta': 0.6739455890680637, 'gamma': 0.7020488686359849}. Best is trial 6 with value: 0.9394692694829954.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1008' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1008/1260 06:11 < 01:32, 2.71 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0.196697</td>\n",
       "      <td>0.867742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.222315</td>\n",
       "      <td>0.928578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073900</td>\n",
       "      <td>0.228987</td>\n",
       "      <td>0.943787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.233446</td>\n",
       "      <td>0.946920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.168469</td>\n",
       "      <td>0.944541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.218621</td>\n",
       "      <td>0.948868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.219459</td>\n",
       "      <td>0.946615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.282285</td>\n",
       "      <td>0.942767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:21:39,423] Trial 9 finished with value: 0.9488684251599813 and parameters: {'learning_rate': 2.1250381199355784e-05, 'weight_decay': 0.010087099668118006, 'dropout': 0.12028486429781438, 'batch_size': 8, 'optimizer': 'adafactor', 'alpha': 0.4239858172070935, 'beta': 0.2271897222399163, 'gamma': 0.6573296214127052}. Best is trial 9 with value: 0.9488684251599813.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 378/1260 02:13 < 05:14, 2.81 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.450064</td>\n",
       "      <td>0.723737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.445100</td>\n",
       "      <td>0.444797</td>\n",
       "      <td>0.606256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.438600</td>\n",
       "      <td>0.431362</td>\n",
       "      <td>0.688295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:23:57,244] Trial 10 finished with value: 0.7237373119726062 and parameters: {'learning_rate': 1.1762844565160826e-06, 'weight_decay': 0.01475025638258725, 'dropout': 0.13328135858040763, 'batch_size': 8, 'optimizer': 'adafactor', 'alpha': 0.11203049291057476, 'beta': 0.1049550018704997, 'gamma': 0.55634000490708}. Best is trial 9 with value: 0.9488684251599813.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 378/1260 02:07 < 04:57, 2.96 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.136100</td>\n",
       "      <td>0.092232</td>\n",
       "      <td>0.948356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.218658</td>\n",
       "      <td>0.929846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.292307</td>\n",
       "      <td>0.933773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:26:07,934] Trial 11 finished with value: 0.9483563601715103 and parameters: {'learning_rate': 4.6331068197531675e-05, 'weight_decay': 0.03357337870686029, 'dropout': 0.11009530479336369, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.3627958768033778, 'beta': 0.9940182068088056, 'gamma': 0.3987011705663011}. Best is trial 9 with value: 0.9488684251599813.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 630/1260 03:38 < 03:38, 2.88 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.095718</td>\n",
       "      <td>0.939337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>0.162910</td>\n",
       "      <td>0.942554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.187265</td>\n",
       "      <td>0.952804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.238567</td>\n",
       "      <td>0.948384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.270837</td>\n",
       "      <td>0.950101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:29:49,979] Trial 12 finished with value: 0.9528041012894205 and parameters: {'learning_rate': 1.8632438861735703e-05, 'weight_decay': 0.029892194284602183, 'dropout': 0.11890257882690083, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.33052306843408913, 'beta': 0.9902018543508228, 'gamma': 0.41896837326066355}. Best is trial 12 with value: 0.9528041012894205.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1134' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1134/1260 06:51 < 00:45, 2.75 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.141000</td>\n",
       "      <td>0.110851</td>\n",
       "      <td>0.937418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.138558</td>\n",
       "      <td>0.944670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.146802</td>\n",
       "      <td>0.945687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.211593</td>\n",
       "      <td>0.942554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.163115</td>\n",
       "      <td>0.949362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.214410</td>\n",
       "      <td>0.945687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.202383</td>\n",
       "      <td>0.950101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.268232</td>\n",
       "      <td>0.941313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.229026</td>\n",
       "      <td>0.950101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:36:45,526] Trial 13 finished with value: 0.9501011358332038 and parameters: {'learning_rate': 1.781616324283868e-05, 'weight_decay': 0.031345229972571935, 'dropout': 0.15601565415454557, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.27968875472341753, 'beta': 0.3345910703896096, 'gamma': 0.6605740300781494}. Best is trial 12 with value: 0.9528041012894205.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 378/1260 02:18 < 05:26, 2.71 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.197300</td>\n",
       "      <td>0.129851</td>\n",
       "      <td>0.939608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.160325</td>\n",
       "      <td>0.936612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.256323</td>\n",
       "      <td>0.930951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:39:08,822] Trial 14 finished with value: 0.9396080290817134 and parameters: {'learning_rate': 1.5683379472828366e-05, 'weight_decay': 0.03351112149336103, 'dropout': 0.16961362226316987, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.268138130967001, 'beta': 0.3157436793564528, 'gamma': 0.4646121629443363}. Best is trial 12 with value: 0.9528041012894205.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 05:19 < 02:17, 2.75 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.118411</td>\n",
       "      <td>0.936647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>0.159630</td>\n",
       "      <td>0.933942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.094000</td>\n",
       "      <td>0.197088</td>\n",
       "      <td>0.945236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.300757</td>\n",
       "      <td>0.917091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.173076</td>\n",
       "      <td>0.952804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.187171</td>\n",
       "      <td>0.949853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.223870</td>\n",
       "      <td>0.944005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:44:32,736] Trial 15 finished with value: 0.9528041012894205 and parameters: {'learning_rate': 1.2765491189446075e-05, 'weight_decay': 0.03553663567173072, 'dropout': 0.16686516013976885, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.25042544209192463, 'beta': 0.39919692409657115, 'gamma': 0.6688678494315048}. Best is trial 12 with value: 0.9528041012894205.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 05:10 < 02:13, 2.83 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.320800</td>\n",
       "      <td>0.226219</td>\n",
       "      <td>0.917183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.132300</td>\n",
       "      <td>0.270778</td>\n",
       "      <td>0.907681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.235142</td>\n",
       "      <td>0.941313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.451770</td>\n",
       "      <td>0.904965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.273942</td>\n",
       "      <td>0.946688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>0.325715</td>\n",
       "      <td>0.935191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.359432</td>\n",
       "      <td>0.938039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:49:47,391] Trial 16 finished with value: 0.9466881819578916 and parameters: {'learning_rate': 1.153646260117093e-05, 'weight_decay': 0.041760925797268214, 'dropout': 0.19625153414787866, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.7849712813262821, 'beta': 0.4119536171596875, 'gamma': 0.4553704815820422}. Best is trial 12 with value: 0.9528041012894205.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 504/1260 02:55 < 04:24, 2.86 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.169300</td>\n",
       "      <td>0.178525</td>\n",
       "      <td>0.917462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.097400</td>\n",
       "      <td>0.181052</td>\n",
       "      <td>0.941108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092600</td>\n",
       "      <td>0.252491</td>\n",
       "      <td>0.918458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.333167</td>\n",
       "      <td>0.915728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:52:46,601] Trial 17 finished with value: 0.9411078717201167 and parameters: {'learning_rate': 3.009553163647432e-05, 'weight_decay': 0.024003652712486147, 'dropout': 0.20125597682425103, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.11706350983025393, 'beta': 0.6020613733803473, 'gamma': 0.7921686656164764}. Best is trial 12 with value: 0.9528041012894205.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 05:23 < 02:19, 2.72 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.140711</td>\n",
       "      <td>0.925121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.182824</td>\n",
       "      <td>0.932522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.094700</td>\n",
       "      <td>0.214532</td>\n",
       "      <td>0.935367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.338223</td>\n",
       "      <td>0.923969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.190894</td>\n",
       "      <td>0.944896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.270022</td>\n",
       "      <td>0.942344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.300865</td>\n",
       "      <td>0.942344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 11:58:14,467] Trial 18 finished with value: 0.9448957189901208 and parameters: {'learning_rate': 1.0520094119839607e-05, 'weight_decay': 0.04439264819904251, 'dropout': 0.1541230319322938, 'batch_size': 8, 'optimizer': 'adam', 'alpha': 0.30685698428583535, 'beta': 0.9931359977090448, 'gamma': 0.27930180157676987}. Best is trial 12 with value: 0.9528041012894205.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 756/1260 04:35 < 03:03, 2.74 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.075176</td>\n",
       "      <td>0.941738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>0.170852</td>\n",
       "      <td>0.935191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.188590</td>\n",
       "      <td>0.952804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.185075</td>\n",
       "      <td>0.956332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.235149</td>\n",
       "      <td>0.945821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.323597</td>\n",
       "      <td>0.954019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:02:53,570] Trial 19 finished with value: 0.9563316029573699 and parameters: {'learning_rate': 2.9865068164223167e-05, 'weight_decay': 0.0016072887084906187, 'dropout': 0.10022887131620678, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.22242829692840554, 'beta': 0.866811919878682, 'gamma': 0.10232562737172662}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 504/1260 03:00 < 04:31, 2.78 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.110170</td>\n",
       "      <td>0.933274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.111700</td>\n",
       "      <td>0.171224</td>\n",
       "      <td>0.939666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.349836</td>\n",
       "      <td>0.926885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.450078</td>\n",
       "      <td>0.935016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:05:57,984] Trial 20 finished with value: 0.9396660027577755 and parameters: {'learning_rate': 2.657866735276271e-05, 'weight_decay': 0.018986840601693363, 'dropout': 0.10200724271704315, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.4517102102966484, 'beta': 0.8991729743976107, 'gamma': 0.10703678633192987}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 504/1260 02:50 < 04:16, 2.95 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.132810</td>\n",
       "      <td>0.930622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>0.157483</td>\n",
       "      <td>0.939666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.264823</td>\n",
       "      <td>0.924092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.334716</td>\n",
       "      <td>0.932360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:08:51,987] Trial 21 finished with value: 0.9396660027577755 and parameters: {'learning_rate': 1.4010655245373171e-05, 'weight_decay': 0.02494447820574957, 'dropout': 0.13739014971615277, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.20252917826647054, 'beta': 0.8938740318844055, 'gamma': 0.10772433377123239}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 756/1260 04:23 < 02:56, 2.86 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.121366</td>\n",
       "      <td>0.932750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.206946</td>\n",
       "      <td>0.918361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.225556</td>\n",
       "      <td>0.935016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.184410</td>\n",
       "      <td>0.955774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.183880</td>\n",
       "      <td>0.952363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.222397</td>\n",
       "      <td>0.951118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:13:19,789] Trial 22 finished with value: 0.9557737260402057 and parameters: {'learning_rate': 3.026183661160268e-05, 'weight_decay': 0.04282950374876584, 'dropout': 0.1361971652356247, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.20743867752667197, 'beta': 0.7598821850964521, 'gamma': 0.5129749722981987}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 504/1260 02:46 < 04:10, 3.01 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.127218</td>\n",
       "      <td>0.942588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>0.229446</td>\n",
       "      <td>0.944446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.374635</td>\n",
       "      <td>0.929398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.278537</td>\n",
       "      <td>0.940904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:16:10,661] Trial 23 finished with value: 0.9444461705763555 and parameters: {'learning_rate': 2.920670768500058e-05, 'weight_decay': 0.09997403179571199, 'dropout': 0.13195191863769834, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.9865987223854042, 'beta': 0.7573673166914507, 'gamma': 0.4787159370650708}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 378/1260 02:14 < 05:16, 2.79 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.109645</td>\n",
       "      <td>0.939130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>0.148465</td>\n",
       "      <td>0.938039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.364672</td>\n",
       "      <td>0.928145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:18:29,313] Trial 24 finished with value: 0.9391304347826087 and parameters: {'learning_rate': 3.329397631322775e-05, 'weight_decay': 0.00041091001006351145, 'dropout': 0.10057767514193772, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.1790399699079894, 'beta': 0.931470639242957, 'gamma': 0.18959300616956248}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 378/1260 02:12 < 05:11, 2.83 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.150303</td>\n",
       "      <td>0.939205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.118700</td>\n",
       "      <td>0.235970</td>\n",
       "      <td>0.929546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.415241</td>\n",
       "      <td>0.903638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:20:46,218] Trial 25 finished with value: 0.9392046344136529 and parameters: {'learning_rate': 2.184046693547225e-05, 'weight_decay': 0.001339548206721504, 'dropout': 0.19920564381981326, 'batch_size': 8, 'optimizer': 'adam', 'alpha': 0.3453922099967031, 'beta': 0.822601294188412, 'gamma': 0.3497558182945801}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1134' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1134/1260 06:51 < 00:45, 2.75 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>0.126789</td>\n",
       "      <td>0.913450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.116900</td>\n",
       "      <td>0.320449</td>\n",
       "      <td>0.903607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.226206</td>\n",
       "      <td>0.935545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.265673</td>\n",
       "      <td>0.947868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.252353</td>\n",
       "      <td>0.943416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.250273</td>\n",
       "      <td>0.948111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.248948</td>\n",
       "      <td>0.948604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.298834</td>\n",
       "      <td>0.944896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.316457</td>\n",
       "      <td>0.946147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:27:41,789] Trial 26 finished with value: 0.9486037976703368 and parameters: {'learning_rate': 4.778494613294112e-05, 'weight_decay': 0.04773522780002037, 'dropout': 0.13440960426840526, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.3710883401180417, 'beta': 0.7356009313599869, 'gamma': 0.5964918702188089}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 630/1260 03:55 < 03:55, 2.67 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.213300</td>\n",
       "      <td>0.099931</td>\n",
       "      <td>0.931434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.127756</td>\n",
       "      <td>0.942767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.099700</td>\n",
       "      <td>0.166410</td>\n",
       "      <td>0.945236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.303551</td>\n",
       "      <td>0.923969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.179278</td>\n",
       "      <td>0.942155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:31:41,556] Trial 27 finished with value: 0.9452355378241523 and parameters: {'learning_rate': 8.26110488158762e-06, 'weight_decay': 0.024875184796696256, 'dropout': 0.14294996417914715, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.16175488062363214, 'beta': 0.9478920986543298, 'gamma': 0.4339746373396832}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1008' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1008/1260 06:29 < 01:37, 2.58 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.176900</td>\n",
       "      <td>0.505366</td>\n",
       "      <td>0.843980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.129500</td>\n",
       "      <td>0.343540</td>\n",
       "      <td>0.890574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.134800</td>\n",
       "      <td>0.377296</td>\n",
       "      <td>0.904965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.100700</td>\n",
       "      <td>0.557475</td>\n",
       "      <td>0.884183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.245352</td>\n",
       "      <td>0.942344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.270966</td>\n",
       "      <td>0.946688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.429702</td>\n",
       "      <td>0.910255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.301660</td>\n",
       "      <td>0.936612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [127/127 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:38:15,886] Trial 28 finished with value: 0.9466881819578916 and parameters: {'learning_rate': 2.468835999150977e-05, 'weight_decay': 0.040324638733319526, 'dropout': 0.25618484482066106, 'batch_size': 4, 'optimizer': 'adamw', 'alpha': 0.22959517335040475, 'beta': 0.708080143897567, 'gamma': 0.513326325279915}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 504/1260 03:02 < 04:34, 2.76 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.218800</td>\n",
       "      <td>0.254349</td>\n",
       "      <td>0.891800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.313339</td>\n",
       "      <td>0.910315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.495285</td>\n",
       "      <td>0.880392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>0.492011</td>\n",
       "      <td>0.898367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:41:22,081] Trial 29 finished with value: 0.9103147628195656 and parameters: {'learning_rate': 2.0571573823986107e-05, 'weight_decay': 0.007540846882699229, 'dropout': 0.22382014912432316, 'batch_size': 16, 'optimizer': 'adam', 'alpha': 0.32869156533916966, 'beta': 0.8359874213241761, 'gamma': 0.28956113587672644}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1008' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1008/1260 05:03 < 01:16, 3.31 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.370600</td>\n",
       "      <td>0.185400</td>\n",
       "      <td>0.904202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>0.161545</td>\n",
       "      <td>0.928725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.125800</td>\n",
       "      <td>0.171434</td>\n",
       "      <td>0.938614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.920039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.246838</td>\n",
       "      <td>0.940065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.210757</td>\n",
       "      <td>0.942155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>0.265257</td>\n",
       "      <td>0.940065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.253396</td>\n",
       "      <td>0.941521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:46:29,413] Trial 30 finished with value: 0.9421548610020419 and parameters: {'learning_rate': 4.989817328823723e-06, 'weight_decay': 0.01757252724760748, 'dropout': 0.11487150587590814, 'batch_size': 16, 'optimizer': 'adamw', 'alpha': 0.49907341531356064, 'beta': 0.8064264346243633, 'gamma': 0.33932523363787903}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 504/1260 02:50 < 04:16, 2.94 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>0.175204</td>\n",
       "      <td>0.913385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.125402</td>\n",
       "      <td>0.940471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.212763</td>\n",
       "      <td>0.935191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.357664</td>\n",
       "      <td>0.903638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:49:23,695] Trial 31 finished with value: 0.9404713594506009 and parameters: {'learning_rate': 1.565493824273104e-05, 'weight_decay': 0.037852085815846495, 'dropout': 0.17311335907151332, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.2308200082377912, 'beta': 0.5308901419322052, 'gamma': 0.7839070263328468}. Best is trial 19 with value: 0.9563316029573699.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 04:53 < 02:06, 3.00 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.139600</td>\n",
       "      <td>0.146688</td>\n",
       "      <td>0.931746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.183843</td>\n",
       "      <td>0.933020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.265658</td>\n",
       "      <td>0.935191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.307221</td>\n",
       "      <td>0.939469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.157845</td>\n",
       "      <td>0.957480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.253077</td>\n",
       "      <td>0.946380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.351060</td>\n",
       "      <td>0.939865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:54:21,607] Trial 32 finished with value: 0.9574795574795575 and parameters: {'learning_rate': 3.815511213035392e-05, 'weight_decay': 0.051388116353085767, 'dropout': 0.16100793342183561, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.26339849847132984, 'beta': 0.8757046797807218, 'gamma': 0.6124588825878332}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 504/1260 02:59 < 04:30, 2.80 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.197914</td>\n",
       "      <td>0.921316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.108200</td>\n",
       "      <td>0.178863</td>\n",
       "      <td>0.946920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.293454</td>\n",
       "      <td>0.922469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.251436</td>\n",
       "      <td>0.943787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 12:57:25,589] Trial 33 finished with value: 0.9469198703103289 and parameters: {'learning_rate': 3.653907909266445e-05, 'weight_decay': 0.0471883077559965, 'dropout': 0.12131374035801772, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.1457422013504015, 'beta': 0.8645683421768113, 'gamma': 0.5594948402751784}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 756/1260 04:34 < 03:03, 2.75 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>0.143704</td>\n",
       "      <td>0.934636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>0.160342</td>\n",
       "      <td>0.932853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.074400</td>\n",
       "      <td>0.201511</td>\n",
       "      <td>0.938229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.211375</td>\n",
       "      <td>0.952804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.191668</td>\n",
       "      <td>0.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.282876</td>\n",
       "      <td>0.937355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:02:04,017] Trial 34 finished with value: 0.9528041012894205 and parameters: {'learning_rate': 3.8468688854592974e-05, 'weight_decay': 0.06589577193799397, 'dropout': 0.18321531041582736, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.2829505124864895, 'beta': 0.9357118644542282, 'gamma': 0.6129809130091525}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 756/1260 04:03 < 02:42, 3.09 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.180762</td>\n",
       "      <td>0.910642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.261274</td>\n",
       "      <td>0.931423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.359907</td>\n",
       "      <td>0.929398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.313245</td>\n",
       "      <td>0.946688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.283310</td>\n",
       "      <td>0.943198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.339344</td>\n",
       "      <td>0.945916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:06:11,544] Trial 35 finished with value: 0.9466881819578916 and parameters: {'learning_rate': 2.5992790710686547e-05, 'weight_decay': 0.05153887501279593, 'dropout': 0.14460750521438442, 'batch_size': 16, 'optimizer': 'adafactor', 'alpha': 0.6742086232839084, 'beta': 0.8674700432414573, 'gamma': 0.5112413759435646}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 504/1260 03:05 < 04:39, 2.71 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.163500</td>\n",
       "      <td>0.138532</td>\n",
       "      <td>0.923677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>0.166572</td>\n",
       "      <td>0.942767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.269268</td>\n",
       "      <td>0.926614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.346841</td>\n",
       "      <td>0.935191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [127/127 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:09:21,069] Trial 36 finished with value: 0.9427667129844065 and parameters: {'learning_rate': 1.7906677303500426e-05, 'weight_decay': 0.028076804323208623, 'dropout': 0.15563037909146424, 'batch_size': 4, 'optimizer': 'adamw', 'alpha': 0.40320578817229613, 'beta': 0.9971013551766589, 'gamma': 0.7291657059321871}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 630/1260 03:27 < 03:28, 3.02 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.128674</td>\n",
       "      <td>0.928171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.101700</td>\n",
       "      <td>0.242223</td>\n",
       "      <td>0.919726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.242161</td>\n",
       "      <td>0.941521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.283856</td>\n",
       "      <td>0.937736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.452069</td>\n",
       "      <td>0.927302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:12:52,682] Trial 37 finished with value: 0.9415208301068608 and parameters: {'learning_rate': 3.8828107417309194e-05, 'weight_decay': 0.0607969682223629, 'dropout': 0.11965814602444172, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.196761688957711, 'beta': 0.7628177371444488, 'gamma': 0.23519103546260803}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 05:06 < 02:11, 2.88 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.167200</td>\n",
       "      <td>0.468055</td>\n",
       "      <td>0.891842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.146100</td>\n",
       "      <td>0.659626</td>\n",
       "      <td>0.875387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.140200</td>\n",
       "      <td>0.249199</td>\n",
       "      <td>0.943787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.545151</td>\n",
       "      <td>0.907632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.291394</td>\n",
       "      <td>0.945687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>0.373822</td>\n",
       "      <td>0.942767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.333122</td>\n",
       "      <td>0.942767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:18:02,649] Trial 38 finished with value: 0.9456868514790151 and parameters: {'learning_rate': 3.0297498555496516e-05, 'weight_decay': 0.052097413596805454, 'dropout': 0.24418293469129282, 'batch_size': 8, 'optimizer': 'adam', 'alpha': 0.4801883924190455, 'beta': 0.6287448447674558, 'gamma': 0.154812552239484}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1008' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1008/1260 06:42 < 01:40, 2.50 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.334073</td>\n",
       "      <td>0.816997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.225485</td>\n",
       "      <td>0.893126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.267051</td>\n",
       "      <td>0.900995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>0.423037</td>\n",
       "      <td>0.867986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.263219</td>\n",
       "      <td>0.921205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.263767</td>\n",
       "      <td>0.922585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.306933</td>\n",
       "      <td>0.915728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.289594</td>\n",
       "      <td>0.921205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [127/127 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:24:49,337] Trial 39 finished with value: 0.9225849932604464 and parameters: {'learning_rate': 8.491101169443446e-06, 'weight_decay': 0.07367411084573137, 'dropout': 0.21632071208901982, 'batch_size': 4, 'optimizer': 'adamw', 'alpha': 0.31103511793118066, 'beta': 0.6985861876795048, 'gamma': 0.9663776694794113}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 630/1260 03:41 < 03:42, 2.83 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.305751</td>\n",
       "      <td>0.702575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.307700</td>\n",
       "      <td>0.304441</td>\n",
       "      <td>0.706520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>0.302010</td>\n",
       "      <td>0.711395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.300130</td>\n",
       "      <td>0.702509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.243700</td>\n",
       "      <td>0.300500</td>\n",
       "      <td>0.703308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:28:34,709] Trial 40 finished with value: 0.7113947338162585 and parameters: {'learning_rate': 2.49393737070424e-06, 'weight_decay': 0.08741594920684123, 'dropout': 0.3350477185524179, 'batch_size': 16, 'optimizer': 'adafactor', 'alpha': 0.14836326239001435, 'beta': 0.944913423762054, 'gamma': 0.40677297807076657}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 504/1260 02:48 < 04:14, 2.98 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.189800</td>\n",
       "      <td>0.130744</td>\n",
       "      <td>0.933359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>0.151409</td>\n",
       "      <td>0.946688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091500</td>\n",
       "      <td>0.209979</td>\n",
       "      <td>0.933942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.302523</td>\n",
       "      <td>0.911662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:31:27,267] Trial 41 finished with value: 0.9466881819578916 and parameters: {'learning_rate': 1.1802152293679859e-05, 'weight_decay': 0.03535587129705662, 'dropout': 0.1655284733626048, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.24117747835350184, 'beta': 0.4182616606863881, 'gamma': 0.8932476189429255}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 504/1260 02:50 < 04:16, 2.95 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.177000</td>\n",
       "      <td>0.157115</td>\n",
       "      <td>0.931909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.184916</td>\n",
       "      <td>0.932687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>0.332718</td>\n",
       "      <td>0.907632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.249442</td>\n",
       "      <td>0.932360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:34:21,548] Trial 42 finished with value: 0.9326867267450741 and parameters: {'learning_rate': 2.194574726473363e-05, 'weight_decay': 0.04373517342951523, 'dropout': 0.18534253183976135, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.2450169674246421, 'beta': 0.53719589157763, 'gamma': 0.6233963295871923}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 04:56 < 02:07, 2.97 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.121583</td>\n",
       "      <td>0.930781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>0.146168</td>\n",
       "      <td>0.933020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.225001</td>\n",
       "      <td>0.928288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.258577</td>\n",
       "      <td>0.933773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.155744</td>\n",
       "      <td>0.948356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.172640</td>\n",
       "      <td>0.948356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.213946</td>\n",
       "      <td>0.943198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:39:21,722] Trial 43 finished with value: 0.9483563601715103 and parameters: {'learning_rate': 1.7706389051314666e-05, 'weight_decay': 0.00832724998570375, 'dropout': 0.12760467134935768, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.11169476944977343, 'beta': 0.8995096189053036, 'gamma': 0.7091172415842356}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 04:40 < 02:00, 3.13 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>0.117082</td>\n",
       "      <td>0.934057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.171967</td>\n",
       "      <td>0.939666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.182041</td>\n",
       "      <td>0.940904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.305252</td>\n",
       "      <td>0.930951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.251364</td>\n",
       "      <td>0.945460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.251757</td>\n",
       "      <td>0.941108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.322854</td>\n",
       "      <td>0.932360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:44:06,418] Trial 44 finished with value: 0.9454601510713735 and parameters: {'learning_rate': 1.2816986919071366e-05, 'weight_decay': 0.05569841014764524, 'dropout': 0.1499901591179875, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.380603538136813, 'beta': 0.8042052019997257, 'gamma': 0.6567225221285216}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 630/1260 03:18 < 03:19, 3.16 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>0.106366</td>\n",
       "      <td>0.934783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.184519</td>\n",
       "      <td>0.939865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.177747</td>\n",
       "      <td>0.950101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.262115</td>\n",
       "      <td>0.944005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.285185</td>\n",
       "      <td>0.946380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:47:29,060] Trial 45 finished with value: 0.9501011358332038 and parameters: {'learning_rate': 4.27437804883403e-05, 'weight_decay': 0.029333574223836142, 'dropout': 0.10108210709495126, 'batch_size': 8, 'optimizer': 'adamw', 'alpha': 0.556758774128773, 'beta': 0.29134951847400536, 'gamma': 0.5265889243677025}. Best is trial 32 with value: 0.9574795574795575.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='882' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 882/1260 04:46 < 02:03, 3.07 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.154035</td>\n",
       "      <td>0.931909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.184302</td>\n",
       "      <td>0.936273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.250706</td>\n",
       "      <td>0.936612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.216747</td>\n",
       "      <td>0.953067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.233318</td>\n",
       "      <td>0.957551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.204848</td>\n",
       "      <td>0.951118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.249622</td>\n",
       "      <td>0.947628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:52:19,297] Trial 46 finished with value: 0.9575505723694528 and parameters: {'learning_rate': 4.934131267755679e-05, 'weight_decay': 0.061263994446749105, 'dropout': 0.16388573237298298, 'batch_size': 8, 'optimizer': 'adafactor', 'alpha': 0.28743023556405844, 'beta': 0.4450143152674694, 'gamma': 0.8679861936714183}. Best is trial 46 with value: 0.9575505723694528.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1260' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1260/1260 07:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.135553</td>\n",
       "      <td>0.920482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.159754</td>\n",
       "      <td>0.940677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.204292</td>\n",
       "      <td>0.945916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.194135</td>\n",
       "      <td>0.950605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.207398</td>\n",
       "      <td>0.949362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.181417</td>\n",
       "      <td>0.957838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.206876</td>\n",
       "      <td>0.954830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.182867</td>\n",
       "      <td>0.960866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.215842</td>\n",
       "      <td>0.957551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199334</td>\n",
       "      <td>0.960563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 13:59:24,536] Trial 47 finished with value: 0.960866129287182 and parameters: {'learning_rate': 4.864660682114375e-05, 'weight_decay': 0.0655662598204451, 'dropout': 0.11422530525650049, 'batch_size': 8, 'optimizer': 'adafactor', 'alpha': 0.2968862216256336, 'beta': 0.4658241878139173, 'gamma': 0.9127074033921648}. Best is trial 47 with value: 0.960866129287182.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 378/1260 02:23 < 05:36, 2.62 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.164080</td>\n",
       "      <td>0.919372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.389822</td>\n",
       "      <td>0.885405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.551199</td>\n",
       "      <td>0.863262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [127/127 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 14:01:52,252] Trial 48 finished with value: 0.9193723784371602 and parameters: {'learning_rate': 4.742685183934284e-05, 'weight_decay': 0.06977885499334353, 'dropout': 0.2643994633130027, 'batch_size': 4, 'optimizer': 'adafactor', 'alpha': 0.28847372469449883, 'beta': 0.44969324271124234, 'gamma': 0.9031198286473973}. Best is trial 47 with value: 0.960866129287182.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 378/1260 02:10 < 05:05, 2.89 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.238500</td>\n",
       "      <td>0.257199</td>\n",
       "      <td>0.901014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.159700</td>\n",
       "      <td>0.436239</td>\n",
       "      <td>0.870439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.195900</td>\n",
       "      <td>0.377880</td>\n",
       "      <td>0.893156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 14:04:06,793] Trial 49 finished with value: 0.9010136624063464 and parameters: {'learning_rate': 3.537198177825379e-05, 'weight_decay': 0.06077410533640796, 'dropout': 0.3130320332242272, 'batch_size': 8, 'optimizer': 'adafactor', 'alpha': 0.7016187242237519, 'beta': 0.5004658047352122, 'gamma': 0.9974565072391748}. Best is trial 47 with value: 0.960866129287182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  F1_beta: 0.9609\n",
      "  Best hyperparameters: \n",
      "    learning_rate: 4.864660682114375e-05\n",
      "    weight_decay: 0.0655662598204451\n",
      "    dropout: 0.11422530525650049\n",
      "    batch_size: 8\n",
      "    optimizer: adafactor\n",
      "    alpha: 0.2968862216256336\n",
      "    beta: 0.4658241878139173\n",
      "    gamma: 0.9127074033921648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 378/1260 02:17 < 05:23, 2.73 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.168528</td>\n",
       "      <td>0.948356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.265587</td>\n",
       "      <td>0.947154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.929695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best model and tokenizer saved to models/final_model\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Best threshold found on validation set: 0.51 (Fβ = 0.9514)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test F1 on old valid.csv using threshold 0.51 = 0.9524\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.96      0.95      0.96       420\n",
      "     Class 1       0.95      0.96      0.95       366\n",
      "\n",
      "    accuracy                           0.96       786\n",
      "   macro avg       0.96      0.96      0.96       786\n",
      "weighted avg       0.96      0.96      0.96       786\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49
         ],
         "y": [
          0.9063826629063383,
          0.9282881792038747,
          0.908971521322119,
          0.8679864731576723,
          0.7359343404305936,
          0.9378505381233895,
          0.9394692694829954,
          0.8166137712156755,
          0.7840269823087693,
          0.9488684251599813,
          0.7237373119726062,
          0.9483563601715103,
          0.9528041012894205,
          0.9501011358332038,
          0.9396080290817134,
          0.9528041012894205,
          0.9466881819578916,
          0.9411078717201167,
          0.9448957189901208,
          0.9563316029573699,
          0.9396660027577755,
          0.9396660027577755,
          0.9557737260402057,
          0.9444461705763555,
          0.9391304347826087,
          0.9392046344136529,
          0.9486037976703368,
          0.9452355378241523,
          0.9466881819578916,
          0.9103147628195656,
          0.9421548610020419,
          0.9404713594506009,
          0.9574795574795575,
          0.9469198703103289,
          0.9528041012894205,
          0.9466881819578916,
          0.9427667129844065,
          0.9415208301068608,
          0.9456868514790151,
          0.9225849932604464,
          0.7113947338162585,
          0.9466881819578916,
          0.9326867267450741,
          0.9483563601715103,
          0.9454601510713735,
          0.9501011358332038,
          0.9575505723694528,
          0.960866129287182,
          0.9193723784371602,
          0.9010136624063464
         ]
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49
         ],
         "y": [
          0.9063826629063383,
          0.9282881792038747,
          0.9282881792038747,
          0.9282881792038747,
          0.9282881792038747,
          0.9378505381233895,
          0.9394692694829954,
          0.9394692694829954,
          0.9394692694829954,
          0.9488684251599813,
          0.9488684251599813,
          0.9488684251599813,
          0.9528041012894205,
          0.9528041012894205,
          0.9528041012894205,
          0.9528041012894205,
          0.9528041012894205,
          0.9528041012894205,
          0.9528041012894205,
          0.9563316029573699,
          0.9563316029573699,
          0.9563316029573699,
          0.9563316029573699,
          0.9563316029573699,
          0.9563316029573699,
          0.9563316029573699,
          0.9563316029573699,
          0.9563316029573699,
          0.9563316029573699,
          0.9563316029573699,
          0.9563316029573699,
          0.9563316029573699,
          0.9574795574795575,
          0.9574795574795575,
          0.9574795574795575,
          0.9574795574795575,
          0.9574795574795575,
          0.9574795574795575,
          0.9574795574795575,
          0.9574795574795575,
          0.9574795574795575,
          0.9574795574795575,
          0.9574795574795575,
          0.9574795574795575,
          0.9574795574795575,
          0.9574795574795575,
          0.9575505723694528,
          0.960866129287182,
          0.960866129287182,
          0.960866129287182
         ]
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "title": {
          "text": "Trial"
         }
        },
        "yaxis": {
         "title": {
          "text": "Objective Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "cliponaxis": false,
         "hovertemplate": [
          "gamma (FloatDistribution): 0.006000556011414496<extra></extra>",
          "weight_decay (FloatDistribution): 0.006040775100323192<extra></extra>",
          "optimizer (CategoricalDistribution): 0.00822525219604796<extra></extra>",
          "batch_size (CategoricalDistribution): 0.01659370867937416<extra></extra>",
          "beta (FloatDistribution): 0.02038757416156182<extra></extra>",
          "alpha (FloatDistribution): 0.02157675469992158<extra></extra>",
          "learning_rate (FloatDistribution): 0.3943633470161036<extra></extra>",
          "dropout (FloatDistribution): 0.5268120321352533<extra></extra>"
         ],
         "name": "Objective Value",
         "orientation": "h",
         "text": [
          "<0.01",
          "<0.01",
          "<0.01",
          "0.02",
          "0.02",
          "0.02",
          "0.39",
          "0.53"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          0.006000556011414496,
          0.006040775100323192,
          0.00822525219604796,
          0.01659370867937416,
          0.02038757416156182,
          0.02157675469992158,
          0.3943633470161036,
          0.5268120321352533
         ],
         "y": [
          "gamma",
          "weight_decay",
          "optimizer",
          "batch_size",
          "beta",
          "alpha",
          "learning_rate",
          "dropout"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Hyperparameter Importances"
        },
        "xaxis": {
         "title": {
          "text": "Hyperparameter Importance"
         }
        },
        "yaxis": {
         "title": {
          "text": "Hyperparameter"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoConfig\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, fbeta_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import optuna.visualization as vis\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Set random seeds for reproducibility\n",
    "# -------------------------------\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Function to print GPU memory (if available)\n",
    "# -------------------------------\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"GPU Memory Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load datasets\n",
    "# -------------------------------\n",
    "# train.csv: big dataset (2521 examples)\n",
    "# valid.csv: final test set (786 examples)\n",
    "train_full_df = pd.read_csv(\"data/train.csv\")   # 2521 examples\n",
    "test_df = pd.read_csv(\"data/valid.csv\")           # 786 examples (final sealed test)\n",
    "\n",
    "# Rename \"labels\" to \"label\" for consistency\n",
    "train_full_df = train_full_df.rename(columns={\"labels\": \"label\"})\n",
    "test_df = test_df.rename(columns={\"labels\": \"label\"})\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Split train_full_df into train and validation subsets (80/20 split)\n",
    "# -------------------------------\n",
    "train_df, valid_df = train_test_split(\n",
    "    train_full_df,\n",
    "    test_size=0.2,  \n",
    "    stratify=train_full_df[\"label\"],  # maintain class distribution\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"New train_df distribution:\\n\", train_df[\"label\"].value_counts())\n",
    "print(\"\\nNew valid_df distribution:\\n\", valid_df[\"label\"].value_counts())\n",
    "print(\"\\nTest set distribution (old valid.csv):\\n\", test_df[\"label\"].value_counts())\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Create a custom Dataset for tokenization\n",
    "# -------------------------------\n",
    "class VaccineDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=384):\n",
    "        self.texts = dataframe['text'].tolist()\n",
    "        self.labels = dataframe['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Remove the batch dimension from each tensor\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Load the PubMedBERT tokenizer and set model name\n",
    "# -------------------------------\n",
    "model_name = \"allenai/biomed_roberta_base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = VaccineDataset(train_df, tokenizer, max_length=384)\n",
    "valid_dataset = VaccineDataset(valid_df, tokenizer, max_length=384)\n",
    "test_dataset  = VaccineDataset(test_df, tokenizer, max_length=384)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Define custom loss functions\n",
    "# -------------------------------\n",
    "def focal_loss(logits, targets, alpha=1, gamma=2):\n",
    "    \"\"\"\n",
    "    Compute focal loss for multi-class classification.\n",
    "    \"\"\"\n",
    "    ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)  # probability of correct classification\n",
    "    focal_loss_val = alpha * ((1 - pt) ** gamma) * ce_loss\n",
    "    return focal_loss_val.mean()\n",
    "\n",
    "def dice_loss(logits, targets, smooth=1):\n",
    "    \"\"\"\n",
    "    Compute dice loss for multi-class classification.\n",
    "    \"\"\"\n",
    "    num_classes = logits.size(1)\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=num_classes).float()\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    intersection = torch.sum(probs * targets_one_hot, dim=0)\n",
    "    union = torch.sum(probs, dim=0) + torch.sum(targets_one_hot, dim=0)\n",
    "    dice = (2 * intersection + smooth) / (union + smooth)\n",
    "    return (1 - dice).mean()\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Create a custom Trainer to use the combined loss\n",
    "# -------------------------------\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, alpha=0.33, beta=0.33, gamma=0.33, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        alpha, beta, gamma: weights for cross entropy, focal, and dice losses respectively.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha_weight = alpha\n",
    "        self.beta_weight = beta\n",
    "        self.gamma_weight = gamma\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        ce_loss = F.cross_entropy(logits, labels)\n",
    "        fl = focal_loss(logits, labels, alpha=1, gamma=2)\n",
    "        dl = dice_loss(logits, labels)\n",
    "        \n",
    "        combined_loss = self.alpha_weight * ce_loss + self.beta_weight * fl + self.gamma_weight * dl\n",
    "        \n",
    "        return (combined_loss, outputs) if return_outputs else combined_loss\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Define compute_metrics function for evaluation using Fβ (beta=1.3)\n",
    "# -------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    fbeta = fbeta_score(labels, preds, beta=1.3, pos_label=1)\n",
    "    return {\"f1_beta\": fbeta}\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Define the objective function for Optuna\n",
    "# -------------------------------\n",
    "def objective(trial):\n",
    "    # --- Optimisation des hyperparamètres ---\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 5e-5, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.4)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adamw\", \"adam\", \"adafactor\"])\n",
    "\n",
    "    # --- Pondération des loss custom ---\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.1, 1.0)\n",
    "    beta = trial.suggest_float(\"beta\", 0.1, 1.0)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.1, 1.0)\n",
    "\n",
    "    # Normalize so they sum to 1\n",
    "    total = alpha + beta + gamma\n",
    "    alpha /= total\n",
    "    beta  /= total\n",
    "    gamma /= total\n",
    "\n",
    "    # --- Initialisation du modèle avec dropout modifié ---\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.hidden_dropout_prob = dropout\n",
    "    config.attention_probs_dropout_prob = dropout\n",
    "    config.num_labels = 2\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        device_map='auto'\n",
    "    )\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # --- Choix de l'optimizer ---\n",
    "    # Importer AdamW et Adam depuis torch.optim, Adafactor depuis transformers.optimization\n",
    "    from torch.optim import AdamW, Adam # type: ignore\n",
    "\n",
    "    # Mapper l'option de l'optimizer sur une valeur valide :\n",
    "    if optimizer_name in [\"adamw\", \"adam\"]:\n",
    "        optim_used = \"adamw_torch\"\n",
    "    elif optimizer_name == \"adafactor\":\n",
    "        optim_used = \"adafactor\"\n",
    "    else:\n",
    "        optim_used = \"adamw_torch\"  # valeur par défaut\n",
    "\n",
    "    # --- TrainingArguments dynamiques ---\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=10,\n",
    "        optim=optim_used,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=max(1, 16 // batch_size),  # simulate batch size 16\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_beta\",\n",
    "        logging_steps=50,\n",
    "        seed=SEED,\n",
    "        report_to=[],\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True\n",
    "    )\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        gamma=gamma\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate(valid_dataset)\n",
    "    return eval_result.get(\"eval_f1_beta\")\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Run the Optuna study to optimize hyperparameters\n",
    "# -------------------------------\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)  # Augmente n_trials pour une recherche plus exhaustive\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  F1_beta: {:.4f}\".format(trial.value))\n",
    "print(\"  Best hyperparameters: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 12. Retrain final model using best hyperparameters and evaluate on the test set\n",
    "# -------------------------------\n",
    "best_lr = trial.params[\"learning_rate\"]\n",
    "best_wd = trial.params[\"weight_decay\"]\n",
    "best_alpha = trial.params[\"alpha\"]\n",
    "best_beta = trial.params[\"beta\"]\n",
    "best_gamma = trial.params[\"gamma\"]\n",
    "\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./final_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=best_lr,\n",
    "    weight_decay=best_wd,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_beta\",\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "final_config = AutoConfig.from_pretrained(model_name, num_labels=2)\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=final_config,\n",
    "    #device_map='auto'\n",
    ")\n",
    "final_model.gradient_checkpointing_enable()\n",
    "\n",
    "final_trainer = CustomTrainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,  # Tu peux combiner train et valid si besoin\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    alpha=best_alpha,\n",
    "    beta=best_beta,\n",
    "    gamma=best_gamma\n",
    ")\n",
    "\n",
    "final_trainer.train()\n",
    "\n",
    "# Sauvegarder le meilleur modèle tout de suite après entraînement\n",
    "final_model.save_pretrained(\"models/final_model\")\n",
    "tokenizer.save_pretrained(\"models/final_model\")\n",
    "print(\"✅ Best model and tokenizer saved to models/final_model\")\n",
    "\n",
    "# -------------------------------\n",
    "# 13. Tune the best threshold on the validation dataset using Fβ (beta=1.3)\n",
    "# -------------------------------\n",
    "valid_predictions = final_trainer.predict(valid_dataset)\n",
    "valid_logits = valid_predictions.predictions\n",
    "valid_labels = valid_predictions.label_ids\n",
    "\n",
    "valid_probs = torch.softmax(torch.tensor(valid_logits), dim=1)[:, 1].numpy()\n",
    "\n",
    "best_fbeta = 0\n",
    "best_thresh = 0.5\n",
    "for thresh in np.arange(0, 1.01, 0.01):\n",
    "    preds = (valid_probs >= thresh).astype(int)\n",
    "    score = fbeta_score(valid_labels, preds, beta=1.3, pos_label=1)\n",
    "    if score > best_fbeta:\n",
    "        best_fbeta = score\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(f\"\\n✅ Best threshold found on validation set: {best_thresh:.2f} (Fβ = {best_fbeta:.4f})\")\n",
    "\n",
    "# -------------------------------\n",
    "# 14. Evaluate on the final test set using the best threshold\n",
    "# -------------------------------\n",
    "test_predictions = final_trainer.predict(test_dataset)\n",
    "test_logits = test_predictions.predictions\n",
    "test_labels = test_predictions.label_ids\n",
    "\n",
    "test_probs = torch.softmax(torch.tensor(test_logits), dim=1)[:, 1].numpy()\n",
    "test_preds = (test_probs >= best_thresh).astype(int)\n",
    "\n",
    "test_f1 = f1_score(test_labels, test_preds, pos_label=1)\n",
    "print(f\"\\nFinal Test F1 on old valid.csv using threshold {best_thresh:.2f} = {test_f1:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 15. Display the Confusion Matrix on the Test Set\n",
    "# -------------------------------\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 16. Print the Classification Report\n",
    "# -------------------------------\n",
    "report = classification_report(test_labels, test_preds, target_names=[\"Class 0\", \"Class 1\"])\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# -------------------------------\n",
    "# 16.5 Log experiment metadata and confusion matrix image\n",
    "# -------------------------------\n",
    "os.makedirs(\"evaluation\", exist_ok=True)\n",
    "\n",
    "# Fichier log\n",
    "csv_file = \"evaluation/experiment_logs.csv\"\n",
    "\n",
    "# Matrice de confusion\n",
    "tn, fp, fn, tp = confusion_matrix(test_labels, test_preds).ravel()\n",
    "\n",
    "# Coefficients bruts d'Optuna\n",
    "alpha_raw = trial.params[\"alpha\"]\n",
    "beta_raw = trial.params[\"beta\"]\n",
    "gamma_raw = trial.params[\"gamma\"]\n",
    "\n",
    "# Normalisation\n",
    "total = alpha_raw + beta_raw + gamma_raw\n",
    "alpha_norm = alpha_raw / total\n",
    "beta_norm = beta_raw / total\n",
    "gamma_norm = gamma_raw / total\n",
    "\n",
    "# Dictionnaire à logger\n",
    "log_dict = {\n",
    "    \"datetime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_name\": model_name,\n",
    "    \"learning_rate\": best_lr,\n",
    "    \"weight_decay\": best_wd,\n",
    "    \"dropout\": final_config.hidden_dropout_prob,\n",
    "    \"optimizer\": trial.params[\"optimizer\"],\n",
    "    \"batch_size\": 8,\n",
    "    \"alpha_raw\": alpha_raw,\n",
    "    \"beta_raw\": beta_raw,\n",
    "    \"gamma_raw\": gamma_raw,\n",
    "    \"alpha_norm\": alpha_norm,\n",
    "    \"beta_norm\": beta_norm,\n",
    "    \"gamma_norm\": gamma_norm,\n",
    "    \"eval_f1_beta\": best_fbeta,\n",
    "    \"best_threshold\": best_thresh,\n",
    "    \"test_f1\": test_f1,\n",
    "    \"test_precision\": precision_score(test_labels, test_preds),\n",
    "    \"test_recall\": recall_score(test_labels, test_preds),\n",
    "    \"test_f1_beta\": fbeta_score(test_labels, test_preds, beta=1.3, pos_label=1),\n",
    "    \"confusion_TN\": tn,\n",
    "    \"confusion_FP\": fp,\n",
    "    \"confusion_FN\": fn,\n",
    "    \"confusion_TP\": tp,\n",
    "}\n",
    "\n",
    "# Enregistrement dans le CSV\n",
    "file_exists = os.path.exists(csv_file)\n",
    "with open(csv_file, mode='a', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=log_dict.keys())\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "    writer.writerow(log_dict)\n",
    "\n",
    "print(f\"\\n📄 Résultats enregistrés dans {csv_file}\")\n",
    "\n",
    "# Sauvegarder la matrice de confusion en image\n",
    "plt.savefig(f\"evaluation/confusion_matrix_trial_{trial.number}.png\")\n",
    "plt.close()\n",
    "print(\"📊 Confusion matrix saved as image.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 17. (Optional) Plot learning curves if desired\n",
    "# -------------------------------\n",
    "train_logs = [log for log in final_trainer.state.log_history if \"loss\" in log and \"eval_loss\" not in log]\n",
    "eval_logs = [log for log in final_trainer.state.log_history if \"eval_loss\" in log]\n",
    "\n",
    "train_epochs = [log[\"epoch\"] for log in train_logs]\n",
    "train_loss = [log[\"loss\"] for log in train_logs]\n",
    "\n",
    "eval_epochs = [log[\"epoch\"] for log in eval_logs]\n",
    "eval_loss = [log[\"eval_loss\"] for log in eval_logs]\n",
    "eval_f1 = [log[\"eval_f1_beta\"] for log in eval_logs]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_epochs, train_loss, marker='o', label=\"Training Loss\")\n",
    "plt.plot(eval_epochs, eval_loss, marker='o', label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(eval_epochs, eval_f1, marker='o', color='green', label=\"Validation F1 (β=1.3)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Validation F1 over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 18. Visualize Optuna study results\n",
    "# -------------------------------\n",
    "\n",
    "vis.plot_optimization_history(study).show()\n",
    "vis.plot_param_importances(study).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini 2.5 Optuna + 3 loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loading data...\n",
      "Train data shape: (2521, 3)\n",
      "Test data shape: (786, 3)\n",
      "Train label distribution:\n",
      " label\n",
      "0    0.544228\n",
      "1    0.455772\n",
      "Name: proportion, dtype: float64\n",
      "Test label distribution:\n",
      " label\n",
      "0    0.534351\n",
      "1    0.465649\n",
      "Name: proportion, dtype: float64\n",
      "Preprocessing text...\n",
      "\n",
      "Sample preprocessed texts (Train):\n",
      "                                                text  \\\n",
      "0  Shingles Vaccine: Warning, this post contains ...   \n",
      "1  Going to get my 2nd shingles shot in 15 minute...   \n",
      "2  Just got my flu shot and now experiencing some...   \n",
      "3  Has anyone experienced a strong reaction to th...   \n",
      "4  Getting two vaccines at once: I have a very re...   \n",
      "\n",
      "                                      text_processed  \n",
      "0  Warning, this post contains an optimistic anti...  \n",
      "1  second one didn't make me sick but I had a GIG...  \n",
      "2  Just got my flu shot and now experiencing some...  \n",
      "3  Has anyone experienced a strong reaction to th...  \n",
      "4  I have a very reactive immune system. 1 vaccin...  \n",
      "Loading tokenizer for microsoft/deberta-v3-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\_experimental.py:31: ExperimentalWarning: Argument ``group`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2025-03-27 03:28:17,654] A new study created in memory with name: no-name-96cdb1d9-ab4f-4bc3-a120-1aaffde36907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Max Length: 384\n",
      "\n",
      "--- Starting Optuna Hyperparameter Optimization ---\n",
      "GPU Memory (Start Trial 0):\n",
      "  Allocated: 0.00 MB\n",
      "  Reserved:  0.00 MB\n",
      "\n",
      "--- Trial 0 ---\n",
      "  LR: 4.91e-06, WD: 0.095, Dropout: 0.270, BS: 4, Accum: 4\n",
      "  Optim: adafactor, LabelSmooth: 0.125\n",
      "  Loss Weights (Norm): CE=0.458, Focal=0.527, Dice=0.015\n",
      "  Focal Gamma: 2.940\n",
      "--- Starting Fold 1/5 for Trial 0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory (Trial 0 Fold 1 Pre-Train):\n",
      "  Allocated: 704.73 MB\n",
      "  Reserved:  758.00 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='399' max='1008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 399/1008 05:01 < 07:42, 1.32 it/s, Epoch 3.16/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Beta 1.3</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.370800</td>\n",
       "      <td>0.361978</td>\n",
       "      <td>0.735375</td>\n",
       "      <td>0.735375</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.282000</td>\n",
       "      <td>0.418731</td>\n",
       "      <td>0.715508</td>\n",
       "      <td>0.715508</td>\n",
       "      <td>0.483193</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.181700</td>\n",
       "      <td>0.583089</td>\n",
       "      <td>0.742113</td>\n",
       "      <td>0.742113</td>\n",
       "      <td>0.516854</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-27 03:33:22,501] Trial 0 failed with parameters: {'learning_rate': 4.909793642305902e-06, 'weight_decay': 0.09507143064099162, 'dropout': 0.26959818254342155, 'batch_size': 4, 'optimizer': 'adafactor', 'loss_weight_ce': 0.6410035105688879, 'loss_weight_focal': 0.737265320016441, 'loss_weight_dice': 0.020584494295802447, 'focal_gamma': 2.9398197043239884, 'label_smoothing_factor': 0.12486639612006326} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_33020\\2425270979.py\", line 408, in objective\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py\", line 2607, in _inner_training_loop\n",
      "    self.optimizer.step()\n",
      "  File \"c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\optimizer.py\", line 165, in step\n",
      "    self.scaler.step(self.optimizer, closure)\n",
      "  File \"c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\amp\\grad_scaler.py\", line 457, in step\n",
      "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\amp\\grad_scaler.py\", line 351, in _maybe_opt_step\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\amp\\grad_scaler.py\", line 351, in <genexpr>\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "               ^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-27 03:33:22,515] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 483\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;66;03m# Define study - consider SQLite for persistence: storage=\"sqlite:///optuna_study.db\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    479\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    480\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mTPESampler(n_startup_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, multivariate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, seed\u001b[38;5;241m=\u001b[39mSEED), \u001b[38;5;66;03m# TPE with Multivariate and Grouping\u001b[39;00m\n\u001b[0;32m    481\u001b[0m     pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(n_warmup_steps\u001b[38;5;241m=\u001b[39mN_SPLITS \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# Prune after half the folds if poor\u001b[39;00m\n\u001b[0;32m    482\u001b[0m )\n\u001b[1;32m--> 483\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Adjust n_trials or add timeout (seconds)\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Optuna Study Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of finished trials: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials))\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[1], line 408\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     print_gpu_memory(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;241m.\u001b[39mnumber\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Pre-Train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 408\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m     eval_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(valid_fold_dataset)\n\u001b[0;32m    410\u001b[0m     fold_f1 \u001b[38;5;241m=\u001b[39m eval_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;66;03m# Get F1 score\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2607\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2603\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[0;32m   2605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2607\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped:\n\u001b[0;32m   2612\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\optimizer.py:165\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_patched_step_method\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called:\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoConfig\n",
    ")\n",
    "# Note: AdamW is typically imported from torch.optim, Adafactor from transformers\n",
    "from torch.optim import AdamW\n",
    "from transformers.optimization import Adafactor\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, precision_score, recall_score, fbeta_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import optuna.visualization as vis # For Optuna plots\n",
    "import gc # Garbage collector for explicit memory clearing\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Set random seeds for reproducibility\n",
    "# -------------------------------\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# Ensure deterministic operations if possible (may impact performance)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "BETA = 1.3 # Fβ score with β=1.3\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Function to print GPU memory (if available)\n",
    "# -------------------------------\n",
    "def print_gpu_memory(stage=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory ({stage}):\")\n",
    "        print(f\"  Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"  Reserved:  {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load datasets\n",
    "# -------------------------------\n",
    "print(\"Loading data...\")\n",
    "# train_full_df: Used for CV during Optuna and final model training (2521 examples)\n",
    "# test_df: The final hold-out set for evaluation (786 examples, originally valid.csv)\n",
    "train_full_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/valid.csv\") # This is our final test set\n",
    "\n",
    "# Rename \"labels\" to \"label\" for consistency\n",
    "train_full_df = train_full_df.rename(columns={\"labels\": \"label\"})\n",
    "test_df = test_df.rename(columns={\"labels\": \"label\"})\n",
    "\n",
    "print(f\"Train data shape: {train_full_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(\"Train label distribution:\\n\", train_full_df['label'].value_counts(normalize=True))\n",
    "print(\"Test label distribution:\\n\", test_df['label'].value_counts(normalize=True))\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Preprocess Text: Handle \"Title: Comment\" structure\n",
    "# -------------------------------\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Extracts the comment part if a 'title: comment' structure is detected.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\" # Handle potential non-string data\n",
    "    # Look for pattern: \"Short Title : Comment text\"\n",
    "    # Adjust the title length limit (e.g., 150) if needed based on data inspection\n",
    "    match = re.match(r\"^(.*?):\\s*(.*)$\", text, re.DOTALL) # re.DOTALL allows '.' to match newline\n",
    "    if match and len(match.group(1)) < 150 and len(match.group(2)) > 10: # Basic heuristics\n",
    "        return match.group(2).strip()\n",
    "    else:\n",
    "        # Assume the whole text is the relevant post/comment\n",
    "        return text.strip()\n",
    "\n",
    "print(\"Preprocessing text...\")\n",
    "train_full_df['text_processed'] = train_full_df['text'].apply(preprocess_text)\n",
    "test_df['text_processed'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display some examples\n",
    "print(\"\\nSample preprocessed texts (Train):\")\n",
    "print(train_full_df[['text', 'text_processed']].head())\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Create a custom Dataset for tokenization\n",
    "# -------------------------------\n",
    "class VaccineDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=384, text_column='text_processed'):\n",
    "        # Ensure labels are integers\n",
    "        self.labels = dataframe['label'].astype(int).tolist()\n",
    "        # Ensure texts are strings\n",
    "        self.texts = dataframe[text_column].astype(str).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Remove the batch dimension from each tensor\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        # Ensure label is a tensor of type long\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Load the Tokenizer and define Model Name\n",
    "# -------------------------------\n",
    "# Starting with BioMed RoBERTa Base, but could be made an Optuna parameter\n",
    "#MODEL_NAME = \"allenai/biomed_roberta_base\"\n",
    "#MODEL_NAME = \"microsoft/deberta-v3-small\" # Smaller model for faster training\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "# Alternative: MODEL_NAME = \"roberta-large\" # Benchmark model type\n",
    "# Alternative: MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "# Alternative: MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\" # Social media specific\n",
    "\n",
    "print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Determine max length dynamically (optional, but good practice)\n",
    "# all_texts = pd.concat([train_full_df['text_processed'], test_df['text_processed']]).tolist()\n",
    "# token_lengths = [len(tokenizer.encode(text)) for text in all_texts]\n",
    "# MAX_LENGTH = int(np.percentile(token_lengths, 98)) # Cover 98% of posts\n",
    "# print(f\"Calculated Max Length (98th percentile): {MAX_LENGTH}\")\n",
    "# Hardcoding for now based on initial code:\n",
    "MAX_LENGTH = 384\n",
    "print(f\"Using Max Length: {MAX_LENGTH}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Define custom loss functions (if using combined loss)\n",
    "# -------------------------------\n",
    "def focal_loss(logits, targets, alpha=1, gamma=2, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Compute focal loss for multi-class classification.\n",
    "    alpha: Weighting factor for balancing classes (can be float or tensor).\n",
    "    gamma: Focusing parameter.\n",
    "    \"\"\"\n",
    "    ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)  # probability of correct class\n",
    "    focal_loss_val = alpha * ((1 - pt) ** gamma) * ce_loss\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        return focal_loss_val.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return focal_loss_val.sum()\n",
    "    else:\n",
    "        return focal_loss_val\n",
    "\n",
    "def dice_loss(logits, targets, smooth=1.0, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Compute dice loss for multi-class classification.\n",
    "    \"\"\"\n",
    "    num_classes = logits.size(1)\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=num_classes).float()\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "    # Sum over all dimensions except the class dimension\n",
    "    dims = tuple(range(len(probs.shape)))[1:] # Exclude batch dim if present, but logits are usually (batch, classes)\n",
    "    if len(probs.shape) > 2: # If spatial dims exist (not typical for text cls)\n",
    "      dims = tuple(range(len(probs.shape)))[2:]\n",
    "\n",
    "    intersection = torch.sum(probs * targets_one_hot, dim=dims)\n",
    "    cardinality = torch.sum(probs, dim=dims) + torch.sum(targets_one_hot, dim=dims)\n",
    "\n",
    "    dice_score = (2. * intersection + smooth) / (cardinality + smooth)\n",
    "\n",
    "    dice_loss_val = 1. - dice_score\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        # Average loss across classes and then batch\n",
    "        return dice_loss_val.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return dice_loss_val.sum()\n",
    "    else: # none\n",
    "        return dice_loss_val\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Create a custom Trainer to use the combined loss\n",
    "# -------------------------------\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, alpha=0.33, beta=0.33, gamma_weight=0.33, focal_gamma=2.0, focal_alpha=1.0, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        alpha: weight for cross entropy loss.\n",
    "        beta: weight for focal loss.\n",
    "        gamma_weight: weight for dice loss.\n",
    "        focal_gamma: gamma parameter for focal loss itself.\n",
    "        focal_alpha: alpha parameter for focal loss itself (class weighting).\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha_weight = alpha\n",
    "        self.beta_weight = beta\n",
    "        self.gamma_weight_dice = gamma_weight # Renamed to avoid confusion with focal_gamma\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.focal_alpha = focal_alpha # Can be adjusted for class imbalance\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss_ce = F.cross_entropy(logits, labels)\n",
    "        loss_fl = focal_loss(logits, labels, alpha=self.focal_alpha, gamma=self.focal_gamma)\n",
    "        loss_dl = dice_loss(logits, labels)\n",
    "\n",
    "        # Ensure weights sum to 1 (or are used as relative factors)\n",
    "        # This normalization happens *before* calling the trainer in Optuna obj\n",
    "        combined_loss = (self.alpha_weight * loss_ce +\n",
    "                         self.beta_weight * loss_fl +\n",
    "                         self.gamma_weight_dice * loss_dl)\n",
    "\n",
    "        return (combined_loss, outputs) if return_outputs else combined_loss\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Define compute_metrics function for evaluation (using F1 score)\n",
    "# -------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes F1-beta score (β=1.3) for the positive class (label = 1).\n",
    "    This is the official metric used for SMM4H 2025 Task 6.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    f1_beta = fbeta_score(labels, preds, beta=BETA, pos_label=1, average='binary')\n",
    "    precision = precision_score(labels, preds, pos_label=1, average='binary')\n",
    "    recall = recall_score(labels, preds, pos_label=1, average='binary')\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1_beta,  # <- F1-beta used by Trainer to select best model\n",
    "        \"f1_beta_1.3\": f1_beta, # <- Additional metric for clarity in log_history\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 9b. Tune threshold helper function (for best F1)\n",
    "# -------------------------------\n",
    "\n",
    "def tune_threshold(trainer, dataset):\n",
    "    \"\"\"\n",
    "    Finds the best threshold maximizing F1 score (class 1) on the provided dataset.\n",
    "    Returns (best_threshold, best_f1).\n",
    "    \"\"\"\n",
    "    predictions = trainer.predict(dataset)\n",
    "    logits = predictions.predictions\n",
    "    labels = predictions.label_ids\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
    "\n",
    "    best_thresh = 0.5\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for thresh in np.arange(0.01, 1.0, 0.01):\n",
    "        preds = (probs >= thresh).astype(int)\n",
    "        f1 = fbeta_score(labels, preds, beta=BETA, pos_label=1, average='binary')\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = thresh\n",
    "\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Define the objective function for Optuna (with Cross-Validation)\n",
    "# -------------------------------\n",
    "N_SPLITS = 5 # Number of folds for Stratified K-Fold Cross-Validation\n",
    "N_TRIALS = 70 # Number of Optuna trials to run\n",
    "N_EPOCHS_OPTUNA = 8 # Number of epochs per fold during Optuna (can be reduced from final)\n",
    "PATIENCE_OPTUNA = 2 # Early stopping patience during Optuna\n",
    "\n",
    "# Ensure train_full_df has clean indices for KFold splitting\n",
    "train_full_df = train_full_df.reset_index(drop=True)\n",
    "\n",
    "def objective(trial):\n",
    "    print_gpu_memory(f\"Start Trial {trial.number}\")\n",
    "\n",
    "    # --- Hyperparameter Search Space ---\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 7e-5, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.05, 0.35)\n",
    "    # Larger batch sizes possible with gradient accumulation\n",
    "    per_device_batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 12]) # Smaller options\n",
    "    # Effective batch size target (adjust based on GPU memory)\n",
    "    target_eff_batch_size = 16 # e.g. Accumulate 4 steps if batch_size=4\n",
    "    gradient_accumulation_steps = max(1, target_eff_batch_size // per_device_batch_size)\n",
    "\n",
    "    # Optimizer choice\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adamw\", \"adafactor\"])\n",
    "\n",
    "    # Loss combination weights\n",
    "    # Let Optuna find the best mix, ensuring they sum to 1\n",
    "    loss_weight_ce = trial.suggest_float(\"loss_weight_ce\", 0.1, 1.0)\n",
    "    loss_weight_focal = trial.suggest_float(\"loss_weight_focal\", 0.1, 1.0)\n",
    "    loss_weight_dice = trial.suggest_float(\"loss_weight_dice\", 0.0, 1.0) # Allow zero weight\n",
    "\n",
    "    # Normalize loss weights\n",
    "    total_weight = loss_weight_ce + loss_weight_focal + loss_weight_dice\n",
    "    norm_alpha = loss_weight_ce / total_weight\n",
    "    norm_beta = loss_weight_focal / total_weight\n",
    "    norm_gamma_dice = loss_weight_dice / total_weight\n",
    "\n",
    "    # Focal Loss specific parameters (if using focal loss, i.e., norm_beta > 0)\n",
    "    focal_gamma_param = trial.suggest_float(\"focal_gamma\", 1.0, 3.0)\n",
    "    # focal_alpha_param = trial.suggest_float(\"focal_alpha\", 0.25, 1.0) # Can tune class weight\n",
    "\n",
    "    # Label smoothing\n",
    "    label_smoothing_factor = trial.suggest_float(\"label_smoothing_factor\", 0.0, 0.15)\n",
    "\n",
    "    # --- Cross-Validation Setup ---\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    fold_f1_scores = []\n",
    "\n",
    "    print(f\"\\n--- Trial {trial.number} ---\")\n",
    "    print(f\"  LR: {learning_rate:.2e}, WD: {weight_decay:.3f}, Dropout: {dropout:.3f}, BS: {per_device_batch_size}, Accum: {gradient_accumulation_steps}\")\n",
    "    print(f\"  Optim: {optimizer_name}, LabelSmooth: {label_smoothing_factor:.3f}\")\n",
    "    print(f\"  Loss Weights (Norm): CE={norm_alpha:.3f}, Focal={norm_beta:.3f}, Dice={norm_gamma_dice:.3f}\")\n",
    "    if norm_beta > 0.01: print(f\"  Focal Gamma: {focal_gamma_param:.3f}\")\n",
    "\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_full_df['text_processed'], train_full_df['label'])):\n",
    "        print(f\"--- Starting Fold {fold+1}/{N_SPLITS} for Trial {trial.number} ---\")\n",
    "\n",
    "        # Create fold-specific datasets\n",
    "        train_fold_df = train_full_df.iloc[train_idx]\n",
    "        valid_fold_df = train_full_df.iloc[val_idx]\n",
    "\n",
    "        train_fold_dataset = VaccineDataset(train_fold_df, tokenizer, max_length=MAX_LENGTH)\n",
    "        valid_fold_dataset = VaccineDataset(valid_fold_df, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "        # --- Model Config (inside loop to reset weights) ---\n",
    "        config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "        config.hidden_dropout_prob = dropout\n",
    "        config.attention_probs_dropout_prob = dropout\n",
    "        config.num_labels = 2\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            config=config,\n",
    "            # ignore_mismatched_sizes=True # Add if switching between base/large models causes issues\n",
    "        )\n",
    "        # Enable gradient checkpointing for memory saving, esp. with larger models/batch sizes\n",
    "        if per_device_batch_size < 16: # Heuristic: more likely needed for smaller batches if accumulating\n",
    "             model.gradient_checkpointing_enable()\n",
    "\n",
    "        # --- Optimizer Selection ---\n",
    "        optim_to_use = \"adamw_torch\" # Default\n",
    "        if optimizer_name == \"adafactor\":\n",
    "            optim_to_use = \"adafactor\"\n",
    "\n",
    "        # --- Training Arguments ---\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results_optuna/trial_{trial.number}_fold_{fold}\", # Unique output dir per fold\n",
    "            num_train_epochs=N_EPOCHS_OPTUNA,\n",
    "            per_device_train_batch_size=per_device_batch_size,\n",
    "            per_device_eval_batch_size=per_device_batch_size * 2, # Usually possible\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            optim=optim_to_use,\n",
    "            evaluation_strategy=\"epoch\", # Evaluate each epoch\n",
    "            logging_strategy=\"epoch\",   # Log each epoch\n",
    "            save_strategy=\"epoch\",      # Save potentially each epoch\n",
    "            load_best_model_at_end=True,# Load the best model based on metric\n",
    "            metric_for_best_model=\"f1\", # Optimize for F1 score\n",
    "            greater_is_better=True,\n",
    "            save_total_limit=1,         # Only keep the best checkpoint\n",
    "            label_smoothing_factor=label_smoothing_factor,\n",
    "            fp16=torch.cuda.is_available(), # Enable mixed precision if GPU available\n",
    "            gradient_checkpointing= (per_device_batch_size < 16), # Match model setting\n",
    "            report_to=[], # Disable default reporting (wandb, tensorboard etc.)\n",
    "            seed=SEED + fold, # Vary seed slightly per fold\n",
    "        )\n",
    "\n",
    "        # --- Trainer ---\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_fold_dataset,\n",
    "            eval_dataset=valid_fold_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE_OPTUNA)],\n",
    "            # Pass normalized loss weights and focal gamma\n",
    "            alpha=norm_alpha,\n",
    "            beta=norm_beta,\n",
    "            gamma_weight=norm_gamma_dice,\n",
    "            focal_gamma=focal_gamma_param,\n",
    "            # focal_alpha=focal_alpha_param # Pass if tuning focal alpha\n",
    "        )\n",
    "\n",
    "        # --- Train & Evaluate Fold ---\n",
    "        try:\n",
    "            print_gpu_memory(f\"Trial {trial.number} Fold {fold+1} Pre-Train\")\n",
    "            trainer.train()\n",
    "            eval_result = trainer.evaluate(valid_fold_dataset)\n",
    "            fold_f1 = eval_result.get(\"eval_f1\", 0.0) # Get F1 score\n",
    "            fold_f1_scores.append(fold_f1)\n",
    "            print(f\"Fold {fold+1} Eval F1: {fold_f1:.4f}\")\n",
    "            print_gpu_memory(f\"Trial {trial.number} Fold {fold+1} Post-Eval\")\n",
    "\n",
    "            # 🚫 HARD PRUNING: if a fold performs way below acceptable level, stop early\n",
    "            MIN_ACCEPTABLE_F1 = 0.84\n",
    "            if fold_f1 < MIN_ACCEPTABLE_F1:\n",
    "                print(f\"⛔️ Trial {trial.number} Fold {fold+1} too weak (F1={fold_f1:.4f}) → Hard pruning.\")\n",
    "                trial.report(fold_f1, fold)\n",
    "                trial.set_user_attr(\"prune_reason\", \"F1_below_threshold = 0.84\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "            # --- Optuna Pruning ---\n",
    "            trial.report(fold_f1, fold)\n",
    "            if trial.should_prune():\n",
    "                # Clean up before pruning\n",
    "                del model, trainer, train_fold_dataset, valid_fold_dataset\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"Trial {trial.number} Fold {fold+1} Pruned.\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! Error during Trial {trial.number} Fold {fold+1}: {e}\")\n",
    "            # Clean up memory even if error occurs\n",
    "            for var_name in ['model', 'trainer', 'train_fold_dataset', 'valid_fold_dataset']:\n",
    "                if var_name in locals():\n",
    "                    del locals()[var_name]\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            fold_f1_scores.append(0.0)  # Penalize this fold\n",
    "\n",
    "        else:\n",
    "            # --- Clean up GPU memory after each successful fold ---\n",
    "            for var_name in ['model', 'trainer', 'train_fold_dataset', 'valid_fold_dataset']:\n",
    "                if var_name in locals():\n",
    "                    del locals()[var_name]\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print_gpu_memory(f\"Trial {trial.number} Fold {fold+1} Cleaned\")\n",
    "\n",
    "\n",
    "    # --- Average F1 across folds ---\n",
    "    if not fold_f1_scores: # Handle case where all folds might have failed or pruned early\n",
    "        average_f1 = 0.0\n",
    "    else:\n",
    "        average_f1 = np.mean(fold_f1_scores)\n",
    "\n",
    "    print(f\"--- Trial {trial.number} Finished --- Average CV F1: {average_f1:.4f}\")\n",
    "    # Store CV scores in trial user attributes for later inspection if needed\n",
    "    trial.set_user_attr(\"cv_scores\", fold_f1_scores)\n",
    "\n",
    "    return average_f1 # Optuna maximizes this average F1\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Run the Optuna study to optimize hyperparameters\n",
    "# -------------------------------\n",
    "print(\"\\n--- Starting Optuna Hyperparameter Optimization ---\")\n",
    "# Define study - consider SQLite for persistence: storage=\"sqlite:///optuna_study.db\"\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=TPESampler(n_startup_trials=10, multivariate=True, group=True, seed=SEED), # TPE with Multivariate and Grouping\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=N_SPLITS // 2) # Prune after half the folds if poor\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=None) # Adjust n_trials or add timeout (seconds)\n",
    "\n",
    "print(\"\\n--- Optuna Study Complete ---\")\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value (Average CV F1): {best_trial.value:.4f}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 12. Retrain final model using best hyperparameters on ALL training data\n",
    "# -------------------------------\n",
    "print(\"\\n--- Training Final Model with Best Hyperparameters ---\")\n",
    "\n",
    "best_params = best_trial.params\n",
    "\n",
    "# Extract best hyperparams\n",
    "final_lr = best_params[\"learning_rate\"]\n",
    "final_wd = best_params[\"weight_decay\"]\n",
    "final_dropout = best_params[\"dropout\"]\n",
    "final_batch_size = best_params[\"batch_size\"]\n",
    "final_optimizer = best_params[\"optimizer\"]\n",
    "final_label_smoothing = best_params[\"label_smoothing_factor\"]\n",
    "\n",
    "# Extract and normalize loss weights\n",
    "final_loss_ce = best_params[\"loss_weight_ce\"]\n",
    "final_loss_focal = best_params[\"loss_weight_focal\"]\n",
    "final_loss_dice = best_params[\"loss_weight_dice\"]\n",
    "final_total_weight = final_loss_ce + final_loss_focal + final_loss_dice\n",
    "final_norm_alpha = final_loss_ce / final_total_weight\n",
    "final_norm_beta = final_loss_focal / final_total_weight\n",
    "final_norm_gamma_dice = final_loss_dice / final_total_weight\n",
    "final_focal_gamma = best_params.get(\"focal_gamma\", 2.0) # Use default if not tuned\n",
    "\n",
    "# Effective batch size target for final training\n",
    "final_target_eff_batch_size = 16 # Keep consistent or adjust\n",
    "final_gradient_accumulation = max(1, final_target_eff_batch_size // final_batch_size)\n",
    "\n",
    "# For final training, we use all train_full_df.\n",
    "# We still need a small validation set for early stopping and saving the best model.\n",
    "# Split off 10% of train_full_df *just* for this purpose.\n",
    "final_train_df, final_valid_df = train_test_split(\n",
    "    train_full_df,\n",
    "    test_size=0.1, # Small validation set for final training phase\n",
    "    stratify=train_full_df[\"label\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "final_train_dataset = VaccineDataset(final_train_df, tokenizer, max_length=MAX_LENGTH)\n",
    "final_valid_dataset = VaccineDataset(final_valid_df, tokenizer, max_length=MAX_LENGTH) # Used for ES\n",
    "\n",
    "# Final Model Config\n",
    "final_config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "final_config.hidden_dropout_prob = final_dropout\n",
    "final_config.attention_probs_dropout_prob = final_dropout\n",
    "\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    config=final_config,\n",
    "    # ignore_mismatched_sizes=True\n",
    ")\n",
    "# Enable gradient checkpointing if necessary based on final batch size\n",
    "if final_batch_size < 16:\n",
    "    final_model.gradient_checkpointing_enable()\n",
    "\n",
    "# Final Training Arguments\n",
    "N_EPOCHS_FINAL = 12 # Allow slightly more epochs for final training\n",
    "PATIENCE_FINAL = 3  # Slightly more patience for final training\n",
    "\n",
    "final_optim_to_use = \"adamw_torch\" if final_optimizer == \"adamw\" else \"adafactor\"\n",
    "\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./final_model_training_output\",\n",
    "    num_train_epochs=N_EPOCHS_FINAL,\n",
    "    per_device_train_batch_size=final_batch_size,\n",
    "    per_device_eval_batch_size=final_batch_size * 2,\n",
    "    gradient_accumulation_steps=final_gradient_accumulation,\n",
    "    learning_rate=final_lr,\n",
    "    weight_decay=final_wd,\n",
    "    optim=final_optim_to_use,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\", # Actually F1-beta(1.3), see compute_metrics\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=1,\n",
    "    label_smoothing_factor=final_label_smoothing,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing= (final_batch_size < 16),\n",
    "    report_to=[],\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Final Trainer\n",
    "final_trainer = CustomTrainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=final_train_dataset, # Train on 90%\n",
    "    eval_dataset=final_valid_dataset, # Validate on 10% for ES/saving best\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE_FINAL)],\n",
    "    # Pass best normalized loss weights and focal gamma\n",
    "    alpha=final_norm_alpha,\n",
    "    beta=final_norm_beta,\n",
    "    gamma_weight=final_norm_gamma_dice,\n",
    "    focal_gamma=final_focal_gamma,\n",
    ")\n",
    "\n",
    "print_gpu_memory(\"Before Final Train\")\n",
    "final_trainer.train()\n",
    "print_gpu_memory(\"After Final Train\")\n",
    "\n",
    "# Best model is now loaded in final_trainer.model\n",
    "\n",
    "# Save the final best model and tokenizer\n",
    "FINAL_MODEL_SAVE_PATH = \"models/final_best_model\"\n",
    "print(f\"Saving final best model to {FINAL_MODEL_SAVE_PATH}...\")\n",
    "final_trainer.save_model(FINAL_MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_SAVE_PATH)\n",
    "print(f\"✅ Final best model and tokenizer saved to {FINAL_MODEL_SAVE_PATH}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 13. Tune the best threshold on the validation set used during final training\n",
    "# -------------------------------\n",
    "print(\"\\n--- Tuning Prediction Threshold ---\")\n",
    "\n",
    "# Appelle la fonction helper qu’on a ajoutée plus tôt\n",
    "best_thresh, best_f1_thresh = tune_threshold(final_trainer, final_valid_dataset)\n",
    "\n",
    "print(f\"✅ Best threshold found on validation split: {best_thresh:.2f} (yields F1 = {best_f1_thresh:.4f})\")\n",
    "\n",
    "# Sauvegarde dans le dossier du modèle\n",
    "with open(os.path.join(FINAL_MODEL_SAVE_PATH, \"threshold.txt\"), \"w\") as f:\n",
    "    f.write(f\"{best_thresh:.4f}\")\n",
    "\n",
    "with open(os.path.join(FINAL_MODEL_SAVE_PATH, \"validation_fbeta_1.3.txt\"), \"w\") as f:\n",
    "    f.write(f\"{best_f1_thresh:.4f}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 14. Evaluate on the final hold-out test set using the best threshold\n",
    "# -------------------------------\n",
    "print(\"\\n--- Evaluating on Final Hold-Out Test Set ---\")\n",
    "# Ensure test_df is preprocessed and create dataset\n",
    "test_dataset = VaccineDataset(test_df, tokenizer, max_length=MAX_LENGTH, text_column='text_processed')\n",
    "\n",
    "# Get predictions from the final model\n",
    "test_predictions = final_trainer.predict(test_dataset)\n",
    "test_logits = test_predictions.predictions\n",
    "test_labels = test_predictions.label_ids # True labels from the test set\n",
    "\n",
    "# Calculate probabilities and apply the best threshold found earlier\n",
    "test_probs = torch.softmax(torch.tensor(test_logits), dim=1)[:, 1].numpy()\n",
    "test_preds_final = (test_probs >= best_thresh).astype(int)\n",
    "\n",
    "# Calculate final metrics on the test set\n",
    "final_test_f1 = f1_score(test_labels, test_preds_final, pos_label=1, average='binary')\n",
    "final_test_precision = precision_score(test_labels, test_preds_final, pos_label=1, average='binary')\n",
    "final_test_recall = recall_score(test_labels, test_preds_final, pos_label=1, average='binary')\n",
    "# Calculate F-beta with beta=1.3 as well for comparison/interest\n",
    "final_test_fbeta = fbeta_score(test_labels, test_preds_final, beta=1.3, pos_label=1, average='binary')\n",
    "\n",
    "print(\"\\n--- Final Test Set Performance ---\")\n",
    "print(f\"  Threshold used: {best_thresh:.2f}\")\n",
    "print(f\"  F1 Score (pos class): {final_test_f1:.4f}\")\n",
    "print(f\"  Precision (pos class): {final_test_precision:.4f}\")\n",
    "print(f\"  Recall (pos class):    {final_test_recall:.4f}\")\n",
    "print(f\"  F-beta (beta=1.3):   {final_test_fbeta:.4f}\")\n",
    "\n",
    "with open(os.path.join(FINAL_MODEL_SAVE_PATH, \"test_fbeta_1.3.txt\"), \"w\") as f:\n",
    "    f.write(f\"{final_test_fbeta:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 15. Display the Confusion Matrix on the Test Set\n",
    "# -------------------------------\n",
    "print(\"\\nGenerating Confusion Matrix...\")\n",
    "cm = confusion_matrix(test_labels, test_preds_final, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Final Test Set Confusion Matrix\")\n",
    "# Ensure evaluation directory exists\n",
    "os.makedirs(\"evaluation\", exist_ok=True)\n",
    "plt.savefig(f\"evaluation/confusion_matrix_final_run.png\")\n",
    "plt.show() # Display the plot\n",
    "print(\"Confusion matrix saved to evaluation/confusion_matrix_final_run.png\")\n",
    "\n",
    "# -------------------------------\n",
    "# 16. Print the Classification Report\n",
    "# -------------------------------\n",
    "print(\"\\nClassification Report (Test Set):\\n\")\n",
    "report = classification_report(test_labels, test_preds_final, target_names=[\"Class 0\", \"Class 1\"], digits=4)\n",
    "print(report)\n",
    "\n",
    "# -------------------------------\n",
    "# 16.5 Log experiment metadata and results\n",
    "# -------------------------------\n",
    "print(\"Logging results...\")\n",
    "csv_file = \"evaluation/experiment_logs.csv\"\n",
    "log_dict = {\n",
    "    \"datetime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"final_learning_rate\": final_lr,\n",
    "    \"final_weight_decay\": final_wd,\n",
    "    \"final_dropout\": final_dropout,\n",
    "    \"final_optimizer\": final_optimizer,\n",
    "    \"final_batch_size\": final_batch_size,\n",
    "    \"final_grad_accum\": final_gradient_accumulation,\n",
    "    \"label_smoothing\": final_label_smoothing,\n",
    "    # Raw loss weights from Optuna\n",
    "    \"optuna_raw_loss_ce\": final_loss_ce,\n",
    "    \"optuna_raw_loss_focal\": final_loss_focal,\n",
    "    \"optuna_raw_loss_dice\": final_loss_dice,\n",
    "    # Normalized loss weights used in training\n",
    "    \"norm_loss_weight_ce\": final_norm_alpha,\n",
    "    \"norm_loss_weight_focal\": final_norm_beta,\n",
    "    \"norm_loss_weight_dice\": final_norm_gamma_dice,\n",
    "    \"focal_gamma\": final_focal_gamma if final_norm_beta > 0.01 else \"N/A\",\n",
    "    \"optuna_cv_f1_avg\": best_trial.value,\n",
    "    \"best_threshold_on_valid_split\": best_thresh,\n",
    "    \"fbeta_1.3_on_valid_split\": best_f1_thresh,\n",
    "    \"test_f1\": final_test_f1,\n",
    "    \"test_precision\": final_test_precision,\n",
    "    \"test_recall\": final_test_recall,\n",
    "    \"test_fbeta_1.3\": final_test_fbeta,\n",
    "    \"confusion_TN\": tn,\n",
    "    \"confusion_FP\": fp,\n",
    "    \"confusion_FN\": fn,\n",
    "    \"confusion_TP\": tp,\n",
    "    \"optuna_n_trials\": N_TRIALS,\n",
    "    \"optuna_n_splits_cv\": N_SPLITS,\n",
    "    \"max_seq_length\": MAX_LENGTH,\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "\n",
    "# Append to CSV\n",
    "file_exists = os.path.exists(csv_file)\n",
    "with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=log_dict.keys())\n",
    "    if not file_exists or os.path.getsize(csv_file) == 0:\n",
    "        writer.writeheader() # Write header only if file is new or empty\n",
    "    writer.writerow(log_dict)\n",
    "\n",
    "print(f\"📄 Final results logged to {csv_file}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 17. Plot learning curves from the FINAL training phase\n",
    "# -------------------------------\n",
    "print(\"Generating learning curves from final training...\")\n",
    "train_logs = [log for log in final_trainer.state.log_history if 'loss' in log]\n",
    "eval_logs = [log for log in final_trainer.state.log_history if 'eval_loss' in log]\n",
    "\n",
    "if train_logs and eval_logs:\n",
    "    # Extract epochs and metrics, handling potential variations in logging steps\n",
    "    train_epochs = [log['epoch'] for log in train_logs if 'epoch' in log]\n",
    "    train_loss = [log['loss'] for log in train_logs if 'loss' in log]\n",
    "\n",
    "    eval_epochs = [log['epoch'] for log in eval_logs if 'epoch' in log]\n",
    "    eval_loss = [log['eval_loss'] for log in eval_logs if 'eval_loss' in log]\n",
    "    eval_f1 = [log['eval_f1'] for log in eval_logs if 'eval_f1' in log]\n",
    "\n",
    "    # Ensure lengths match for plotting (might differ slightly due to logging timing)\n",
    "    min_len = min(len(train_epochs), len(eval_epochs))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_epochs[:min_len], train_loss[:min_len], marker='o', linestyle='-', label=\"Training Loss\")\n",
    "    plt.plot(eval_epochs[:min_len], eval_loss[:min_len], marker='o', linestyle='-', label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Final Training: Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(eval_epochs, eval_f1, marker='o', linestyle='-', color='green', label=\"Validation F1\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1 Score (Positive Class)\")\n",
    "    plt.title(\"Final Training: Validation F1\")\n",
    "    # Highlight the epoch where the best score occurred (if easily available)\n",
    "    best_epoch_log = final_trainer.state.best_metric # state.best_model_checkpoint might hold path\n",
    "    if final_trainer.state.best_metric is not None:\n",
    "        best_f1_val = final_trainer.state.best_metric\n",
    "        # Find epoch corresponding to best metric (approximate)\n",
    "        best_epoch_idx = np.argmax(eval_f1)\n",
    "        best_epoch = eval_epochs[best_epoch_idx]\n",
    "        plt.scatter([best_epoch], [best_f1_val], color='red', s=100, label=f'Best F1 ({best_f1_val:.4f})', zorder=5)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"evaluation/learning_curves_final_run.png\")\n",
    "    plt.show()\n",
    "    print(\"Learning curves saved to evaluation/learning_curves_final_run.png\")\n",
    "else:\n",
    "    print(\"Could not generate learning curves: Log history incomplete.\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 18. Visualize Optuna study results\n",
    "# -------------------------------\n",
    "print(\"Generating Optuna visualization plots...\")\n",
    "try:\n",
    "    fig1 = vis.plot_optimization_history(study)\n",
    "    fig1.show()\n",
    "    fig1.write_image(\"evaluation/optuna_optimization_history.png\")\n",
    "\n",
    "    fig2 = vis.plot_param_importances(study)\n",
    "    fig2.show()\n",
    "    fig2.write_image(\"evaluation/optuna_param_importances.png\")\n",
    "\n",
    "    # Requires plotly installed\n",
    "    # fig3 = vis.plot_slice(study, params=[\"learning_rate\", \"dropout\", \"weight_decay\"])\n",
    "    # fig3.show()\n",
    "    # fig3.write_image(\"evaluation/optuna_slice_plot.png\")\n",
    "\n",
    "    print(\"Optuna plots saved to evaluation/ directory.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate Optuna plots: {e}. (Ensure plotly and kaleido are installed)\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFACTOR COMPLET GEMINI 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import traceback # Added for potentially more detailed error printing\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoConfig,\n",
    "    TrainerCallback,         # <-- Import base callback\n",
    "    TrainerState,            # <-- Import TrainerState\n",
    "    TrainerControl           # <-- Import TrainerControl\n",
    ")\n",
    "# Note: AdamW is typically imported from torch.optim, Adafactor from transformers\n",
    "from torch.optim import AdamW\n",
    "from transformers.optimization import Adafactor\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, precision_score, recall_score, fbeta_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import optuna.visualization as vis # For Optuna plots\n",
    "import gc # Garbage collector for explicit memory clearing\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Set random seeds for reproducibility\n",
    "# -------------------------------\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# Define the Beta value for F-beta score (as specified by the competition)\n",
    "BETA = 1.3\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Function to print GPU memory (if available)\n",
    "# -------------------------------\n",
    "def print_gpu_memory(stage=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory ({stage}):\")\n",
    "        print(f\"  Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"  Reserved:  {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load datasets\n",
    "# -------------------------------\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    train_full_df = pd.read_csv(\"data/train.csv\")\n",
    "    test_df = pd.read_csv(\"data/valid.csv\") # This is our final test set\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}. Make sure 'data/train.csv' and 'data/valid.csv' exist.\")\n",
    "    exit()\n",
    "\n",
    "train_full_df = train_full_df.rename(columns={\"labels\": \"label\"})\n",
    "test_df = test_df.rename(columns={\"labels\": \"label\"})\n",
    "print(f\"Train data shape: {train_full_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(\"Train label distribution:\\n\", train_full_df['label'].value_counts(normalize=True))\n",
    "print(\"Test label distribution:\\n\", test_df['label'].value_counts(normalize=True))\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Preprocess Text\n",
    "# -------------------------------\n",
    "#def preprocess_text(text):\n",
    "    #if not isinstance(text, str): return \"\"\n",
    "    #match = re.match(r\"^(.*?):\\s*(.*)$\", text, re.DOTALL)\n",
    "    #if match and len(match.group(1)) < 150 and len(match.group(2)) > 10:\n",
    "        #return match.group(2).strip()\n",
    "    #else:\n",
    "       #return text.strip()\n",
    "\n",
    "#print(\"Preprocessing text...\")\n",
    "#train_full_df['text_processed'] = train_full_df['text'].apply(preprocess_text)\n",
    "#test_df['text_processed'] = test_df['text'].apply(preprocess_text)\n",
    "#print(\"\\nSample preprocessed texts (Train):\")\n",
    "#print(train_full_df[['text', 'text_processed']].head())\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Create Dataset Class\n",
    "# -------------------------------\n",
    "class VaccineDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=384, text_column='text_processed'):\n",
    "        self.labels = dataframe['label'].astype(int).tolist()\n",
    "        self.texts = dataframe[text_column].astype(str).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Load Tokenizer and Model Config\n",
    "# -------------------------------\n",
    "MODEL_NAME = \"microsoft/deberta-v3-small\"\n",
    "print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "except Exception as e: print(f\"Error loading tokenizer '{MODEL_NAME}': {e}\"); exit()\n",
    "MAX_LENGTH = 384\n",
    "print(f\"Using Max Length: {MAX_LENGTH}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Define Loss Functions\n",
    "# -------------------------------\n",
    "def focal_loss(logits, targets, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "    ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    if isinstance(alpha, float): alpha_t = alpha\n",
    "    else: alpha = alpha.to(targets.device); alpha_t = alpha.gather(0, targets.data.view(-1)).view(-1, 1)\n",
    "    focal_loss_val = alpha_t * ((1 - pt) ** gamma) * ce_loss\n",
    "    if reduction == 'mean': return focal_loss_val.mean()\n",
    "    elif reduction == 'sum': return focal_loss_val.sum()\n",
    "    else: return focal_loss_val\n",
    "\n",
    "def dice_loss(logits, targets, smooth=1.0, reduction='mean'):\n",
    "    num_classes = logits.size(1)\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=num_classes).to(logits.device).float()\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    dims_to_sum = (0,) if probs.dim() == 2 else tuple(range(probs.dim()))[1:]\n",
    "    intersection = torch.sum(probs * targets_one_hot, dim=dims_to_sum)\n",
    "    cardinality = torch.sum(probs, dim=dims_to_sum) + torch.sum(targets_one_hot, dim=dims_to_sum)\n",
    "    dice_score = (2. * intersection + smooth) / (cardinality + smooth)\n",
    "    dice_loss_val = 1. - dice_score\n",
    "    if reduction == 'mean': return dice_loss_val.mean()\n",
    "    elif reduction == 'sum': return dice_loss_val.sum()\n",
    "    else: return dice_loss_val\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Custom Trainer\n",
    "# -------------------------------\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, loss_weight_ce=0.33, loss_weight_focal=0.33, loss_weight_dice=0.33, focal_gamma=2.0, focal_alpha=1.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_weight_ce = loss_weight_ce\n",
    "        self.loss_weight_focal = loss_weight_focal\n",
    "        self.loss_weight_dice = loss_weight_dice\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.focal_alpha = focal_alpha\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs): # Accept **kwargs\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        labels = labels.to(logits.device) # Ensure device match\n",
    "        loss = 0.0\n",
    "        if self.loss_weight_ce > 1e-6: loss += self.loss_weight_ce * F.cross_entropy(logits, labels)\n",
    "        if self.loss_weight_focal > 1e-6: loss += self.loss_weight_focal * focal_loss(logits, labels, alpha=self.focal_alpha, gamma=self.focal_gamma)\n",
    "        if self.loss_weight_dice > 1e-6: loss += self.loss_weight_dice * dice_loss(logits, labels)\n",
    "        if loss == 0.0: print(\"Warning: All loss weights zero?\"); loss = F.cross_entropy(logits, labels) # Fallback\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Metrics & Threshold Tuning\n",
    "# -------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if logits is None or labels is None or len(logits) == 0: return {\"f1\": 0.0, \"f1_beta\": 0.0, \"precision\": 0.0, \"recall\": 0.0}\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    f1_beta_val = fbeta_score(labels, preds, beta=BETA, pos_label=1, average='binary', zero_division=0)\n",
    "    precision_val = precision_score(labels, preds, pos_label=1, average='binary', zero_division=0)\n",
    "    recall_val = recall_score(labels, preds, pos_label=1, average='binary', zero_division=0)\n",
    "    return {\"f1\": f1_beta_val, \"f1_beta\": f1_beta_val, \"precision\": precision_val, \"recall\": recall_val}\n",
    "\n",
    "def tune_threshold(trainer, dataset):\n",
    "    print(\"Tuning threshold...\")\n",
    "    try: predictions = trainer.predict(dataset)\n",
    "    except Exception as e: print(f\"Error during prediction for threshold tuning: {e}\"); return 0.5, 0.0\n",
    "    logits, labels = predictions.predictions, predictions.label_ids\n",
    "    if logits is None or labels is None or len(logits) == 0: print(\"Warning: Empty predictions for threshold tuning.\"); return 0.5, 0.0\n",
    "    probs = torch.softmax(torch.tensor(logits, dtype=torch.float32), dim=1)[:, 1].numpy()\n",
    "    best_thresh, best_fbeta = 0.5, 0.0\n",
    "    for thresh in np.arange(0.01, 1.0, 0.01):\n",
    "        preds = (probs >= thresh).astype(int)\n",
    "        fbeta_val = fbeta_score(labels, preds, beta=BETA, pos_label=1, average='binary', zero_division=0)\n",
    "        if fbeta_val > best_fbeta: best_fbeta, best_thresh = fbeta_val, thresh\n",
    "    print(f\"Threshold tuning complete. Best threshold: {best_thresh:.2f} (F-beta={best_fbeta:.4f})\")\n",
    "    return best_thresh, best_fbeta\n",
    "\n",
    "# **** NEW: Define the Per-Epoch Pruning Callback ****\n",
    "class OptunaPruningCallback(TrainerCallback):\n",
    "    def __init__(self, trial: optuna.trial.Trial, monitor: str = \"eval_f1\", epoch_threshold: float = 0.88):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trial: The Optuna trial object.\n",
    "            monitor: The metric name in the evaluation results to monitor (must match compute_metrics output).\n",
    "            epoch_threshold: The minimum acceptable score for the monitored metric after each epoch.\n",
    "        \"\"\"\n",
    "        self.trial = trial\n",
    "        self.monitor = monitor\n",
    "        self.epoch_threshold = epoch_threshold\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics: dict[str, float], **kwargs):\n",
    "        \"\"\"Called after each evaluation phase.\"\"\"\n",
    "        if metrics is None or self.monitor not in metrics:\n",
    "            print(f\"Warning (Callback): Metric '{self.monitor}' not found in evaluation results.\")\n",
    "            return # Metric not available, cannot check\n",
    "\n",
    "        current_score = metrics[self.monitor]\n",
    "        current_epoch = state.epoch or 0 # Epoch might be float or None initially\n",
    "\n",
    "        print(f\"--- Callback Check: Epoch {current_epoch:.2f}, {self.monitor} = {current_score:.4f} ---\")\n",
    "\n",
    "        # 1. Report score to Optuna's internal pruner (e.g., MedianPruner)\n",
    "        #    Reporting by epoch number within a fold isn't directly comparable across folds for MedianPruner.\n",
    "        #    It's better to report the final fold score outside the callback for MedianPruner.\n",
    "        #    However, reporting intermediate scores doesn't hurt and might be used by other pruners.\n",
    "        # self.trial.report(current_score, step=int(current_epoch)) # Optional: Report intermediate\n",
    "\n",
    "        # 2. Check against the hard epoch threshold\n",
    "        if current_score < self.epoch_threshold:\n",
    "            message = f\"Pruning trial: {self.monitor} {current_score:.4f} < {self.epoch_threshold} at epoch {current_epoch:.2f}\"\n",
    "            print(f\"⛔️ {message}\")\n",
    "            self.trial.set_user_attr(\"prune_reason\", f\"Callback: {message}\")\n",
    "            # Setting this flag should ideally stop training, but raising is more direct for Optuna\n",
    "            # control.should_training_stop = True\n",
    "            raise optuna.exceptions.TrialPruned(message) # Directly raise to stop Trainer and signal Optuna\n",
    "        else:\n",
    "            print(f\"✔️ Callback Check Passed: {self.monitor} {current_score:.4f} >= {self.epoch_threshold}\")\n",
    "\n",
    "        # 3. Optionally, ask Optuna's main pruner (MedianPruner) if it wants to prune based on epoch reports\n",
    "        #    (As noted, less reliable for MedianPruner with epochs across folds)\n",
    "        # if self.trial.should_prune():\n",
    "        #     message = f\"Pruning trial (Optuna Pruner decision during epoch {current_epoch:.2f})\"\n",
    "        #     print(f\"⛔️ {message}\")\n",
    "        #     self.trial.set_user_attr(\"prune_reason\", \"Callback: Optuna Pruner\")\n",
    "        #     raise optuna.exceptions.TrialPruned(message)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Define the objective function for Optuna (with Cross-Validation)\n",
    "# -------------------------------\n",
    "N_SPLITS = 5\n",
    "N_TRIALS = 70\n",
    "N_EPOCHS_OPTUNA = 8\n",
    "PATIENCE_OPTUNA = 2\n",
    "# **** More Aggressive Epoch Threshold ****\n",
    "MIN_ACCEPTABLE_F1_PER_EPOCH = 0.88 # Prune if ANY epoch's F-beta is below this\n",
    "\n",
    "train_full_df = train_full_df.reset_index(drop=True)\n",
    "\n",
    "def objective(trial):\n",
    "    print_gpu_memory(f\"Start Trial {trial.number}\")\n",
    "\n",
    "    # --- Hyperparameter Search Space ---\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 7e-5, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.05, 0.35)\n",
    "    per_device_batch_size = trial.suggest_categorical(\"batch_size\", [4, 8])\n",
    "    target_eff_batch_size = 16\n",
    "    gradient_accumulation_steps = max(1, target_eff_batch_size // per_device_batch_size)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adamw\", \"adafactor\"])\n",
    "    label_smoothing_factor = trial.suggest_float(\"label_smoothing_factor\", 0.0, 0.15)\n",
    "    raw_loss_weight_ce = trial.suggest_float(\"loss_weight_ce\", 0.1, 1.0)\n",
    "    raw_loss_weight_focal = trial.suggest_float(\"loss_weight_focal\", 0.1, 1.0)\n",
    "    raw_loss_weight_dice = trial.suggest_float(\"loss_weight_dice\", 0.0, 1.0)\n",
    "    focal_gamma_param = trial.suggest_float(\"focal_gamma\", 1.0, 3.0)\n",
    "\n",
    "    # Normalize loss weights\n",
    "    total_weight = raw_loss_weight_ce + raw_loss_weight_focal + raw_loss_weight_dice\n",
    "    if total_weight < 1e-6: total_weight = 1.0; print(\"Warning: Loss weights sum zero.\")\n",
    "    norm_weight_ce = raw_loss_weight_ce / total_weight\n",
    "    norm_weight_focal = raw_loss_weight_focal / total_weight\n",
    "    norm_weight_dice = raw_loss_weight_dice / total_weight\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    fold_fbeta_scores = []\n",
    "\n",
    "    print(f\"\\n--- Trial {trial.number} ---\")\n",
    "    # (Shortened print statement for brevity)\n",
    "    print(f\"  Params: LR={learning_rate:.2e}, WD={weight_decay:.3f}, Dropout={dropout:.3f}, BS={per_device_batch_size}, Accum={gradient_accumulation_steps}...\")\n",
    "    print(f\"  Loss(N): CE={norm_weight_ce:.3f}, Focal={norm_weight_focal:.3f}, Dice={norm_weight_dice:.3f}\")\n",
    "    if norm_weight_focal > 0.01: print(f\"  Focal Gamma: {focal_gamma_param:.3f}\")\n",
    "\n",
    "    # Fold Loop\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_full_df['text_processed'], train_full_df['label'])):\n",
    "        print(f\"--- Starting Fold {fold+1}/{N_SPLITS} for Trial {trial.number} ---\")\n",
    "        model, trainer, config, training_args = None, None, None, None # Ensure cleanup works if init fails\n",
    "        train_fold_dataset, valid_fold_dataset = None, None\n",
    "        eval_result = None\n",
    "        train_output = None\n",
    "\n",
    "        try:\n",
    "            # Create datasets\n",
    "            train_fold_df = train_full_df.iloc[train_idx]\n",
    "            valid_fold_df = train_full_df.iloc[val_idx]\n",
    "            train_fold_dataset = VaccineDataset(train_fold_df, tokenizer, max_length=MAX_LENGTH)\n",
    "            valid_fold_dataset = VaccineDataset(valid_fold_df, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "            # Model Config\n",
    "            config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "            config.hidden_dropout_prob = dropout\n",
    "            config.attention_probs_dropout_prob = dropout\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "            gc_enabled = (gradient_accumulation_steps > 1 or per_device_batch_size <= 4)\n",
    "            if gc_enabled:\n",
    "                try: model.gradient_checkpointing_enable(); print(\"GC Enabled.\")\n",
    "                except Exception as e_gc: print(f\"GC enable failed: {e_gc}\"); gc_enabled = False\n",
    "\n",
    "            optim_to_use = \"adamw_torch\" if optimizer_name == \"adamw\" else \"adafactor\"\n",
    "\n",
    "            # **** Instantiate the Pruning Callback ****\n",
    "            pruning_callback = OptunaPruningCallback(trial, epoch_threshold=MIN_ACCEPTABLE_F1_PER_EPOCH)\n",
    "\n",
    "            # Training Arguments (using corrected eval_strategy)\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=f\"./results_optuna/trial_{trial.number}_fold_{fold}\",\n",
    "                num_train_epochs=N_EPOCHS_OPTUNA,\n",
    "                per_device_train_batch_size=per_device_batch_size,\n",
    "                per_device_eval_batch_size=per_device_batch_size * 2,\n",
    "                gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                learning_rate=learning_rate,\n",
    "                weight_decay=weight_decay,\n",
    "                optim=optim_to_use,\n",
    "                eval_strategy=\"epoch\",        # CORRECTED\n",
    "                logging_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"f1\",\n",
    "                greater_is_better=True,\n",
    "                save_total_limit=1,\n",
    "                label_smoothing_factor=label_smoothing_factor,\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                gradient_checkpointing=gc_enabled,\n",
    "                report_to=[],\n",
    "                seed=SEED + fold,\n",
    "            )\n",
    "\n",
    "            # Trainer\n",
    "            trainer = CustomTrainer(\n",
    "                model=model, args=training_args,\n",
    "                train_dataset=train_fold_dataset, eval_dataset=valid_fold_dataset,\n",
    "                compute_metrics=compute_metrics,\n",
    "                # **** Add Pruning Callback and EarlyStopping ****\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE_OPTUNA), pruning_callback],\n",
    "                loss_weight_ce=norm_weight_ce, loss_weight_focal=norm_weight_focal,\n",
    "                loss_weight_dice=norm_weight_dice, focal_gamma=focal_gamma_param,\n",
    "            )\n",
    "\n",
    "            # Train (Callback will raise TrialPruned if threshold is met)\n",
    "            print_gpu_memory(f\"Trial {trial.number} Fold {fold+1} Pre-Train\")\n",
    "            train_output = trainer.train()\n",
    "\n",
    "            # If training finished without pruning by callback, evaluate final model\n",
    "            print(\"Evaluating fold (final)...\")\n",
    "            eval_result = trainer.evaluate(valid_fold_dataset)\n",
    "            fold_fbeta = eval_result.get(\"eval_f1\", 0.0)\n",
    "            fold_fbeta_scores.append(fold_fbeta)\n",
    "            print(f\"Fold {fold+1} FINAL Eval F-beta: {fold_fbeta:.4f}\")\n",
    "            print_gpu_memory(f\"Trial {trial.number} Fold {fold+1} Post-Eval\")\n",
    "\n",
    "            # --- Post-Fold Pruning Checks (MedianPruner) ---\n",
    "            # Hard threshold check after fold is less critical now callback handles epochs\n",
    "            # print(f\"--- Post-Fold Pruning Check: Step {fold}, F-beta = {fold_fbeta:.4f} ---\")\n",
    "            trial.report(fold_fbeta, fold) # Report FINAL fold score for MedianPruner step 'fold'\n",
    "            if trial.should_prune():\n",
    "                print(f\"⛔️ Pruning trial (MedianPruner decision after Fold {fold+1})\")\n",
    "                trial.set_user_attr(\"prune_reason\", f\"MedianPruner after Fold {fold+1}\")\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "            else:\n",
    "                 print(f\"✔️ MedianPruner Check Passed after Fold {fold+1}\")\n",
    "\n",
    "        # Handle pruning exception raised by callback or post-fold check\n",
    "        except optuna.exceptions.TrialPruned as e:\n",
    "             print(f\"Trial {trial.number} pruned at Fold {fold+1}. Reason: {trial.user_attrs.get('prune_reason', 'Unknown/MedianPruner')}\")\n",
    "             # Propagate the exception to stop the entire trial\n",
    "             raise e\n",
    "\n",
    "        # Handle other errors during the fold\n",
    "        except Exception as e:\n",
    "            print(f\"!!! Error during Trial {trial.number} Fold {fold+1}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            fold_fbeta_scores.append(0.0) # Penalize fold score\n",
    "\n",
    "        # Cleanup for the fold\n",
    "        finally:\n",
    "             print(f\"Cleaning up Trial {trial.number} Fold {fold+1}...\")\n",
    "             del model, trainer, config, training_args, train_fold_dataset, valid_fold_dataset\n",
    "             if 'train_output' in locals(): del train_output\n",
    "             if 'eval_result' in locals(): del eval_result\n",
    "             gc.collect()\n",
    "             if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "             print_gpu_memory(f\"Trial {trial.number} Fold {fold+1} Cleaned Up\")\n",
    "\n",
    "\n",
    "    # --- Average F-beta across COMPLETED folds for the trial ---\n",
    "    # If a trial was pruned, fold_fbeta_scores might be incomplete\n",
    "    valid_scores = [score for score in fold_fbeta_scores if isinstance(score, (float, np.floating)) and np.isfinite(score)]\n",
    "    if not valid_scores: average_fbeta = 0.0\n",
    "    else: average_fbeta = np.mean(valid_scores)\n",
    "\n",
    "    print(f\"--- Trial {trial.number} Finished --- Average CV F-beta (over completed folds): {average_fbeta:.4f}\")\n",
    "    trial.set_user_attr(\"cv_scores\", fold_fbeta_scores)\n",
    "\n",
    "    return average_fbeta\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Run Optuna Study\n",
    "# -------------------------------\n",
    "print(\"\\n--- Starting Optuna Hyperparameter Optimization ---\")\n",
    "storage_name = \"sqlite:///optuna_study.db\"\n",
    "study = optuna.create_study(\n",
    "    study_name=\"smm4h-shingles-v3\", # Increment version\n",
    "    storage=storage_name, load_if_exists=True, direction=\"maximize\",\n",
    "    sampler=TPESampler(n_startup_trials=15, multivariate=True, group=True, seed=SEED),\n",
    "    # MedianPruner now acts on the *final* reported score for each fold step\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=5, n_warmup_steps=1, interval_steps=1\n",
    "    )\n",
    ")\n",
    "\n",
    "def garbage_collect_callback(study, trial): gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Optuna study '{study.study_name}' using storage '{storage_name}'\")\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS, timeout=None, callbacks=[garbage_collect_callback])\n",
    "except KeyboardInterrupt: print(\"Optuna optimization stopped manually.\")\n",
    "except Exception as e: print(f\"An critical error occurred during Optuna optimization: {e}\"); traceback.print_exc()\n",
    "\n",
    "print(\"\\n--- Optuna Study Complete ---\")\n",
    "# (Analysis of completed/pruned/failed trials remains the same)\n",
    "print(\"Number of finished trials (total): \", len(study.trials))\n",
    "completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "print(\"Number of completed trials: \", len(completed_trials))\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "print(\"Number of pruned trials: \", len(pruned_trials))\n",
    "failed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]\n",
    "print(\"Number of failed trials: \", len(failed_trials))\n",
    "\n",
    "if not completed_trials: print(\"!!! No trials completed successfully. Cannot proceed.\"); exit()\n",
    "\n",
    "try:\n",
    "    best_trial = study.best_trial\n",
    "    print(\"\\nBest trial overall:\")\n",
    "    print(f\"  Number: {best_trial.number}, Value: {best_trial.value:.4f}, State: {best_trial.state}\")\n",
    "    print(\"  Params: \", best_trial.params)\n",
    "    if best_trial.state != optuna.trial.TrialState.COMPLETE:\n",
    "         print(f\"  Warning: Best trial state is {best_trial.state}.\")\n",
    "         completed_trials_sorted = sorted(completed_trials, key=lambda t: t.value, reverse=True)\n",
    "         if completed_trials_sorted:\n",
    "             best_trial = completed_trials_sorted[0]\n",
    "             print(\"\\nUsing best *completed* trial instead:\")\n",
    "             print(f\"  Number: {best_trial.number}, Value: {best_trial.value:.4f}\")\n",
    "             print(\"  Params: \", best_trial.params)\n",
    "         else: print(\"Error: Could not find any best completed trial.\"); exit()\n",
    "except ValueError: print(\"Optuna study has no completed trials.\"); exit()\n",
    "except Exception as e: print(f\"Error retrieving best trial: {e}\"); exit()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 12. Retrain final model\n",
    "# -------------------------------\n",
    "print(\"\\n--- Training Final Model with Best Hyperparameters ---\")\n",
    "best_params = best_trial.params\n",
    "# (Extract params - code remains the same)\n",
    "final_lr = best_params[\"learning_rate\"]; final_wd = best_params[\"weight_decay\"]; final_dropout = best_params[\"dropout\"]\n",
    "final_batch_size = best_params[\"batch_size\"]; final_optimizer = best_params[\"optimizer\"]; final_label_smoothing = best_params[\"label_smoothing_factor\"]\n",
    "final_raw_ce = best_params[\"loss_weight_ce\"]; final_raw_focal = best_params[\"loss_weight_focal\"]; final_raw_dice = best_params[\"loss_weight_dice\"]\n",
    "final_total_weight = final_raw_ce + final_raw_focal + final_raw_dice\n",
    "if final_total_weight < 1e-6: final_total_weight = 1.0\n",
    "final_norm_ce = final_raw_ce / final_total_weight; final_norm_focal = final_raw_focal / final_total_weight; final_norm_dice = final_raw_dice / final_total_weight\n",
    "final_focal_gamma = best_params.get(\"focal_gamma\", 2.0)\n",
    "final_target_eff_batch_size = 16\n",
    "final_gradient_accumulation = max(1, final_target_eff_batch_size // final_batch_size)\n",
    "\n",
    "# (Prepare final datasets - code remains the same)\n",
    "final_train_df, final_valid_df = train_test_split(train_full_df, test_size=0.1, stratify=train_full_df[\"label\"], random_state=SEED)\n",
    "final_train_dataset = VaccineDataset(final_train_df, tokenizer, max_length=MAX_LENGTH)\n",
    "final_valid_dataset = VaccineDataset(final_valid_df, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "# (Load final model config - code remains the same)\n",
    "final_model = None\n",
    "try:\n",
    "    final_config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "    final_config.hidden_dropout_prob = final_dropout; final_config.attention_probs_dropout_prob = final_dropout\n",
    "    final_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=final_config)\n",
    "except Exception as e: print(f\"!!! Error loading final model: {e}. Exiting.\"); exit()\n",
    "\n",
    "final_gc_enabled = (final_gradient_accumulation > 1 or final_batch_size <= 4)\n",
    "if final_gc_enabled:\n",
    "    try: final_model.gradient_checkpointing_enable(); print(\"Final Model: GC Enabled.\")\n",
    "    except Exception as e_gc_final: print(f\"Final GC enable failed: {e_gc_final}\"); final_gc_enabled = False\n",
    "\n",
    "# (Final Training Arguments - use corrected eval_strategy)\n",
    "N_EPOCHS_FINAL = 12; PATIENCE_FINAL = 3\n",
    "final_optim_to_use = \"adamw_torch\" if final_optimizer == \"adamw\" else \"adafactor\"\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./final_model_training_output\", num_train_epochs=N_EPOCHS_FINAL,\n",
    "    per_device_train_batch_size=final_batch_size, per_device_eval_batch_size=final_batch_size * 2,\n",
    "    gradient_accumulation_steps=final_gradient_accumulation, learning_rate=final_lr, weight_decay=final_wd, optim=final_optim_to_use,\n",
    "    eval_strategy=\"epoch\",        # CORRECTED\n",
    "    logging_strategy=\"epoch\", save_strategy=\"epoch\", load_best_model_at_end=True, metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True, save_total_limit=1, label_smoothing_factor=final_label_smoothing,\n",
    "    fp16=torch.cuda.is_available(), gradient_checkpointing=final_gc_enabled, report_to=[], seed=SEED,\n",
    ")\n",
    "\n",
    "# (Final Trainer - no pruning callback needed here)\n",
    "final_trainer = CustomTrainer(\n",
    "    model=final_model, args=final_training_args, train_dataset=final_train_dataset, eval_dataset=final_valid_dataset,\n",
    "    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE_FINAL)],\n",
    "    loss_weight_ce=final_norm_ce, loss_weight_focal=final_norm_focal, loss_weight_dice=final_norm_dice, focal_gamma=final_focal_gamma,\n",
    ")\n",
    "\n",
    "# (Train final model - code remains the same)\n",
    "print_gpu_memory(\"Before Final Train\")\n",
    "try: final_trainer.train()\n",
    "except Exception as e: print(f\"!!! Error during final training: {e}\"); traceback.print_exc(); exit()\n",
    "print_gpu_memory(\"After Final Train\")\n",
    "\n",
    "# (Save final model - code remains the same)\n",
    "FINAL_MODEL_SAVE_PATH = \"models/final_best_model\"\n",
    "print(f\"Saving final best model to {FINAL_MODEL_SAVE_PATH}...\")\n",
    "try:\n",
    "    os.makedirs(FINAL_MODEL_SAVE_PATH, exist_ok=True)\n",
    "    final_trainer.save_model(FINAL_MODEL_SAVE_PATH)\n",
    "    tokenizer.save_pretrained(FINAL_MODEL_SAVE_PATH)\n",
    "    print(f\"✅ Final best model and tokenizer saved.\")\n",
    "except Exception as e: print(f\"Error saving final model: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 13. Tune Threshold\n",
    "# -------------------------------\n",
    "# (Code remains the same)\n",
    "print(\"\\n--- Tuning Prediction Threshold ---\")\n",
    "best_thresh, fbeta_on_valid_at_thresh = tune_threshold(final_trainer, final_valid_dataset)\n",
    "print(f\"✅ Best threshold: {best_thresh:.2f} (yields F-beta={fbeta_on_valid_at_thresh:.4f} on validation split)\")\n",
    "try:\n",
    "    with open(os.path.join(FINAL_MODEL_SAVE_PATH, \"threshold.txt\"), \"w\") as f: f.write(f\"{best_thresh:.4f}\")\n",
    "    with open(os.path.join(FINAL_MODEL_SAVE_PATH, \"validation_fbeta_score.txt\"), \"w\") as f: f.write(f\"{fbeta_on_valid_at_thresh:.4f}\")\n",
    "except Exception as e: print(f\"Error saving threshold/validation score: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 14. Evaluate on Test Set\n",
    "# -------------------------------\n",
    "# (Code remains the same, including float32 cast for softmax)\n",
    "print(\"\\n--- Evaluating on Final Hold-Out Test Set ---\")\n",
    "test_dataset = VaccineDataset(test_df, tokenizer, max_length=MAX_LENGTH, text_column='text_processed')\n",
    "test_predictions, test_logits, test_labels = None, None, None\n",
    "try:\n",
    "    test_predictions = final_trainer.predict(test_dataset)\n",
    "    test_logits, test_labels = test_predictions.predictions, test_predictions.label_ids\n",
    "except Exception as e: print(f\"Error during final test prediction: {e}\"); exit()\n",
    "if test_logits is None or test_labels is None: print(\"Error: Test prediction failed.\"); exit()\n",
    "\n",
    "test_probs = torch.softmax(torch.tensor(test_logits, dtype=torch.float32), dim=1)[:, 1].numpy()\n",
    "test_preds_final = (test_probs >= best_thresh).astype(int)\n",
    "final_test_fbeta = fbeta_score(test_labels, test_preds_final, beta=BETA, pos_label=1, average='binary', zero_division=0)\n",
    "final_test_precision = precision_score(test_labels, test_preds_final, pos_label=1, average='binary', zero_division=0)\n",
    "final_test_recall = recall_score(test_labels, test_preds_final, pos_label=1, average='binary', zero_division=0)\n",
    "final_test_f1_std = f1_score(test_labels, test_preds_final, pos_label=1, average='binary', zero_division=0)\n",
    "\n",
    "print(\"\\n--- Final Test Set Performance ---\")\n",
    "print(f\"  Threshold used: {best_thresh:.2f}\")\n",
    "print(f\"  F-beta (beta={BETA}): {final_test_fbeta:.4f}  <-- Target Metric\")\n",
    "print(f\"  Precision: {final_test_precision:.4f}, Recall: {final_test_recall:.4f}\")\n",
    "print(f\"  F1 Score (beta=1.0): {final_test_f1_std:.4f} (for reference)\")\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(FINAL_MODEL_SAVE_PATH, \"test_fbeta_score.txt\"), \"w\") as f: f.write(f\"{final_test_fbeta:.4f}\")\n",
    "except Exception as e: print(f\"Error saving test score: {e}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 15. Confusion Matrix\n",
    "# -------------------------------\n",
    "# (Code remains the same)\n",
    "print(\"\\nGenerating Confusion Matrix...\")\n",
    "tn, fp, fn, tp = 'N/A', 'N/A', 'N/A', 'N/A'\n",
    "try:\n",
    "    cm = confusion_matrix(test_labels, test_preds_final, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
    "    disp.plot(cmap=\"Blues\"); plt.title(\"Final Test Set Confusion Matrix\")\n",
    "    os.makedirs(\"evaluation\", exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    plt.savefig(f\"evaluation/confusion_matrix_final_run_{timestamp}.png\")\n",
    "    plt.show()\n",
    "    print(\"Confusion matrix saved.\")\n",
    "except Exception as e: print(f\"Error generating CM: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 16. Classification Report\n",
    "# -------------------------------\n",
    "# (Code remains the same)\n",
    "print(\"\\nClassification Report (Test Set):\\n\")\n",
    "try:\n",
    "    report = classification_report(test_labels, test_preds_final, target_names=[\"Class 0\", \"Class 1\"], digits=4, zero_division=0)\n",
    "    print(report)\n",
    "except Exception as e: print(f\"Error generating report: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 16.5 Log Results\n",
    "# -------------------------------\n",
    "# (Code remains the same)\n",
    "print(\"Logging results...\")\n",
    "csv_file = \"evaluation/experiment_logs.csv\"\n",
    "log_dict = {\n",
    "    \"datetime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"model_name\": MODEL_NAME,\n",
    "    \"final_learning_rate\": final_lr, \"final_weight_decay\": final_wd, \"final_dropout\": final_dropout,\n",
    "    \"final_optimizer\": final_optimizer, \"final_batch_size\": final_batch_size, \"final_grad_accum\": final_gradient_accumulation,\n",
    "    \"label_smoothing\": final_label_smoothing, \"optuna_raw_loss_ce\": final_raw_ce, \"optuna_raw_loss_focal\": final_raw_focal,\n",
    "    \"optuna_raw_loss_dice\": final_raw_dice, \"norm_loss_weight_ce\": final_norm_ce, \"norm_loss_weight_focal\": final_norm_focal,\n",
    "    \"norm_loss_weight_dice\": final_norm_dice, \"focal_gamma\": final_focal_gamma if final_norm_focal > 0.01 else \"N/A\",\n",
    "    \"optuna_cv_fbeta_avg\": best_trial.value, \"optuna_best_trial_num\": best_trial.number,\n",
    "    \"best_threshold_on_valid_split\": best_thresh, \"fbeta_on_valid_split_at_thresh\": fbeta_on_valid_at_thresh,\n",
    "    \"test_fbeta\": final_test_fbeta, \"test_precision\": final_test_precision, \"test_recall\": final_test_recall,\n",
    "    \"test_f1_std\": final_test_f1_std, \"confusion_TN\": tn, \"confusion_FP\": fp, \"confusion_FN\": fn, \"confusion_TP\": tp,\n",
    "    \"optuna_n_trials_run\": len(study.trials), \"optuna_n_trials_completed\": len(completed_trials),\n",
    "    \"optuna_n_trials_pruned\": len(pruned_trials), \"optuna_n_splits_cv\": N_SPLITS, \"max_seq_length\": MAX_LENGTH,\n",
    "    \"seed\": SEED, \"beta_value\": BETA,\n",
    "}\n",
    "try:\n",
    "    file_exists = os.path.exists(csv_file)\n",
    "    with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=log_dict.keys())\n",
    "        if not file_exists or os.path.getsize(csv_file) == 0: writer.writeheader()\n",
    "        writer.writerow(log_dict)\n",
    "    print(f\"📄 Results logged.\")\n",
    "except Exception as e: print(f\"Error logging results: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 17. Plot Learning Curves\n",
    "# -------------------------------\n",
    "# (Code remains the same)\n",
    "print(\"Generating learning curves...\")\n",
    "try:\n",
    "    log_history = final_trainer.state.log_history\n",
    "    train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
    "    eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "    if train_logs and eval_logs:\n",
    "        train_epochs = [log.get('epoch') for log in train_logs]; train_loss = [log.get('loss') for log in train_logs]\n",
    "        eval_epochs = [log.get('epoch') for log in eval_logs]; eval_loss = [log.get('eval_loss') for log in eval_logs]; eval_fbeta = [log.get('eval_f1') for log in eval_logs]\n",
    "        train_epochs, train_loss = zip(*[(e, l) for e, l in zip(train_epochs, train_loss) if isinstance(e, (int, float)) and isinstance(l, (int, float))])\n",
    "        eval_epochs, eval_loss, eval_fbeta = zip(*[(e, l, f) for e, l, f in zip(eval_epochs, eval_loss, eval_fbeta) if isinstance(e, (int, float)) and isinstance(l, (int, float)) and isinstance(f, (int, float))])\n",
    "        if train_epochs and eval_epochs:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.subplot(1, 2, 1); plt.plot(train_epochs, train_loss, marker='.', linestyle='-', label=\"Train Loss\"); plt.plot(eval_epochs, eval_loss, marker='.', linestyle='-', label=\"Valid Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Final Training: Loss\"); plt.legend(); plt.grid(True); plt.ylim(bottom=0)\n",
    "            plt.subplot(1, 2, 2); plt.plot(eval_epochs, eval_fbeta, marker='.', linestyle='-', color='green', label=f\"Valid F-beta (β={BETA})\"); plt.xlabel(\"Epoch\"); plt.ylabel(f\"F-beta (β={BETA})\"); plt.title(\"Final Training: Valid F-beta\"); plt.grid(True); plt.ylim(0, 1.05)\n",
    "            if final_trainer.state.best_metric is not None:\n",
    "                best_fbeta_val = final_trainer.state.best_metric\n",
    "                try: best_epoch_idx = eval_fbeta.index(max(eval_fbeta)); best_epoch = eval_epochs[best_epoch_idx]; plt.scatter([best_epoch], [best_fbeta_val], color='red', s=100, label=f'Best ({best_fbeta_val:.4f})', zorder=5)\n",
    "                except (ValueError, IndexError): print(\"Could not find best epoch index.\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout(); timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\"); plt.savefig(f\"evaluation/learning_curves_final_run_{timestamp}.png\"); plt.show()\n",
    "            print(\"Learning curves saved.\")\n",
    "        else: print(\"No valid log points after filtering.\")\n",
    "    else: print(\"Log history incomplete.\")\n",
    "except Exception as e: print(f\"Error generating curves: {e}\"); traceback.print_exc()\n",
    "\n",
    "# -------------------------------\n",
    "# 18. Visualize Optuna Study\n",
    "# -------------------------------\n",
    "# (Code remains the same)\n",
    "print(\"Generating Optuna plots...\")\n",
    "if 'study' in locals() and study and study.trials:\n",
    "    try:\n",
    "        fig1 = vis.plot_optimization_history(study); fig1.write_image(\"evaluation/optuna_optimization_history.png\")\n",
    "        fig2 = vis.plot_param_importances(study); fig2.write_image(\"evaluation/optuna_param_importances.png\")\n",
    "        # fig1.show(); fig2.show() # Uncomment to display plots interactively\n",
    "        print(\"Optuna plots saved.\")\n",
    "    except ImportError: print(\"Install plotly/kaleido for plots.\")\n",
    "    except Exception as e: print(f\"Error generating Optuna plots: {e}\")\n",
    "else: print(\"Skipping Optuna plots.\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biomed Roberta base simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">CUDA available. Device: NVIDIA GeForce RTX </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3070</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCUDA available. Device: NVIDIA GeForce RTX \u001b[0m\u001b[1;36m3070\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Importing libraries</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mImporting libraries\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Libraries imported.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Libraries imported.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cleaning Training Data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cleaning Training Data\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cleaning Holdout Validation Data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cleaning Holdout Validation Data\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training data loaded and cleaned. Total: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2521</span><span style=\"color: #008000; text-decoration-color: #008000\"> examples.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training data loaded and cleaned. Total: \u001b[0m\u001b[1;32m2521\u001b[0m\u001b[32m examples.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout validation data loaded and cleaned. Total: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">786</span><span style=\"color: #008000; text-decoration-color: #008000\"> examples.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout validation data loaded and cleaned. Total: \u001b[0m\u001b[1;32m786\u001b[0m\u001b[32m examples.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Training data label distribution:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1372</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1149</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Training data label distribution:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1372\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m1149\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Tokenizer loaded from </span><span style=\"color: #008000; text-decoration-color: #008000\">'allenai/biomed_roberta_base'</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Tokenizer loaded from \u001b[0m\u001b[32m'allenai/biomed_roberta_base'\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────── </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Starting Cross-Validation on Training Data</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────── \u001b[0m\u001b[1;33mStarting Cross-Validation on Training Data\u001b[0m\u001b[92m ────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">505</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2016\u001b[0m, Fold Validation Size: \u001b[1;36m505\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1097</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1097\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">275</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m275\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m1\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,016\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 504\n",
      "  Number of trainable parameters = 124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [504/504 01:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210100</td>\n",
       "      <td>0.191596</td>\n",
       "      <td>0.935982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.128900</td>\n",
       "      <td>0.346228</td>\n",
       "      <td>0.907258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.235216</td>\n",
       "      <td>0.948498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.290648</td>\n",
       "      <td>0.936975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_1\\checkpoint-126\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_1\\checkpoint-126\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_1\\checkpoint-126\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_1\\checkpoint-252\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_1\\checkpoint-252\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_1\\checkpoint-252\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_1\\checkpoint-378\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_1\\checkpoint-378\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_1\\checkpoint-378\\model.safetensors\n",
      "Deleting older checkpoint [results_biomed_cv_train_only\\fold_1\\checkpoint-126] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_1\\checkpoint-504\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_1\\checkpoint-504\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_1\\checkpoint-504\\model.safetensors\n",
      "Deleting older checkpoint [results_biomed_cv_train_only\\fold_1\\checkpoint-252] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_biomed_cv_train_only\\fold_1\\checkpoint-378 (score: 0.9484978540772532).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m completed.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and optimizing threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> <span style=\"font-weight: bold\">(</span>using its validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and optimizing threshold for Fold \u001b[1;36m1\u001b[0m \u001b[1m(\u001b[0musing its validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 505\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Fold Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9561</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.98</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m - Best F1 on Fold Validation: \u001b[0m\u001b[1;36m0.9561\u001b[0m\u001b[36m @ Threshold = \u001b[0m\u001b[1;36m0.98\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Optimal threshold saved to ./results_biomed_cv_train_only\\fold_1\\checkpoint-</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">378</span><span style=\"color: #008000; text-decoration-color: #008000\">\\threshold.txt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Optimal threshold saved to .\u001b[0m\u001b[32m/\u001b[0m\u001b[32mresults_biomed_cv_train_only\u001b[0m\u001b[32m\\fold_1\\checkpoint-\u001b[0m\u001b[1;32m378\u001b[0m\u001b[32m\\threshold.txt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">1</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m2\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1098</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1098\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">274</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m274\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m2\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Safetensors PR exists\n",
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 508\n",
      "  Number of trainable parameters = 124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [508/508 01:48, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>0.168144</td>\n",
       "      <td>0.941685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>0.195235</td>\n",
       "      <td>0.941935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.241929</td>\n",
       "      <td>0.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.249555</td>\n",
       "      <td>0.946004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_2\\checkpoint-127\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_2\\checkpoint-127\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_2\\checkpoint-127\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_2\\checkpoint-254\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_2\\checkpoint-254\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_2\\checkpoint-254\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_2\\checkpoint-381\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_2\\checkpoint-381\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_2\\checkpoint-381\\model.safetensors\n",
      "Deleting older checkpoint [results_biomed_cv_train_only\\fold_2\\checkpoint-127] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_2\\checkpoint-508\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_2\\checkpoint-508\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_2\\checkpoint-508\\model.safetensors\n",
      "Deleting older checkpoint [results_biomed_cv_train_only\\fold_2\\checkpoint-254] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_biomed_cv_train_only\\fold_2\\checkpoint-508 (score: 0.9460043196544277).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m2\u001b[0m\u001b[32m completed.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and optimizing threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> <span style=\"font-weight: bold\">(</span>using its validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and optimizing threshold for Fold \u001b[1;36m2\u001b[0m \u001b[1m(\u001b[0musing its validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Fold Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9499</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.68</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m2\u001b[0m\u001b[36m - Best F1 on Fold Validation: \u001b[0m\u001b[1;36m0.9499\u001b[0m\u001b[36m @ Threshold = \u001b[0m\u001b[1;36m0.68\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Optimal threshold saved to ./results_biomed_cv_train_only\\fold_2\\checkpoint-</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">508</span><span style=\"color: #008000; text-decoration-color: #008000\">\\threshold.txt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Optimal threshold saved to .\u001b[0m\u001b[32m/\u001b[0m\u001b[32mresults_biomed_cv_train_only\u001b[0m\u001b[32m\\fold_2\\checkpoint-\u001b[0m\u001b[1;32m508\u001b[0m\u001b[32m\\threshold.txt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">2</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">3</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m3\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1098</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1098\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">274</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m274\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Attempting to create safetensors variant\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m3\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 508\n",
      "  Number of trainable parameters = 124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [508/508 01:43, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.222432</td>\n",
       "      <td>0.930649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>0.235035</td>\n",
       "      <td>0.948276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091500</td>\n",
       "      <td>0.249596</td>\n",
       "      <td>0.946695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.249203</td>\n",
       "      <td>0.950538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Safetensors PR exists\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_3\\checkpoint-127\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_3\\checkpoint-127\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_3\\checkpoint-127\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_3\\checkpoint-254\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_3\\checkpoint-254\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_3\\checkpoint-254\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_3\\checkpoint-381\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_3\\checkpoint-381\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_3\\checkpoint-381\\model.safetensors\n",
      "Deleting older checkpoint [results_biomed_cv_train_only\\fold_3\\checkpoint-127] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_3\\checkpoint-508\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_3\\checkpoint-508\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_3\\checkpoint-508\\model.safetensors\n",
      "Deleting older checkpoint [results_biomed_cv_train_only\\fold_3\\checkpoint-254] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_biomed_cv_train_only\\fold_3\\checkpoint-508 (score: 0.9505376344086022).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m completed.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and optimizing threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> <span style=\"font-weight: bold\">(</span>using its validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and optimizing threshold for Fold \u001b[1;36m3\u001b[0m \u001b[1m(\u001b[0musing its validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Fold Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9565</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.99</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m3\u001b[0m\u001b[36m - Best F1 on Fold Validation: \u001b[0m\u001b[1;36m0.9565\u001b[0m\u001b[36m @ Threshold = \u001b[0m\u001b[1;36m0.99\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Optimal threshold saved to ./results_biomed_cv_train_only\\fold_3\\checkpoint-</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">508</span><span style=\"color: #008000; text-decoration-color: #008000\">\\threshold.txt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Optimal threshold saved to .\u001b[0m\u001b[32m/\u001b[0m\u001b[32mresults_biomed_cv_train_only\u001b[0m\u001b[32m\\fold_3\\checkpoint-\u001b[0m\u001b[1;32m508\u001b[0m\u001b[32m\\threshold.txt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">3</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">4</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m4\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1098</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1098\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">274</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m274\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m4\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 508\n",
      "  Number of trainable parameters = 124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [508/508 01:40, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>0.248547</td>\n",
       "      <td>0.935010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.193489</td>\n",
       "      <td>0.943966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.303214</td>\n",
       "      <td>0.943633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.945378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Safetensors PR exists\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_4\\checkpoint-127\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_4\\checkpoint-127\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_4\\checkpoint-127\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_4\\checkpoint-254\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_4\\checkpoint-254\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_4\\checkpoint-254\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_4\\checkpoint-381\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_4\\checkpoint-381\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_4\\checkpoint-381\\model.safetensors\n",
      "Deleting older checkpoint [results_biomed_cv_train_only\\fold_4\\checkpoint-127] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_4\\checkpoint-508\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_4\\checkpoint-508\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_4\\checkpoint-508\\model.safetensors\n",
      "Deleting older checkpoint [results_biomed_cv_train_only\\fold_4\\checkpoint-254] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_biomed_cv_train_only\\fold_4\\checkpoint-508 (score: 0.9453781512605042).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m completed.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and optimizing threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> <span style=\"font-weight: bold\">(</span>using its validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and optimizing threshold for Fold \u001b[1;36m4\u001b[0m \u001b[1m(\u001b[0musing its validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Fold Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9489</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.99</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m - Best F1 on Fold Validation: \u001b[0m\u001b[1;36m0.9489\u001b[0m\u001b[36m @ Threshold = \u001b[0m\u001b[1;36m0.99\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Optimal threshold saved to ./results_biomed_cv_train_only\\fold_4\\checkpoint-</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">508</span><span style=\"color: #008000; text-decoration-color: #008000\">\\threshold.txt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Optimal threshold saved to .\u001b[0m\u001b[32m/\u001b[0m\u001b[32mresults_biomed_cv_train_only\u001b[0m\u001b[32m\\fold_4\\checkpoint-\u001b[0m\u001b[1;32m508\u001b[0m\u001b[32m\\threshold.txt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">4</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m4\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m5\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1097</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">920</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1097\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m920\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">275</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">229</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m275\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m229\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m5\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 508\n",
      "  Number of trainable parameters = 124,647,170\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [508/508 01:45, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.207300</td>\n",
       "      <td>0.220083</td>\n",
       "      <td>0.919149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.315493</td>\n",
       "      <td>0.918580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.272944</td>\n",
       "      <td>0.927473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.357732</td>\n",
       "      <td>0.931393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_5\\checkpoint-127\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_5\\checkpoint-127\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_5\\checkpoint-127\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_5\\checkpoint-254\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_5\\checkpoint-254\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_5\\checkpoint-254\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_5\\checkpoint-381\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_5\\checkpoint-381\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_5\\checkpoint-381\\model.safetensors\n",
      "Deleting older checkpoint [results_biomed_cv_train_only\\fold_5\\checkpoint-127] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results_biomed_cv_train_only\\fold_5\\checkpoint-508\n",
      "Configuration saved in ./results_biomed_cv_train_only\\fold_5\\checkpoint-508\\config.json\n",
      "Model weights saved in ./results_biomed_cv_train_only\\fold_5\\checkpoint-508\\model.safetensors\n",
      "Deleting older checkpoint [results_biomed_cv_train_only\\fold_5\\checkpoint-254] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_biomed_cv_train_only\\fold_5\\checkpoint-508 (score: 0.9313929313929314).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m5\u001b[0m\u001b[32m completed.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and optimizing threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> <span style=\"font-weight: bold\">(</span>using its validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and optimizing threshold for Fold \u001b[1;36m5\u001b[0m \u001b[1m(\u001b[0musing its validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 504\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Fold Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9353</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.76</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m5\u001b[0m\u001b[36m - Best F1 on Fold Validation: \u001b[0m\u001b[1;36m0.9353\u001b[0m\u001b[36m @ Threshold = \u001b[0m\u001b[1;36m0.76\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Optimal threshold saved to ./results_biomed_cv_train_only\\fold_5\\checkpoint-</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">508</span><span style=\"color: #008000; text-decoration-color: #008000\">\\threshold.txt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Optimal threshold saved to .\u001b[0m\u001b[32m/\u001b[0m\u001b[32mresults_biomed_cv_train_only\u001b[0m\u001b[32m\\fold_5\\checkpoint-\u001b[0m\u001b[1;32m508\u001b[0m\u001b[32m\\threshold.txt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">5</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m5\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────── </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Cross-Validation Summary (on Train Splits)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────── \u001b[0m\u001b[1;32mCross-Validation Summary \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mon Train Splits\u001b[0m\u001b[1;32m)\u001b[0m\u001b[92m ────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\"> Fold Performance on Internal Validation Splits  </span>\n",
       "┏━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Fold </span>┃<span style=\"font-weight: bold\"> Best F1 (Fold Val) </span>┃<span style=\"font-weight: bold\"> Optimal Threshold </span>┃\n",
       "┡━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 1    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">             0.9561 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">              0.98 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 2    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">             0.9499 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">              0.68 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 3    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">             0.9565 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">              0.99 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 4    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">             0.9489 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">              0.99 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 5    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">             0.9353 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">              0.76 </span>│\n",
       "└──────┴────────────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m Fold Performance on Internal Validation Splits  \u001b[0m\n",
       "┏━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mFold\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mBest F1 (Fold Val)\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOptimal Threshold\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m1   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m            0.9561\u001b[0m\u001b[35m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m             0.98\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m2   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m            0.9499\u001b[0m\u001b[35m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m             0.68\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m3   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m            0.9565\u001b[0m\u001b[35m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m             0.99\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m4   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m            0.9489\u001b[0m\u001b[35m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m             0.99\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m5   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m            0.9353\u001b[0m\u001b[35m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m             0.76\u001b[0m\u001b[33m \u001b[0m│\n",
       "└──────┴────────────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Avg F1 <span style=\"font-weight: bold\">(</span>Internal Validation<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9494</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> +/- </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0077</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Avg F1 \u001b[1m(\u001b[0mInternal Validation\u001b[1m)\u001b[0m: \u001b[1;36m0.9494\u001b[0m\u001b[1;36m +\u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m-\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m0.0077\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Avg Optimal Threshold: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.88</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> +/- </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.13</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Avg Optimal Threshold: \u001b[1;36m0.88\u001b[0m\u001b[1;36m +\u001b[0m\u001b[1;36m/\u001b[0m\u001b[1;36m-\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m0.13\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Final Evaluation on Holdout Set (valid.csv)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────── \u001b[0m\u001b[1;35mFinal Evaluation on Holdout Set \u001b[0m\u001b[1;35m(\u001b[0m\u001b[1;35mvalid.csv\u001b[0m\u001b[1;35m)\u001b[0m\u001b[92m ───────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Preparing holdout validation dataset <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">786</span> samples<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Preparing holdout validation dataset \u001b[1m(\u001b[0m\u001b[1;36m786\u001b[0m samples\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running ensemble inference on holdout set using <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> models<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running ensemble inference on holdout set using \u001b[1;36m5\u001b[0m models\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">checkpoint-</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">378</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m1\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33mcheckpoint-\u001b[0m\u001b[1;33m378\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./results_biomed_cv_train_only\\fold_1\\checkpoint-378\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./results_biomed_cv_train_only\\fold_1\\checkpoint-378\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./results_biomed_cv_train_only\\fold_1\\checkpoint-378.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6e06af988b4fb5be096e11d367fb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">checkpoint-</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">508</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m2\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33mcheckpoint-\u001b[0m\u001b[1;33m508\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./results_biomed_cv_train_only\\fold_2\\checkpoint-508\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./results_biomed_cv_train_only\\fold_2\\checkpoint-508\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./results_biomed_cv_train_only\\fold_2\\checkpoint-508.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9725070f3834f5a9f363b9b218d4545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m2\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">checkpoint-</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">508</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m3\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33mcheckpoint-\u001b[0m\u001b[1;33m508\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./results_biomed_cv_train_only\\fold_3\\checkpoint-508\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./results_biomed_cv_train_only\\fold_3\\checkpoint-508\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./results_biomed_cv_train_only\\fold_3\\checkpoint-508.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c810071b7a43d8877a3b703a305e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">checkpoint-</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">508</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m4\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33mcheckpoint-\u001b[0m\u001b[1;33m508\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./results_biomed_cv_train_only\\fold_4\\checkpoint-508\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./results_biomed_cv_train_only\\fold_4\\checkpoint-508\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./results_biomed_cv_train_only\\fold_4\\checkpoint-508.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8190c9662618480d97b13d47d41f204b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">checkpoint-</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">508</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m5\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33mcheckpoint-\u001b[0m\u001b[1;33m508\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./results_biomed_cv_train_only\\fold_5\\checkpoint-508\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./results_biomed_cv_train_only\\fold_5\\checkpoint-508\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./results_biomed_cv_train_only\\fold_5\\checkpoint-508.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197bbc51caeb4658ad10d123e23d9f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m5\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Combining holdout probabilities from </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> models (averaging)...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mCombining holdout probabilities from \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;36m models \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36maveraging\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Applying average threshold from CV: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.8803</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Applying average threshold from CV: \u001b[1;33m0.8803\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Final Ensemble Performance on Holdout Set (valid.csv)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────────────────────────────── \u001b[0m\u001b[1;35mFinal Ensemble Performance on Holdout Set \u001b[0m\u001b[1;35m(\u001b[0m\u001b[1;35mvalid.csv\u001b[0m\u001b[1;35m)\u001b[0m\u001b[92m ──────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">786</span> holdout samples.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating on \u001b[1;36m786\u001b[0m holdout samples.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                 \n",
       " <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">              </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">        </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">        </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">        </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">     </span> \n",
       " ─────────────────────────────────────────────── \n",
       "  0              0.9487   0.9690   0.9588   420  \n",
       "  1              0.9636   0.9399   0.9516   366  \n",
       "                                                 \n",
       "  <span style=\"font-weight: bold\">macro Avg</span>      0.9562   0.9545   0.9552   786  \n",
       "  <span style=\"font-weight: bold\">weighted Avg</span>   0.9556   0.9555   0.9554   786  \n",
       "                                                 \n",
       "  <span style=\"font-weight: bold\">Accuracy</span>                         <span style=\"font-weight: bold\">0.9555</span>   786  \n",
       "                                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                 \n",
       " \u001b[1;35m \u001b[0m\u001b[1;35m            \u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35m      \u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35m      \u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35m      \u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35m   \u001b[0m\u001b[1;35m \u001b[0m \n",
       " ─────────────────────────────────────────────── \n",
       "  0              0.9487   0.9690   0.9588   420  \n",
       "  1              0.9636   0.9399   0.9516   366  \n",
       "                                                 \n",
       "  \u001b[1mmacro Avg\u001b[0m      0.9562   0.9545   0.9552   786  \n",
       "  \u001b[1mweighted Avg\u001b[0m   0.9556   0.9555   0.9554   786  \n",
       "                                                 \n",
       "  \u001b[1mAccuracy\u001b[0m                         \u001b[1m0.9555\u001b[0m   786  \n",
       "                                                 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "🎯 <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Holdout Confusion Matrix</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "🎯 \u001b[1;34mHoldout Confusion Matrix\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> CV completed. Best models and thresholds saved in subdirectories of <span style=\"color: #008000; text-decoration-color: #008000\">'./results_biomed_cv_train_only'</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m CV completed. Best models and thresholds saved in subdirectories of \u001b[32m'./results_biomed_cv_train_only'\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> Final evaluation performed on the original <span style=\"color: #008000; text-decoration-color: #008000\">'valid.csv'</span> using an ensemble of the best models from each CV \n",
       "fold.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m Final evaluation performed on the original \u001b[32m'valid.csv'\u001b[0m using an ensemble of the best models from each CV \n",
       "fold.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">🏁 Script finished.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m🏁 Script finished.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 605\u001b[0m\n\u001b[0;32m    602\u001b[0m console\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[bold green]🏁 Script finished.[/]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 605\u001b[0m     \u001b[43mmain\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "# Keep lightweight imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import gc\n",
    "# Defer heavy imports\n",
    "# from transformers import ...\n",
    "# from sklearn.metrics import ...\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration\n",
    "# -------------------------------\n",
    "SEED = 42\n",
    "MODEL_NAME = \"allenai/biomed_roberta_base\"\n",
    "N_SPLITS = 5\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 4 # Max epochs per fold, EarlyStopping will likely stop sooner\n",
    "WEIGHT_DECAY = 0.01\n",
    "# Output directory for CV models and results\n",
    "BASE_OUTPUT_DIR = \"./results_biomed_cv_train_only\"\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    console.print(f\"[cyan]CUDA available. Device: {torch.cuda.get_device_name(0)}[/]\")\n",
    "else:\n",
    "    console.print(\"[yellow]⚠️ CUDA not available. Training on CPU (will be very slow).[/]\")\n",
    "\n",
    "# --- Defer heavy library imports ---\n",
    "console.print(\"[dim]Importing libraries...[/]\")\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSequenceClassification,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        EarlyStoppingCallback\n",
    "    )\n",
    "    from sklearn.metrics import f1_score, precision_recall_curve, classification_report, confusion_matrix\n",
    "    from rich.panel import Panel\n",
    "    from rich import box\n",
    "    from rich.progress import track\n",
    "    from torch.utils.data import Dataset, DataLoader # Import DataLoader here\n",
    "except ImportError as e:\n",
    "    console.print(f\"[bold red]Error: Missing required library -> {e}[/]\")\n",
    "    console.print(\"[yellow]Please install all necessary libraries (pandas, torch, transformers, scikit-learn, rich, tqdm, accelerate).[/]\")\n",
    "    sys.exit(1)\n",
    "console.print(\"[green]✓ Libraries imported.[/]\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Data (Separately!)\n",
    "# -------------------------------\n",
    "try:\n",
    "    train_df = pd.read_csv(\"data/train.csv\")\n",
    "    holdout_valid_df = pd.read_csv(\"data/valid.csv\") # Load original valid set separately\n",
    "\n",
    "    # Rename columns consistently\n",
    "    train_df = train_df.rename(columns={\"labels\": \"label\"})\n",
    "    holdout_valid_df = holdout_valid_df.rename(columns={\"labels\": \"label\"})\n",
    "\n",
    "    # --- Clean Training Data ---\n",
    "    console.print(\"Cleaning Training Data...\")\n",
    "    initial_train_count = len(train_df)\n",
    "    if train_df['text'].isnull().any() or train_df['label'].isnull().any():\n",
    "        console.print(\"[yellow]⚠️ NaNs found in training data. Dropping rows...[/]\")\n",
    "        train_df.dropna(subset=['text', 'label'], inplace=True)\n",
    "    # Ensure label is int\n",
    "    train_df['label'] = pd.to_numeric(train_df['label'], errors='coerce')\n",
    "    train_df.dropna(subset=['label'], inplace=True)\n",
    "    train_df['label'] = train_df['label'].astype(int)\n",
    "    cleaned_train_count = len(train_df)\n",
    "    if cleaned_train_count < initial_train_count:\n",
    "        console.print(f\"[dim]Dropped {initial_train_count - cleaned_train_count} training rows due to NaNs.[/]\")\n",
    "\n",
    "    # --- Clean Holdout Validation Data ---\n",
    "    console.print(\"Cleaning Holdout Validation Data...\")\n",
    "    initial_valid_count = len(holdout_valid_df)\n",
    "    if holdout_valid_df['text'].isnull().any() or holdout_valid_df['label'].isnull().any():\n",
    "        console.print(\"[yellow]⚠️ NaNs found in holdout validation data. Dropping rows...[/]\")\n",
    "        holdout_valid_df.dropna(subset=['text', 'label'], inplace=True)\n",
    "     # Ensure label is int\n",
    "    holdout_valid_df['label'] = pd.to_numeric(holdout_valid_df['label'], errors='coerce')\n",
    "    holdout_valid_df.dropna(subset=['label'], inplace=True)\n",
    "    holdout_valid_df['label'] = holdout_valid_df['label'].astype(int)\n",
    "    cleaned_valid_count = len(holdout_valid_df)\n",
    "    if cleaned_valid_count < initial_valid_count:\n",
    "         console.print(f\"[dim]Dropped {initial_valid_count - cleaned_valid_count} holdout validation rows due to NaNs.[/]\")\n",
    "\n",
    "\n",
    "    console.print(f\"[green]✓ Training data loaded and cleaned. Total: {len(train_df)} examples.[/]\")\n",
    "    console.print(f\"[green]✓ Holdout validation data loaded and cleaned. Total: {len(holdout_valid_df)} examples.[/]\")\n",
    "\n",
    "    # Check label distribution in training data\n",
    "    train_label_counts = train_df['label'].value_counts()\n",
    "    console.print(f\"Training data label distribution:\\n{train_label_counts}\")\n",
    "    if not (0 in train_label_counts.index and 1 in train_label_counts.index):\n",
    "        console.print(\"[bold red]Error: Training data must contain both labels 0 and 1 for stratified splitting.[/]\")\n",
    "        exit()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    console.print(f\"[bold red]Error: CSV file not found - {e}.[/]\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    console.print(f\"[bold red]Error: Missing column in CSV - {e}.[/]\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]Unexpected error loading data: {e}[/]\")\n",
    "    exit()\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define Dataset Class (Modified for Inference OR Training)\n",
    "# -------------------------------\n",
    "class VaccineDataset(Dataset):\n",
    "    # Flag to indicate if labels should be included (for training/eval)\n",
    "    # or excluded (for prediction/inference on unlabeled data)\n",
    "    def __init__(self, texts, labels=None, ids=None, tokenizer=None, max_length=256, is_inference=False):\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided.\")\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.ids = ids # Store IDs if provided (useful for inference)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_inference = (labels is None) # Determine mode based on labels\n",
    "\n",
    "        if not self.is_inference and len(texts) != len(labels):\n",
    "            raise ValueError(\"Texts and Labels must have the same length for training/evaluation.\")\n",
    "        if self.ids is not None and len(texts) != len(self.ids):\n",
    "             raise ValueError(\"Texts and IDs must have the same length if IDs are provided.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx]) if idx < len(self.texts) and self.texts[idx] is not None else \"\"\n",
    "        item_id = self.ids[idx] if self.ids is not None and idx < len(self.ids) else idx # Use index as fallback ID\n",
    "\n",
    "        try:\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            # Use squeeze(0) to remove the batch dimension\n",
    "            item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "            # Add labels only if they exist (not in inference mode)\n",
    "            if not self.is_inference:\n",
    "                if idx < len(self.labels):\n",
    "                    label = self.labels[idx]\n",
    "                    item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "                else:\n",
    "                     # This case should ideally not happen if lengths match check passed\n",
    "                     item['labels'] = torch.tensor(-1, dtype=torch.long) # Invalid label\n",
    "\n",
    "            # Add ID if available\n",
    "            if self.ids is not None:\n",
    "                item['id'] = item_id\n",
    "\n",
    "            return item\n",
    "\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error in __getitem__ at index {idx} (ID: {item_id}): {e}[/]\")\n",
    "            # Return dummy tensors with invalid label, but keep ID\n",
    "            dummy_item = {\n",
    "                'input_ids': torch.zeros(self.max_length, dtype=torch.long),\n",
    "                'attention_mask': torch.zeros(self.max_length, dtype=torch.long),\n",
    "            }\n",
    "            if not self.is_inference:\n",
    "                dummy_item['labels'] = torch.tensor(-1, dtype=torch.long)\n",
    "            if self.ids is not None:\n",
    "                dummy_item['id'] = item_id\n",
    "            return dummy_item\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load Tokenizer (once)\n",
    "# -------------------------------\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    console.print(f\"[green]✓ Tokenizer loaded from '{MODEL_NAME}'.[/]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]Error loading tokenizer '{MODEL_NAME}': {e}[/]\")\n",
    "    exit()\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Metric Function (for Trainer, based on argmax)\n",
    "# -------------------------------\n",
    "def compute_metrics_for_trainer(eval_pred):\n",
    "    \"\"\"Calculates F1 based on argmax for checkpoint selection during training.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    valid_indices = labels != -1\n",
    "    labels = labels[valid_indices]\n",
    "    logits = logits[valid_indices]\n",
    "\n",
    "    if len(labels) == 0: return {\"f1\": 0.0}\n",
    "\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_score(labels, preds, average='binary', pos_label=1, zero_division=0)\n",
    "    return {\"f1\": f1}\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. Threshold Optimization Function\n",
    "# -----------------------------------\n",
    "def find_optimal_threshold(labels, probs):\n",
    "    \"\"\"Finds the threshold that maximizes the binary F1 score.\"\"\"\n",
    "    valid_indices = labels != -1\n",
    "    labels = labels[valid_indices]\n",
    "    probs = probs[valid_indices]\n",
    "\n",
    "    if len(labels) == 0 or len(np.unique(labels)) < 2:\n",
    "         console.print(\"[yellow]⚠️ Not enough valid data or classes for threshold optimization. Returning default 0.5.[/]\")\n",
    "         return 0.5, 0.0\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, probs, pos_label=1)\n",
    "    f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-9) # Epsilon for safety\n",
    "    f1_scores = np.nan_to_num(f1_scores) # Handle potential NaNs\n",
    "\n",
    "    if len(f1_scores) == 0: return 0.5, 0.0 # Fallback if precision_recall_curve had issues\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    best_f1 = f1_scores[best_f1_idx]\n",
    "    # Ensure index is valid for thresholds array\n",
    "    best_thresh = thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else thresholds[-1] if thresholds.size > 0 else 0.5\n",
    "\n",
    "    # Sanity check against 0.5 threshold\n",
    "    preds_at_05 = (probs >= 0.5).astype(int)\n",
    "    f1_at_05 = f1_score(labels, preds_at_05, pos_label=1, zero_division=0)\n",
    "    if f1_at_05 > best_f1:\n",
    "         # console.print(f\"[dim]Threshold 0.5 yields better F1 ({f1_at_05:.4f}). Using 0.5.[/]\")\n",
    "         return 0.5, f1_at_05\n",
    "\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Cross-Validation Loop (on train_df ONLY)\n",
    "# -------------------------------\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "fold_results = []\n",
    "best_model_paths = [] # Store paths to best model checkpoint of each fold\n",
    "\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "console.rule(\"[bold yellow]Starting Cross-Validation on Training Data[/]\")\n",
    "\n",
    "# Use train_df for splitting\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df['text'], train_df['label'])):\n",
    "    current_fold = fold + 1\n",
    "    console.rule(f\"[bold blue]CV Fold {current_fold}/{N_SPLITS}[/]\")\n",
    "\n",
    "    # --- Get fold data from train_df ---\n",
    "    train_fold_df = train_df.iloc[train_idx].copy()\n",
    "    fold_valid_df = train_df.iloc[val_idx].copy() # This is the validation set FOR THIS FOLD\n",
    "\n",
    "    console.print(f\"Fold Train Size: {len(train_fold_df)}, Fold Validation Size: {len(fold_valid_df)}\")\n",
    "    console.print(f\"Fold Train Labels:\\n{train_fold_df['label'].value_counts(dropna=False)}\")\n",
    "    console.print(f\"Fold Validation Labels:\\n{fold_valid_df['label'].value_counts(dropna=False)}\")\n",
    "\n",
    "    # --- Create fold datasets ---\n",
    "    try:\n",
    "        train_dataset = VaccineDataset(\n",
    "            texts=train_fold_df['text'].tolist(),\n",
    "            labels=train_fold_df['label'].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        # This validation dataset is used for evaluating epochs within this fold\n",
    "        fold_eval_dataset = VaccineDataset(\n",
    "            texts=fold_valid_df['text'].tolist(),\n",
    "            labels=fold_valid_df['label'].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        if len(train_dataset) == 0 or len(fold_eval_dataset) == 0:\n",
    "             console.print(f\"[bold red]Error: Empty dataset for fold {current_fold}. Skipping fold.[/]\")\n",
    "             continue\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error creating datasets for fold {current_fold}: {e}[/]\")\n",
    "        continue\n",
    "\n",
    "    # --- Load fresh model ---\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error loading model for fold {current_fold}: {e}[/]\")\n",
    "        continue\n",
    "\n",
    "    # --- Training Arguments ---\n",
    "    fold_output_dir = os.path.join(BASE_OUTPUT_DIR, f\"fold_{current_fold}\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=fold_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\", # Use F1 calculated by compute_metrics_for_trainer\n",
    "        greater_is_better=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=max(10, len(train_dataset) // (BATCH_SIZE * (torch.cuda.device_count() if torch.cuda.is_available() else 1) * 4)),\n",
    "        log_level=\"info\",\n",
    "        save_total_limit=2, # Keep best and last\n",
    "        seed=SEED + fold,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=[],\n",
    "        dataloader_num_workers=0, # Keep 0 for stability unless proven safe\n",
    "        dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        optim=\"adamw_torch\",\n",
    "    )\n",
    "\n",
    "    # --- Trainer Setup ---\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=fold_eval_dataset, # Use the fold's validation set for evaluation during training\n",
    "        compute_metrics=compute_metrics_for_trainer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.001)]\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    console.print(f\"🚀 Training Fold {current_fold}...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        console.print(f\"[green]✓ Training Fold {current_fold} completed.[/]\")\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error during training for fold {current_fold}: {e}[/]\")\n",
    "        del model, trainer, train_dataset, fold_eval_dataset; gc.collect(); torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "    # --- Evaluate on Fold's Validation Set & Optimize Threshold ---\n",
    "    console.print(f\"🔍 Evaluating and optimizing threshold for Fold {current_fold} (using its validation split)...\")\n",
    "    try:\n",
    "        # Predict on the fold's validation set\n",
    "        predictions_output = trainer.predict(fold_eval_dataset)\n",
    "        logits = predictions_output.predictions\n",
    "        labels = predictions_output.label_ids # True labels for this fold's validation split\n",
    "\n",
    "        if logits is not None and labels is not None:\n",
    "            # Calculate probabilities for the positive class (1)\n",
    "            if logits.shape[1] >= 2:\n",
    "                probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
    "            elif logits.shape[1] == 1:\n",
    "                 probs = torch.sigmoid(torch.tensor(logits)).squeeze(-1).numpy()\n",
    "            else: # Should not happen\n",
    "                 probs = np.array([])\n",
    "\n",
    "            if probs.size > 0:\n",
    "                # Find optimal threshold based on this fold's validation split\n",
    "                optimal_thresh, best_f1_val = find_optimal_threshold(labels, probs)\n",
    "\n",
    "                console.print(f\"[cyan]Fold {current_fold} - Best F1 on Fold Validation: {best_f1_val:.4f} @ Threshold = {optimal_thresh:.2f}[/]\")\n",
    "                fold_results.append({\n",
    "                    \"fold\": current_fold,\n",
    "                    \"best_f1_fold_val\": best_f1_val, # F1 on this fold's val set\n",
    "                    \"optimal_threshold\": optimal_thresh\n",
    "                })\n",
    "\n",
    "                # --- Save Threshold & Clean Checkpoints ---\n",
    "                best_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "                if best_checkpoint_path and os.path.isdir(best_checkpoint_path):\n",
    "                    best_model_paths.append(best_checkpoint_path) # Store path for later ensemble/evaluation\n",
    "                    threshold_file_path = os.path.join(best_checkpoint_path, \"threshold.txt\")\n",
    "                    try:\n",
    "                        with open(threshold_file_path, \"w\") as f: f.write(str(optimal_thresh))\n",
    "                        console.print(f\"[green]✓ Optimal threshold saved to {threshold_file_path}[/]\")\n",
    "                        # Clean other checkpoints in this fold's output dir\n",
    "                        # ... (logic for cleaning checkpoints, same as before) ...\n",
    "                        for item in os.listdir(fold_output_dir):\n",
    "                             item_path = os.path.join(fold_output_dir, item)\n",
    "                             if os.path.isdir(item_path) and item.startswith(\"checkpoint-\") and item_path != best_checkpoint_path:\n",
    "                                 if \"tmp\" not in item:\n",
    "                                     try: shutil.rmtree(item_path)\n",
    "                                     except OSError as rm_error: console.print(f\"[yellow]⚠️ Error deleting {item_path}: {rm_error}[/]\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        console.print(f\"[bold red]Error saving threshold for fold {current_fold}: {e}[/]\")\n",
    "                else:\n",
    "                    console.print(f\"[yellow]⚠️ No best checkpoint path found for fold {current_fold}. Cannot save threshold or use for ensemble.[/]\")\n",
    "                    # Optionally save the final state if needed\n",
    "                    final_model_path = os.path.join(fold_output_dir, \"final_model_no_best\")\n",
    "                    if not os.path.exists(final_model_path): # Avoid overwriting if it exists from previous attempts\n",
    "                         trainer.save_model(final_model_path)\n",
    "                         console.print(f\"[dim]Saved final model state to {final_model_path}[/]\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                 console.print(f\"[yellow]⚠️ No probabilities calculated for fold {current_fold}. Cannot optimize threshold.[/]\")\n",
    "\n",
    "        else:\n",
    "            console.print(f\"[yellow]⚠️ Prediction output missing logits or labels for fold {current_fold}.[/]\")\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error during evaluation/optimization for fold {current_fold}: {e}[/]\")\n",
    "        # Ensure memory is freed even if evaluation fails\n",
    "        del model, trainer, train_dataset, fold_eval_dataset; gc.collect(); torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "    # --- Free memory after fold completion ---\n",
    "    console.print(f\"[dim]Cleaning up memory for fold {current_fold}...[/dim]\")\n",
    "    del model, trainer, train_dataset, fold_eval_dataset, predictions_output, logits, labels, probs\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# --- End of CV Loop ---\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Display CV Summary\n",
    "# -------------------------------\n",
    "console.rule(\"[bold green]Cross-Validation Summary (on Train Splits)[/]\")\n",
    "if fold_results:\n",
    "    results_table = Table(title=\"Fold Performance on Internal Validation Splits\")\n",
    "    results_table.add_column(\"Fold\", style=\"cyan\")\n",
    "    results_table.add_column(\"Best F1 (Fold Val)\", style=\"magenta\", justify=\"right\")\n",
    "    results_table.add_column(\"Optimal Threshold\", style=\"yellow\", justify=\"right\")\n",
    "\n",
    "    all_f1s = [res[\"best_f1_fold_val\"] for res in fold_results]\n",
    "    all_thresholds = [res[\"optimal_threshold\"] for res in fold_results]\n",
    "\n",
    "    for res in fold_results:\n",
    "        results_table.add_row(\n",
    "            str(res[\"fold\"]),\n",
    "            f\"{res['best_f1_fold_val']:.4f}\",\n",
    "            f\"{res['optimal_threshold']:.2f}\"\n",
    "        )\n",
    "    console.print(results_table)\n",
    "\n",
    "    avg_f1 = np.mean(all_f1s)\n",
    "    std_f1 = np.std(all_f1s)\n",
    "    avg_thresh = np.mean(all_thresholds)\n",
    "    std_thresh = np.std(all_thresholds)\n",
    "\n",
    "    console.print(f\"\\nAvg F1 (Internal Validation): [bold cyan]{avg_f1:.4f} +/- {std_f1:.4f}[/]\")\n",
    "    console.print(f\"Avg Optimal Threshold: [bold cyan]{avg_thresh:.2f} +/- {std_thresh:.2f}[/]\")\n",
    "\n",
    "    if len(best_model_paths) != N_SPLITS:\n",
    "         console.print(f\"[yellow]⚠️ Found {len(best_model_paths)} best models, expected {N_SPLITS}. Ensemble evaluation might be affected.[/]\")\n",
    "\n",
    "else:\n",
    "    console.print(\"[yellow]No fold results recorded. Check for errors during training/evaluation.[/]\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 8. FINAL EVALUATION ON HOLDOUT VALIDATION SET (valid.csv)\n",
    "# Using Ensemble of Best Fold Models\n",
    "# -----------------------------------------------------\n",
    "console.rule(\"[bold magenta]Final Evaluation on Holdout Set (valid.csv)[/]\")\n",
    "\n",
    "if not best_model_paths:\n",
    "    console.print(\"[bold red]❌ No best models saved from CV folds. Cannot perform final evaluation.[/]\")\n",
    "    exit()\n",
    "if len(holdout_valid_df) == 0:\n",
    "    console.print(\"[bold red]❌ Holdout validation data is empty. Cannot perform final evaluation.[/]\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Create Dataset & Loader for Holdout Set ---\n",
    "console.print(f\"Preparing holdout validation dataset ({len(holdout_valid_df)} samples)...\")\n",
    "# Include IDs for potential later analysis, labels for evaluation\n",
    "holdout_dataset = VaccineDataset(\n",
    "    texts=holdout_valid_df['text'].tolist(),\n",
    "    labels=holdout_valid_df['label'].tolist(), # Include true labels\n",
    "    ids=holdout_valid_df['id'].tolist(),       # Include IDs\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    is_inference=False # We have labels for evaluation\n",
    ")\n",
    "holdout_loader = DataLoader(holdout_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "# Get true labels in the correct order\n",
    "holdout_y_true = np.array(holdout_valid_df['label'].tolist())\n",
    "\n",
    "\n",
    "# --- Ensemble Inference on Holdout Set ---\n",
    "console.print(f\"Running ensemble inference on holdout set using {len(best_model_paths)} models...\")\n",
    "all_holdout_probs_np = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Ensure device is set\n",
    "\n",
    "for i, model_p in enumerate(best_model_paths):\n",
    "    console.print(f\"--- Loading model {i+1}/{len(best_model_paths)} from [yellow]{os.path.basename(model_p)}[/] ---\")\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_p).to(device).eval()\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]❌ Error loading model {model_p}: {e}. Skipping.[/]\")\n",
    "        continue\n",
    "\n",
    "    fold_holdout_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in track(holdout_loader, description=f\"Predicting (Holdout, Model {i+1})...\", console=console, transient=False):\n",
    "            # We don't need IDs here unless debugging, but ensure data is valid\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            # Skip if input_ids are all zero (potential dummy data from Dataset __getitem__)\n",
    "            is_dummy = torch.all(input_ids == 0, dim=1)\n",
    "            valid_indices_batch = ~is_dummy\n",
    "            if not torch.any(valid_indices_batch): continue\n",
    "\n",
    "            try:\n",
    "                outputs = model(input_ids=input_ids[valid_indices_batch], attention_mask=attention_mask[valid_indices_batch])\n",
    "                logits = outputs.logits\n",
    "                if logits.shape[1] >= 2: probs = torch.softmax(logits, dim=1)[:, 1]\n",
    "                elif logits.shape[1] == 1: probs = torch.sigmoid(logits).squeeze(-1)\n",
    "                else: probs = torch.zeros(torch.sum(valid_indices_batch), device=device)\n",
    "\n",
    "                # Store probabilities for valid items in batch\n",
    "                batch_probs = torch.zeros(len(batch[\"input_ids\"]), device='cpu') # Default to 0 prob\n",
    "                batch_probs[valid_indices_batch.cpu()] = probs.cpu()\n",
    "                fold_holdout_probs.extend(batch_probs.numpy())\n",
    "\n",
    "            except Exception as pred_e:\n",
    "                console.print(f\"\\n[bold red]❌ Error during holdout prediction batch: {pred_e}[/]\")\n",
    "                 # Add zeros for the failed batch to maintain length\n",
    "                fold_holdout_probs.extend(np.zeros(len(batch[\"input_ids\"])))\n",
    "\n",
    "\n",
    "    # Ensure length matches dataset size, pad with zeros if needed (e.g., if last batches failed)\n",
    "    if len(fold_holdout_probs) < len(holdout_dataset):\n",
    "         fold_holdout_probs.extend(np.zeros(len(holdout_dataset) - len(fold_holdout_probs)))\n",
    "\n",
    "    all_holdout_probs_np.append(np.array(fold_holdout_probs[:len(holdout_dataset)])) # Ensure correct length\n",
    "    console.print(f\"[green]✓ Holdout predictions collected for model {i+1}.[/]\")\n",
    "\n",
    "    # Free memory\n",
    "    del model, outputs, logits, probs; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- Aggregate and Evaluate Holdout Predictions ---\n",
    "if not all_holdout_probs_np:\n",
    "    console.print(\"[bold red]❌ No predictions generated for the holdout set.[/]\")\n",
    "else:\n",
    "    num_ensemble_models = len(all_holdout_probs_np)\n",
    "    console.print(f\"\\n[bold cyan]Combining holdout probabilities from {num_ensemble_models} models (averaging)...[/]\")\n",
    "    holdout_avg_probs = np.mean(np.array(all_holdout_probs_np), axis=0)\n",
    "\n",
    "    # Use the average of thresholds found during CV\n",
    "    final_holdout_threshold = np.mean([res[\"optimal_threshold\"] for res in fold_results if \"optimal_threshold\" in res]) if fold_results else 0.5\n",
    "    console.print(f\"Applying average threshold from CV: [yellow]{final_holdout_threshold:.4f}[/]\")\n",
    "\n",
    "    holdout_predictions = (holdout_avg_probs >= final_holdout_threshold).astype(int)\n",
    "\n",
    "    # --- Final Report on Holdout Set ---\n",
    "    console.rule(\"[bold magenta]Final Ensemble Performance on Holdout Set (valid.csv)[/]\")\n",
    "    try:\n",
    "        # Filter true labels and predictions where true labels are valid (-1 was used for errors)\n",
    "        valid_indices_holdout = holdout_y_true != -1\n",
    "        y_true_holdout_eval = holdout_y_true[valid_indices_holdout]\n",
    "        y_pred_holdout_eval = holdout_predictions[valid_indices_holdout]\n",
    "\n",
    "        if len(y_true_holdout_eval) == 0:\n",
    "             console.print(\"[yellow]⚠️ No valid labels in the holdout set to evaluate against.[/]\")\n",
    "        else:\n",
    "             console.print(f\"Evaluating on {len(y_true_holdout_eval)} holdout samples.\")\n",
    "             report = classification_report(y_true_holdout_eval, y_pred_holdout_eval, output_dict=True, digits=4, zero_division=0)\n",
    "\n",
    "             # Display Report Table (similar to the one used in classify.py)\n",
    "             report_table = Table(show_header=True, header_style=\"bold magenta\", box=box.SIMPLE)\n",
    "             # ... (Add columns) ...\n",
    "             labels_in_report = [label for label in sorted(report.keys()) if label not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "             for label in labels_in_report:\n",
    "                 metrics = report[label]\n",
    "                 report_table.add_row(str(label), f\"{metrics['precision']:.4f}\", f\"{metrics['recall']:.4f}\", f\"{metrics['f1-score']:.4f}\", f\"{int(metrics['support'])}\")\n",
    "             report_table.add_section()\n",
    "             for avg_type in [\"macro avg\", \"weighted avg\"]:\n",
    "                 if avg_type in report: metrics = report[avg_type]; name = avg_type.replace(\" avg\", \" Avg\"); report_table.add_row(f\"[bold]{name}[/]\", f\"{metrics['precision']:.4f}\", f\"{metrics['recall']:.4f}\", f\"{metrics['f1-score']:.4f}\", f\"{int(metrics['support'])}\")\n",
    "             if \"accuracy\" in report: accuracy = report[\"accuracy\"]; total_support = int(report[\"weighted avg\"][\"support\"]) if \"weighted avg\" in report else len(y_true_holdout_eval) ; report_table.add_section(); report_table.add_row(\"[bold]Accuracy[/]\", \"\", \"\", f\"[bold]{accuracy:.4f}[/]\", f\"{total_support}\")\n",
    "             console.print(report_table)\n",
    "\n",
    "             # Display Confusion Matrix (similar to classify.py)\n",
    "             console.print(\"\\n🎯 [bold blue]Holdout Confusion Matrix[/bold blue]\\n\")\n",
    "             cm_labels = sorted(list(set(y_true_holdout_eval) | set(y_pred_holdout_eval)))\n",
    "             if not cm_labels: cm_labels = [0, 1]\n",
    "             cm = confusion_matrix(y_true_holdout_eval, y_pred_holdout_eval, labels=cm_labels)\n",
    "             cm_table = Table(title=\"True \\\\ Predicted\", box=box.SIMPLE_HEAVY, show_header=True, header_style=\"bold\")\n",
    "             # ... (Add columns and rows based on cm_labels and cm) ...\n",
    "             console.print(cm_table)\n",
    "\n",
    "    except Exception as report_e:\n",
    "        console.print(f\"[bold red]❌ Error generating final holdout report: {report_e}[/]\")\n",
    "\n",
    "\n",
    "console.print(f\"\\n[INFO] CV completed. Best models and thresholds saved in subdirectories of '{BASE_OUTPUT_DIR}'.\")\n",
    "console.print(\"[INFO] Final evaluation performed on the original 'valid.csv' using an ensemble of the best models from each CV fold.\")\n",
    "console.print(\"[bold green]🏁 Script finished.[/]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Final Ensemble Performance on Holdout Set (valid.csv)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────────────────────────────── \u001b[0m\u001b[1;35mFinal Ensemble Performance on Holdout Set \u001b[0m\u001b[1;35m(\u001b[0m\u001b[1;35mvalid.csv\u001b[0m\u001b[1;35m)\u001b[0m\u001b[92m ──────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">786</span> holdout samples.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating on \u001b[1;36m786\u001b[0m holdout samples.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                 \n",
       " <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">              </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">        </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">        </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">        </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">     </span> \n",
       " ─────────────────────────────────────────────── \n",
       "  0              0.9487   0.9690   0.9588   420  \n",
       "  1              0.9636   0.9399   0.9516   366  \n",
       "                                                 \n",
       "  <span style=\"font-weight: bold\">macro Avg</span>      0.9562   0.9545   0.9552   786  \n",
       "  <span style=\"font-weight: bold\">weighted Avg</span>   0.9556   0.9555   0.9554   786  \n",
       "                                                 \n",
       "  <span style=\"font-weight: bold\">Accuracy</span>                         <span style=\"font-weight: bold\">0.9555</span>   786  \n",
       "                                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                 \n",
       " \u001b[1;35m \u001b[0m\u001b[1;35m            \u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35m      \u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35m      \u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35m      \u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35m   \u001b[0m\u001b[1;35m \u001b[0m \n",
       " ─────────────────────────────────────────────── \n",
       "  0              0.9487   0.9690   0.9588   420  \n",
       "  1              0.9636   0.9399   0.9516   366  \n",
       "                                                 \n",
       "  \u001b[1mmacro Avg\u001b[0m      0.9562   0.9545   0.9552   786  \n",
       "  \u001b[1mweighted Avg\u001b[0m   0.9556   0.9555   0.9554   786  \n",
       "                                                 \n",
       "  \u001b[1mAccuracy\u001b[0m                         \u001b[1m0.9555\u001b[0m   786  \n",
       "                                                 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "🎯 <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Holdout Confusion Matrix</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "🎯 \u001b[1;34mHoldout Confusion Matrix\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Final Report on Holdout Set ---\n",
    "console.rule(\"[bold magenta]Final Ensemble Performance on Holdout Set (valid.csv)[/]\")\n",
    "try:\n",
    "    # Filter true labels and predictions where true labels are valid (-1 was used for errors)\n",
    "    valid_indices_holdout = holdout_y_true != -1\n",
    "    y_true_holdout_eval = holdout_y_true[valid_indices_holdout]\n",
    "    y_pred_holdout_eval = holdout_predictions[valid_indices_holdout]\n",
    "\n",
    "    if len(y_true_holdout_eval) == 0:\n",
    "            console.print(\"[yellow]⚠️ No valid labels in the holdout set to evaluate against.[/]\")\n",
    "    else:\n",
    "            console.print(f\"Evaluating on {len(y_true_holdout_eval)} holdout samples.\")\n",
    "            report = classification_report(y_true_holdout_eval, y_pred_holdout_eval, output_dict=True, digits=4, zero_division=0)\n",
    "\n",
    "            # Display Report Table (similar to the one used in classify.py)\n",
    "            report_table = Table(show_header=True, header_style=\"bold magenta\", box=box.SIMPLE)\n",
    "            # ... (Add columns) ...\n",
    "            labels_in_report = [label for label in sorted(report.keys()) if label not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "            for label in labels_in_report:\n",
    "                metrics = report[label]\n",
    "                report_table.add_row(str(label), f\"{metrics['precision']:.4f}\", f\"{metrics['recall']:.4f}\", f\"{metrics['f1-score']:.4f}\", f\"{int(metrics['support'])}\")\n",
    "            report_table.add_section()\n",
    "            for avg_type in [\"macro avg\", \"weighted avg\"]:\n",
    "                if avg_type in report: metrics = report[avg_type]; name = avg_type.replace(\" avg\", \" Avg\"); report_table.add_row(f\"[bold]{name}[/]\", f\"{metrics['precision']:.4f}\", f\"{metrics['recall']:.4f}\", f\"{metrics['f1-score']:.4f}\", f\"{int(metrics['support'])}\")\n",
    "            if \"accuracy\" in report: accuracy = report[\"accuracy\"]; total_support = int(report[\"weighted avg\"][\"support\"]) if \"weighted avg\" in report else len(y_true_holdout_eval) ; report_table.add_section(); report_table.add_row(\"[bold]Accuracy[/]\", \"\", \"\", f\"[bold]{accuracy:.4f}[/]\", f\"{total_support}\")\n",
    "            console.print(report_table)\n",
    "\n",
    "            # Display Confusion Matrix (similar to classify.py)\n",
    "            console.print(\"\\n🎯 [bold blue]Holdout Confusion Matrix[/bold blue]\\n\")\n",
    "            cm_labels = sorted(list(set(y_true_holdout_eval) | set(y_pred_holdout_eval)))\n",
    "            if not cm_labels: cm_labels = [0, 1]\n",
    "            cm = confusion_matrix(y_true_holdout_eval, y_pred_holdout_eval, labels=cm_labels)\n",
    "            cm_table = Table(title=\"True \\\\ Predicted\", box=box.SIMPLE_HEAVY, show_header=True, header_style=\"bold\")\n",
    "            # ... (Add columns and rows based on cm_labels and cm) ...\n",
    "            console.print(cm_table)\n",
    "\n",
    "except Exception as report_e:\n",
    "    console.print(f\"[bold red]❌ Error generating final holdout report: {report_e}[/]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "print(config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biomed Roberta base + weighted loss cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">CUDA available. Using </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">GPU(</span><span style=\"color: #008080; text-decoration-color: #008080\">s</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">. Device: NVIDIA GeForce RTX </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3070</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCUDA available. Using \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m \u001b[0m\u001b[1;36mGPU\u001b[0m\u001b[1;36m(\u001b[0m\u001b[36ms\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m. Device: NVIDIA GeForce RTX \u001b[0m\u001b[1;36m3070\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Effective Batch Size: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #008080; text-decoration-color: #008080\">, Per-Device Batch Size: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #008080; text-decoration-color: #008080\">, Num GPUs: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> =&gt; Gradient Accumulation Steps: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mEffective Batch Size: \u001b[0m\u001b[1;36m32\u001b[0m\u001b[36m, Per-Device Batch Size: \u001b[0m\u001b[1;36m8\u001b[0m\u001b[36m, Num GPUs: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m => Gradient Accumulation Steps: \u001b[0m\u001b[1;36m4\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Importing libraries</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mImporting libraries\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Libraries imported.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Libraries imported.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cleaning Training Data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cleaning Training Data\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training data loaded and cleaned. Total: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2521</span><span style=\"color: #008000; text-decoration-color: #008000\"> examples.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training data loaded and cleaned. Total: \u001b[0m\u001b[1;32m2521\u001b[0m\u001b[32m examples.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Training data label distribution:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1372</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1149</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Training data label distribution:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1372\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m1149\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cleaning Holdout Validation Data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cleaning Holdout Validation Data\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout Validation data loaded and cleaned. Total: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">786</span><span style=\"color: #008000; text-decoration-color: #008000\"> examples.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout Validation data loaded and cleaned. Total: \u001b[0m\u001b[1;32m786\u001b[0m\u001b[32m examples.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Holdout Validation data label distribution:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">420</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">366</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Holdout Validation data label distribution:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m420\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m366\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Tokenizer loaded from </span><span style=\"color: #008000; text-decoration-color: #008000\">'allenai/biomed_roberta_base'</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Tokenizer loaded from \u001b[0m\u001b[32m'allenai/biomed_roberta_base'\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────── </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Starting Cross-Validation on Training Data</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────── \u001b[0m\u001b[1;33mStarting Cross-Validation on Training Data\u001b[0m\u001b[92m ────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">505</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2016\u001b[0m, Fold Validation Size: \u001b[1;36m505\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1097</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1097\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">275</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m275\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculated class weights for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9188697</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0968444</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Calculated class weights for Fold \u001b[1;36m1\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.9188697\u001b[0m \u001b[1;36m1.0968444\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Custom Trainer initialized with class weights </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">stored on CPU</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9188697</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0968444</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCustom Trainer initialized with class weights \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mstored on CPU\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m0.9188697\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m1.0968444\u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m1\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,016\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 378\n",
      "  Number of trainable parameters = 124,647,170\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='378' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [378/378 04:40, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.172794</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.335126</td>\n",
       "      <td>0.895874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.159350</td>\n",
       "      <td>0.954447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.240542</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.325354</td>\n",
       "      <td>0.935818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.244121</td>\n",
       "      <td>0.949153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_1_checkpoints\\checkpoint-63\n",
      "Configuration saved in ./models\\fold_1_checkpoints\\checkpoint-63\\config.json\n",
      "Model weights saved in ./models\\fold_1_checkpoints\\checkpoint-63\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_1_checkpoints\\checkpoint-126\n",
      "Configuration saved in ./models\\fold_1_checkpoints\\checkpoint-126\\config.json\n",
      "Model weights saved in ./models\\fold_1_checkpoints\\checkpoint-126\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_1_checkpoints\\checkpoint-189\n",
      "Configuration saved in ./models\\fold_1_checkpoints\\checkpoint-189\\config.json\n",
      "Model weights saved in ./models\\fold_1_checkpoints\\checkpoint-189\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_1_checkpoints\\checkpoint-63] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_1_checkpoints\\checkpoint-252\n",
      "Configuration saved in ./models\\fold_1_checkpoints\\checkpoint-252\\config.json\n",
      "Model weights saved in ./models\\fold_1_checkpoints\\checkpoint-252\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_1_checkpoints\\checkpoint-126] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_1_checkpoints\\checkpoint-315\n",
      "Configuration saved in ./models\\fold_1_checkpoints\\checkpoint-315\\config.json\n",
      "Model weights saved in ./models\\fold_1_checkpoints\\checkpoint-315\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_1_checkpoints\\checkpoint-252] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_1_checkpoints\\checkpoint-378\n",
      "Configuration saved in ./models\\fold_1_checkpoints\\checkpoint-378\\config.json\n",
      "Model weights saved in ./models\\fold_1_checkpoints\\checkpoint-378\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_1_checkpoints\\checkpoint-315] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models\\fold_1_checkpoints\\checkpoint-189 (score: 0.9544468546637744).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed after </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">281.</span><span style=\"color: #008000; text-decoration-color: #008000\">03s.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m completed after \u001b[0m\u001b[1;32m281.\u001b[0m\u001b[32m03s.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 score on internal validation set during training: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9544</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m - Best F1 score on internal validation set during training: \u001b[0m\u001b[1;36m0.9544\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and finding best threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> <span style=\"font-weight: bold\">(</span>using its internal validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and finding best threshold for Fold \u001b[1;36m1\u001b[0m \u001b[1m(\u001b[0musing its internal validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> Internal Validation Metrics <span style=\"font-weight: bold\">(</span>at threshold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'test_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.15934990346431732</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_f1'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9544468546637744</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_runtime'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.5931</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_samples_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">194.748</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_steps_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.34</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m1\u001b[0m Internal Validation Metrics \u001b[1m(\u001b[0mat threshold \u001b[1;36m0.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'test_loss'\u001b[0m: \u001b[1;36m0.15934990346431732\u001b[0m, \u001b[32m'test_f1'\u001b[0m: \n",
       "\u001b[1;36m0.9544468546637744\u001b[0m, \u001b[32m'test_runtime'\u001b[0m: \u001b[1;36m2.5931\u001b[0m, \u001b[32m'test_samples_per_second'\u001b[0m: \u001b[1;36m194.748\u001b[0m, \u001b[32m'test_steps_per_second'\u001b[0m: \u001b[1;36m12.34\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Internal Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9544</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Optimized Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6125</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m - Best F1 on Internal Validation: \u001b[0m\u001b[1;36m0.9544\u001b[0m\u001b[36m @ Optimized Threshold = \u001b[0m\u001b[1;36m0.6125\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\\fold_1_best\n",
      "Configuration saved in ./models\\fold_1_best\\config.json\n",
      "Model weights saved in ./models\\fold_1_best\\model.safetensors\n",
      "tokenizer config file saved in ./models\\fold_1_best\\tokenizer_config.json\n",
      "Special tokens file saved in ./models\\fold_1_best\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Best model for Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\fold_1_best'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Best model for Fold \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\fold_1_best'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up checkpoints directory: </span><span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'./models\\fold_1_checkpoints'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up checkpoints directory: \u001b[0m\u001b[2;32m'./models\\fold_1_checkpoints'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">1</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m2\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1098</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1098\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">274</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m274\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculated class weights for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span> <span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Calculated class weights for Fold \u001b[1;36m2\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m \u001b[1;36m1.0973885\u001b[0m \u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Custom Trainer initialized with class weights </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">stored on CPU</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCustom Trainer initialized with class weights \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mstored on CPU\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m1.0973885\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m2\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 378\n",
      "  Number of trainable parameters = 124,647,170\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='378' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [378/378 04:44, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.197215</td>\n",
       "      <td>0.929461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.140900</td>\n",
       "      <td>0.156370</td>\n",
       "      <td>0.947146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.162186</td>\n",
       "      <td>0.946903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.203400</td>\n",
       "      <td>0.949367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.207048</td>\n",
       "      <td>0.955032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_2_checkpoints\\checkpoint-64\n",
      "Configuration saved in ./models\\fold_2_checkpoints\\checkpoint-64\\config.json\n",
      "Model weights saved in ./models\\fold_2_checkpoints\\checkpoint-64\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_2_checkpoints\\checkpoint-128\n",
      "Configuration saved in ./models\\fold_2_checkpoints\\checkpoint-128\\config.json\n",
      "Model weights saved in ./models\\fold_2_checkpoints\\checkpoint-128\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_2_checkpoints\\checkpoint-192\n",
      "Configuration saved in ./models\\fold_2_checkpoints\\checkpoint-192\\config.json\n",
      "Model weights saved in ./models\\fold_2_checkpoints\\checkpoint-192\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_2_checkpoints\\checkpoint-64] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_2_checkpoints\\checkpoint-256\n",
      "Configuration saved in ./models\\fold_2_checkpoints\\checkpoint-256\\config.json\n",
      "Model weights saved in ./models\\fold_2_checkpoints\\checkpoint-256\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_2_checkpoints\\checkpoint-128] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_2_checkpoints\\checkpoint-320\n",
      "Configuration saved in ./models\\fold_2_checkpoints\\checkpoint-320\\config.json\n",
      "Model weights saved in ./models\\fold_2_checkpoints\\checkpoint-320\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_2_checkpoints\\checkpoint-192] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_2_checkpoints\\checkpoint-378\n",
      "Configuration saved in ./models\\fold_2_checkpoints\\checkpoint-378\\config.json\n",
      "Model weights saved in ./models\\fold_2_checkpoints\\checkpoint-378\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_2_checkpoints\\checkpoint-256] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models\\fold_2_checkpoints\\checkpoint-378 (score: 0.9550321199143469).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed after </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">285.</span><span style=\"color: #008000; text-decoration-color: #008000\">27s.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m2\u001b[0m\u001b[32m completed after \u001b[0m\u001b[1;32m285.\u001b[0m\u001b[32m27s.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 score on internal validation set during training: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9550</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m2\u001b[0m\u001b[36m - Best F1 score on internal validation set during training: \u001b[0m\u001b[1;36m0.9550\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and finding best threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> <span style=\"font-weight: bold\">(</span>using its internal validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and finding best threshold for Fold \u001b[1;36m2\u001b[0m \u001b[1m(\u001b[0musing its internal validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> Internal Validation Metrics <span style=\"font-weight: bold\">(</span>at threshold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'test_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.20704832673072815</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_f1'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9550321199143469</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_runtime'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.5629</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_samples_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">196.651</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_steps_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.486</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m2\u001b[0m Internal Validation Metrics \u001b[1m(\u001b[0mat threshold \u001b[1;36m0.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'test_loss'\u001b[0m: \u001b[1;36m0.20704832673072815\u001b[0m, \u001b[32m'test_f1'\u001b[0m: \n",
       "\u001b[1;36m0.9550321199143469\u001b[0m, \u001b[32m'test_runtime'\u001b[0m: \u001b[1;36m2.5629\u001b[0m, \u001b[32m'test_samples_per_second'\u001b[0m: \u001b[1;36m196.651\u001b[0m, \u001b[32m'test_steps_per_second'\u001b[0m: \u001b[1;36m12.486\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Internal Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9654</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Optimized Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9707</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m2\u001b[0m\u001b[36m - Best F1 on Internal Validation: \u001b[0m\u001b[1;36m0.9654\u001b[0m\u001b[36m @ Optimized Threshold = \u001b[0m\u001b[1;36m0.9707\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\\fold_2_best\n",
      "Configuration saved in ./models\\fold_2_best\\config.json\n",
      "Model weights saved in ./models\\fold_2_best\\model.safetensors\n",
      "tokenizer config file saved in ./models\\fold_2_best\\tokenizer_config.json\n",
      "Special tokens file saved in ./models\\fold_2_best\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Best model for Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\fold_2_best'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Best model for Fold \u001b[0m\u001b[1;32m2\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\fold_2_best'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up checkpoints directory: </span><span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'./models\\fold_2_checkpoints'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up checkpoints directory: \u001b[0m\u001b[2;32m'./models\\fold_2_checkpoints'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">2</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">3</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m3\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1098</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1098\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">274</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m274\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculated class weights for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span> <span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Calculated class weights for Fold \u001b[1;36m3\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m \u001b[1;36m1.0973885\u001b[0m \u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Custom Trainer initialized with class weights </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">stored on CPU</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCustom Trainer initialized with class weights \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mstored on CPU\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m1.0973885\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m3\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Safetensors PR exists\n",
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 378\n",
      "  Number of trainable parameters = 124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='378' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [378/378 04:38, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.317100</td>\n",
       "      <td>0.260128</td>\n",
       "      <td>0.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>0.156316</td>\n",
       "      <td>0.950959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.162200</td>\n",
       "      <td>0.202659</td>\n",
       "      <td>0.943396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.181967</td>\n",
       "      <td>0.959488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.207376</td>\n",
       "      <td>0.957082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_3_checkpoints\\checkpoint-64\n",
      "Configuration saved in ./models\\fold_3_checkpoints\\checkpoint-64\\config.json\n",
      "Model weights saved in ./models\\fold_3_checkpoints\\checkpoint-64\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_3_checkpoints\\checkpoint-128\n",
      "Configuration saved in ./models\\fold_3_checkpoints\\checkpoint-128\\config.json\n",
      "Model weights saved in ./models\\fold_3_checkpoints\\checkpoint-128\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_3_checkpoints\\checkpoint-192\n",
      "Configuration saved in ./models\\fold_3_checkpoints\\checkpoint-192\\config.json\n",
      "Model weights saved in ./models\\fold_3_checkpoints\\checkpoint-192\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_3_checkpoints\\checkpoint-64] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_3_checkpoints\\checkpoint-256\n",
      "Configuration saved in ./models\\fold_3_checkpoints\\checkpoint-256\\config.json\n",
      "Model weights saved in ./models\\fold_3_checkpoints\\checkpoint-256\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_3_checkpoints\\checkpoint-128] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_3_checkpoints\\checkpoint-320\n",
      "Configuration saved in ./models\\fold_3_checkpoints\\checkpoint-320\\config.json\n",
      "Model weights saved in ./models\\fold_3_checkpoints\\checkpoint-320\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_3_checkpoints\\checkpoint-192] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_3_checkpoints\\checkpoint-378\n",
      "Configuration saved in ./models\\fold_3_checkpoints\\checkpoint-378\\config.json\n",
      "Model weights saved in ./models\\fold_3_checkpoints\\checkpoint-378\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_3_checkpoints\\checkpoint-320] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models\\fold_3_checkpoints\\checkpoint-256 (score: 0.9594882729211087).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed after </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">278.</span><span style=\"color: #008000; text-decoration-color: #008000\">99s.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m completed after \u001b[0m\u001b[1;32m278.\u001b[0m\u001b[32m99s.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 score on internal validation set during training: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9595</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m3\u001b[0m\u001b[36m - Best F1 score on internal validation set during training: \u001b[0m\u001b[1;36m0.9595\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and finding best threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> <span style=\"font-weight: bold\">(</span>using its internal validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and finding best threshold for Fold \u001b[1;36m3\u001b[0m \u001b[1m(\u001b[0musing its internal validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> Internal Validation Metrics <span style=\"font-weight: bold\">(</span>at threshold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'test_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.18196691572666168</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_f1'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9594882729211087</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_runtime'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.7497</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_samples_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">183.296</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_steps_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.638</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m3\u001b[0m Internal Validation Metrics \u001b[1m(\u001b[0mat threshold \u001b[1;36m0.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'test_loss'\u001b[0m: \u001b[1;36m0.18196691572666168\u001b[0m, \u001b[32m'test_f1'\u001b[0m: \n",
       "\u001b[1;36m0.9594882729211087\u001b[0m, \u001b[32m'test_runtime'\u001b[0m: \u001b[1;36m2.7497\u001b[0m, \u001b[32m'test_samples_per_second'\u001b[0m: \u001b[1;36m183.296\u001b[0m, \u001b[32m'test_steps_per_second'\u001b[0m: \u001b[1;36m11.638\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Internal Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9597</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Optimized Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2841</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m3\u001b[0m\u001b[36m - Best F1 on Internal Validation: \u001b[0m\u001b[1;36m0.9597\u001b[0m\u001b[36m @ Optimized Threshold = \u001b[0m\u001b[1;36m0.2841\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\\fold_3_best\n",
      "Configuration saved in ./models\\fold_3_best\\config.json\n",
      "Model weights saved in ./models\\fold_3_best\\model.safetensors\n",
      "tokenizer config file saved in ./models\\fold_3_best\\tokenizer_config.json\n",
      "Special tokens file saved in ./models\\fold_3_best\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Best model for Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\fold_3_best'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Best model for Fold \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\fold_3_best'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up checkpoints directory: </span><span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'./models\\fold_3_checkpoints'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up checkpoints directory: \u001b[0m\u001b[2;32m'./models\\fold_3_checkpoints'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">3</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">4</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m4\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1098</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1098\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">274</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m274\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculated class weights for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span> <span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Calculated class weights for Fold \u001b[1;36m4\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m \u001b[1;36m1.0973885\u001b[0m \u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Custom Trainer initialized with class weights </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">stored on CPU</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCustom Trainer initialized with class weights \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mstored on CPU\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m1.0973885\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m4\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Safetensors PR exists\n",
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 378\n",
      "  Number of trainable parameters = 124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='378' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [378/378 04:33, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.223960</td>\n",
       "      <td>0.920502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.930131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.197596</td>\n",
       "      <td>0.944915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.225438</td>\n",
       "      <td>0.947589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.282796</td>\n",
       "      <td>0.943158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_4_checkpoints\\checkpoint-64\n",
      "Configuration saved in ./models\\fold_4_checkpoints\\checkpoint-64\\config.json\n",
      "Model weights saved in ./models\\fold_4_checkpoints\\checkpoint-64\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_4_checkpoints\\checkpoint-128\n",
      "Configuration saved in ./models\\fold_4_checkpoints\\checkpoint-128\\config.json\n",
      "Model weights saved in ./models\\fold_4_checkpoints\\checkpoint-128\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_4_checkpoints\\checkpoint-192\n",
      "Configuration saved in ./models\\fold_4_checkpoints\\checkpoint-192\\config.json\n",
      "Model weights saved in ./models\\fold_4_checkpoints\\checkpoint-192\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_4_checkpoints\\checkpoint-64] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_4_checkpoints\\checkpoint-256\n",
      "Configuration saved in ./models\\fold_4_checkpoints\\checkpoint-256\\config.json\n",
      "Model weights saved in ./models\\fold_4_checkpoints\\checkpoint-256\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_4_checkpoints\\checkpoint-128] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_4_checkpoints\\checkpoint-320\n",
      "Configuration saved in ./models\\fold_4_checkpoints\\checkpoint-320\\config.json\n",
      "Model weights saved in ./models\\fold_4_checkpoints\\checkpoint-320\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_4_checkpoints\\checkpoint-192] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_4_checkpoints\\checkpoint-378\n",
      "Configuration saved in ./models\\fold_4_checkpoints\\checkpoint-378\\config.json\n",
      "Model weights saved in ./models\\fold_4_checkpoints\\checkpoint-378\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_4_checkpoints\\checkpoint-320] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models\\fold_4_checkpoints\\checkpoint-256 (score: 0.9475890985324947).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed after </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">274.</span><span style=\"color: #008000; text-decoration-color: #008000\">41s.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m completed after \u001b[0m\u001b[1;32m274.\u001b[0m\u001b[32m41s.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 score on internal validation set during training: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9476</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m - Best F1 score on internal validation set during training: \u001b[0m\u001b[1;36m0.9476\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and finding best threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> <span style=\"font-weight: bold\">(</span>using its internal validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and finding best threshold for Fold \u001b[1;36m4\u001b[0m \u001b[1m(\u001b[0musing its internal validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> Internal Validation Metrics <span style=\"font-weight: bold\">(</span>at threshold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'test_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2254377156496048</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_f1'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9475890985324947</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_runtime'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.5993</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_samples_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">193.902</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_steps_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.311</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m4\u001b[0m Internal Validation Metrics \u001b[1m(\u001b[0mat threshold \u001b[1;36m0.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'test_loss'\u001b[0m: \u001b[1;36m0.2254377156496048\u001b[0m, \u001b[32m'test_f1'\u001b[0m: \n",
       "\u001b[1;36m0.9475890985324947\u001b[0m, \u001b[32m'test_runtime'\u001b[0m: \u001b[1;36m2.5993\u001b[0m, \u001b[32m'test_samples_per_second'\u001b[0m: \u001b[1;36m193.902\u001b[0m, \u001b[32m'test_steps_per_second'\u001b[0m: \u001b[1;36m12.311\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Internal Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9478</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Optimized Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3820</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m - Best F1 on Internal Validation: \u001b[0m\u001b[1;36m0.9478\u001b[0m\u001b[36m @ Optimized Threshold = \u001b[0m\u001b[1;36m0.3820\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\\fold_4_best\n",
      "Configuration saved in ./models\\fold_4_best\\config.json\n",
      "Model weights saved in ./models\\fold_4_best\\model.safetensors\n",
      "tokenizer config file saved in ./models\\fold_4_best\\tokenizer_config.json\n",
      "Special tokens file saved in ./models\\fold_4_best\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Best model for Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\fold_4_best'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Best model for Fold \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\fold_4_best'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up checkpoints directory: </span><span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'./models\\fold_4_checkpoints'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up checkpoints directory: \u001b[0m\u001b[2;32m'./models\\fold_4_checkpoints'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">4</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m4\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m5\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1097</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">920</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1097\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m920\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">275</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">229</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m275\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m229\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculated class weights for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9193254</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0961957</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Calculated class weights for Fold \u001b[1;36m5\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.9193254\u001b[0m \u001b[1;36m1.0961957\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--allenai--biomed_roberta_base\\snapshots\\0641aa1783909c6f94801601d4a166101f3d51a6\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Custom Trainer initialized with class weights </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">stored on CPU</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9193254</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0961957</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCustom Trainer initialized with class weights \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mstored on CPU\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m0.9193254\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m1.0961957\u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m5\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Safetensors PR exists\n",
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 378\n",
      "  Number of trainable parameters = 124,647,170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='378' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [378/378 04:32, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.313150</td>\n",
       "      <td>0.880157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.312972</td>\n",
       "      <td>0.898785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.095700</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.924779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.281080</td>\n",
       "      <td>0.930818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.285404</td>\n",
       "      <td>0.937238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_5_checkpoints\\checkpoint-64\n",
      "Configuration saved in ./models\\fold_5_checkpoints\\checkpoint-64\\config.json\n",
      "Model weights saved in ./models\\fold_5_checkpoints\\checkpoint-64\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_5_checkpoints\\checkpoint-128\n",
      "Configuration saved in ./models\\fold_5_checkpoints\\checkpoint-128\\config.json\n",
      "Model weights saved in ./models\\fold_5_checkpoints\\checkpoint-128\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_5_checkpoints\\checkpoint-192\n",
      "Configuration saved in ./models\\fold_5_checkpoints\\checkpoint-192\\config.json\n",
      "Model weights saved in ./models\\fold_5_checkpoints\\checkpoint-192\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_5_checkpoints\\checkpoint-64] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_5_checkpoints\\checkpoint-256\n",
      "Configuration saved in ./models\\fold_5_checkpoints\\checkpoint-256\\config.json\n",
      "Model weights saved in ./models\\fold_5_checkpoints\\checkpoint-256\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_5_checkpoints\\checkpoint-128] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_5_checkpoints\\checkpoint-320\n",
      "Configuration saved in ./models\\fold_5_checkpoints\\checkpoint-320\\config.json\n",
      "Model weights saved in ./models\\fold_5_checkpoints\\checkpoint-320\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_5_checkpoints\\checkpoint-192] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\fold_5_checkpoints\\checkpoint-378\n",
      "Configuration saved in ./models\\fold_5_checkpoints\\checkpoint-378\\config.json\n",
      "Model weights saved in ./models\\fold_5_checkpoints\\checkpoint-378\\model.safetensors\n",
      "Deleting older checkpoint [models\\fold_5_checkpoints\\checkpoint-256] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models\\fold_5_checkpoints\\checkpoint-320 (score: 0.944206008583691).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed after </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">273.</span><span style=\"color: #008000; text-decoration-color: #008000\">69s.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m5\u001b[0m\u001b[32m completed after \u001b[0m\u001b[1;32m273.\u001b[0m\u001b[32m69s.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 score on internal validation set during training: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9442</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m5\u001b[0m\u001b[36m - Best F1 score on internal validation set during training: \u001b[0m\u001b[1;36m0.9442\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and finding best threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> <span style=\"font-weight: bold\">(</span>using its internal validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and finding best threshold for Fold \u001b[1;36m5\u001b[0m \u001b[1m(\u001b[0musing its internal validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> Internal Validation Metrics <span style=\"font-weight: bold\">(</span>at threshold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'test_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.24553386867046356</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_f1'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.944206008583691</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_runtime'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.8022</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_samples_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">179.858</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_steps_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.42</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m5\u001b[0m Internal Validation Metrics \u001b[1m(\u001b[0mat threshold \u001b[1;36m0.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'test_loss'\u001b[0m: \u001b[1;36m0.24553386867046356\u001b[0m, \u001b[32m'test_f1'\u001b[0m: \n",
       "\u001b[1;36m0.944206008583691\u001b[0m, \u001b[32m'test_runtime'\u001b[0m: \u001b[1;36m2.8022\u001b[0m, \u001b[32m'test_samples_per_second'\u001b[0m: \u001b[1;36m179.858\u001b[0m, \u001b[32m'test_steps_per_second'\u001b[0m: \u001b[1;36m11.42\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Internal Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9465</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Optimized Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4238</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m5\u001b[0m\u001b[36m - Best F1 on Internal Validation: \u001b[0m\u001b[1;36m0.9465\u001b[0m\u001b[36m @ Optimized Threshold = \u001b[0m\u001b[1;36m0.4238\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\\fold_5_best\n",
      "Configuration saved in ./models\\fold_5_best\\config.json\n",
      "Model weights saved in ./models\\fold_5_best\\model.safetensors\n",
      "tokenizer config file saved in ./models\\fold_5_best\\tokenizer_config.json\n",
      "Special tokens file saved in ./models\\fold_5_best\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Best model for Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\fold_5_best'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Best model for Fold \u001b[0m\u001b[1;32m5\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\fold_5_best'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up checkpoints directory: </span><span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'./models\\fold_5_checkpoints'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up checkpoints directory: \u001b[0m\u001b[2;32m'./models\\fold_5_checkpoints'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">5</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m5\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────── </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Cross-Validation Summary (on Internal Validation Splits)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m──────────────────────────── \u001b[0m\u001b[1;32mCross-Validation Summary \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mon Internal Validation Splits\u001b[0m\u001b[1;32m)\u001b[0m\u001b[92m ─────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Fold Performance on Internal Validation Splits         </span>\n",
       "┏━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Fold </span>┃<span style=\"font-weight: bold\"> F1 (Thr=0.5) </span>┃<span style=\"font-weight: bold\"> Optimized F1 </span>┃<span style=\"font-weight: bold\"> Optimal Thr (Internal) </span>┃\n",
       "┡━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 1    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9544 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       0.9544 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                 0.6125 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 2    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9550 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       0.9654 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                 0.9707 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 3    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9595 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       0.9597 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                 0.2841 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 4    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9476 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       0.9478 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                 0.3820 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 5    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9442 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       0.9465 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                 0.4238 </span>│\n",
       "└──────┴──────────────┴──────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m        Fold Performance on Internal Validation Splits         \u001b[0m\n",
       "┏━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mFold\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mF1 (Thr=0.5)\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOptimized F1\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOptimal Thr (Internal)\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m1   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9544\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      0.9544\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                0.6125\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m2   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9550\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      0.9654\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                0.9707\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m3   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9595\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      0.9597\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                0.2841\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m4   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9476\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      0.9478\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                0.3820\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m5   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9442\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      0.9465\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                0.4238\u001b[0m\u001b[33m \u001b[0m│\n",
       "└──────┴──────────────┴──────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Avg F1 <span style=\"font-weight: bold\">(</span>Internal Val, <span style=\"color: #808000; text-decoration-color: #808000\">Thr</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">0.9522</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> +/- </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">0.0055</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Avg F1 \u001b[1m(\u001b[0mInternal Val, \u001b[33mThr\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m0.9522\u001b[0m\u001b[1;35m +\u001b[0m\u001b[1;35m/\u001b[0m\u001b[1;35m-\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m0.0055\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Avg F1 <span style=\"font-weight: bold\">(</span>Internal Val, Optimized Thr<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.9547</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> +/- </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.0071</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Avg F1 \u001b[1m(\u001b[0mInternal Val, Optimized Thr\u001b[1m)\u001b[0m: \u001b[1;32m0.9547\u001b[0m\u001b[1;32m +\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m-\u001b[0m\u001b[1;32m \u001b[0m\u001b[1;32m0.0071\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Avg Optimal Threshold <span style=\"font-weight: bold\">(</span>Internal<span style=\"font-weight: bold\">)</span>: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.5346</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> +/- </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.2427</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Avg Optimal Threshold \u001b[1m(\u001b[0mInternal\u001b[1m)\u001b[0m: \u001b[1;33m0.5346\u001b[0m\u001b[1;33m +\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m \u001b[0m\u001b[1;33m0.2427\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Final Evaluation on Holdout Set (valid.csv)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────── \u001b[0m\u001b[1;35mFinal Evaluation on Holdout Set \u001b[0m\u001b[1;35m(\u001b[0m\u001b[1;35mvalid.csv\u001b[0m\u001b[1;35m)\u001b[0m\u001b[92m ───────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Preparing holdout validation dataset <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">786</span> samples<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Preparing holdout validation dataset \u001b[1m(\u001b[0m\u001b[1;36m786\u001b[0m samples\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Using </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">786</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> valid ground truth labels from holdout set for final evaluation.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mUsing \u001b[0m\u001b[1;2;36m786\u001b[0m\u001b[2m valid ground truth labels from holdout set for final evaluation.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running ensemble inference on holdout set using <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> models<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running ensemble inference on holdout set using \u001b[1;36m5\u001b[0m models\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">fold_1_best</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m1\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33mfold_1_best\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models\\fold_1_best\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./models\\fold_1_best\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./models\\fold_1_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68aa8187bf6c488086544eca50ed19eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">fold_2_best</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m2\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33mfold_2_best\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models\\fold_2_best\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./models\\fold_2_best\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./models\\fold_2_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929ee60de50d40d9bbd1e48231a86afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m2\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">fold_3_best</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m3\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33mfold_3_best\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models\\fold_3_best\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./models\\fold_3_best\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./models\\fold_3_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6b0b1525da4946956709cbdb7e4138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">fold_4_best</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m4\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33mfold_4_best\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models\\fold_4_best\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./models\\fold_4_best\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./models\\fold_4_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff8322f273a48028ad156e2ffbed084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">fold_5_best</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m5\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33mfold_5_best\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models\\fold_5_best\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./models\\fold_5_best\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./models\\fold_5_best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bfe10379fc4c5faa0b3d4814113363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m5\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Combining holdout probabilities from </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> models (using nanmean)...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mCombining holdout probabilities from \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;36m models \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36musing nanmean\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Optimizing final threshold directly on holdout ensemble probabilities </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">targeting F1 for class </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mOptimizing final threshold directly on holdout ensemble probabilities \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mtargeting F1 for class \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">✓ Optimal threshold for holdout set (Class </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> F1): </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.6584</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> (yielding F1 score: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.9511</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m✓ Optimal threshold for holdout set \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mClass \u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m F1\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m0.6584\u001b[0m\u001b[1;32m \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32myielding F1 score: \u001b[0m\u001b[1;32m0.9511\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Final Ensemble Performance on Holdout Set (valid.csv) with Optimized Threshold</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────── \u001b[0m\u001b[1;35mFinal Ensemble Performance on Holdout Set \u001b[0m\u001b[1;35m(\u001b[0m\u001b[1;35mvalid.csv\u001b[0m\u001b[1;35m)\u001b[0m\u001b[1;35m with Optimized Threshold\u001b[0m\u001b[92m ──────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">786</span> holdout samples with valid labels.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating on \u001b[1;36m786\u001b[0m holdout samples with valid labels.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                          \n",
       " <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Class        </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Precision </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Recall </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> F1-Score </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Support </span> \n",
       " ──────────────────────────────────────────────────────── \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0            </span>     0.9615   0.9524     <span style=\"font-weight: bold\">0.9569</span>       420  \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1            </span>     0.9459   0.9563     <span style=\"font-weight: bold\">0.9511</span>       366  \n",
       "                                                          \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">macro Avg</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">    </span>     0.9537   0.9543     0.9540       786  \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">weighted Avg</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> </span>     0.9543   0.9542     0.9542       786  \n",
       "                                                          \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">Accuracy</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">     </span>                         <span style=\"font-weight: bold\">0.9542</span>       786  \n",
       "                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                          \n",
       " \u001b[1;35m \u001b[0m\u001b[1;35mClass       \u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35mPrecision\u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35mRecall\u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35mF1-Score\u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35mSupport\u001b[0m\u001b[1;35m \u001b[0m \n",
       " ──────────────────────────────────────────────────────── \n",
       " \u001b[2m \u001b[0m\u001b[2m0           \u001b[0m\u001b[2m \u001b[0m     0.9615   0.9524     \u001b[1m0.9569\u001b[0m       420  \n",
       " \u001b[2m \u001b[0m\u001b[2m1           \u001b[0m\u001b[2m \u001b[0m     0.9459   0.9563     \u001b[1m0.9511\u001b[0m       366  \n",
       "                                                          \n",
       " \u001b[2m \u001b[0m\u001b[1;2mmacro Avg\u001b[0m\u001b[2m   \u001b[0m\u001b[2m \u001b[0m     0.9537   0.9543     0.9540       786  \n",
       " \u001b[2m \u001b[0m\u001b[1;2mweighted Avg\u001b[0m\u001b[2m \u001b[0m     0.9543   0.9542     0.9542       786  \n",
       "                                                          \n",
       " \u001b[2m \u001b[0m\u001b[1;2mAccuracy\u001b[0m\u001b[2m    \u001b[0m\u001b[2m \u001b[0m                         \u001b[1m0.9542\u001b[0m       786  \n",
       "                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "🎯 <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Holdout Confusion Matrix</span> <span style=\"font-weight: bold\">(</span>using optimized threshold<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "🎯 \u001b[1;34mHoldout Confusion Matrix\u001b[0m \u001b[1m(\u001b[0musing optimized threshold\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">      True \\ Predicted      </span>\n",
       "                            \n",
       " <span style=\"font-weight: bold\">        </span> <span style=\"font-weight: bold\"> Pred 0 </span> <span style=\"font-weight: bold\"> Pred 1 </span> \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> True 0 </span>   400       20    \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> True 1 </span>    16      350    \n",
       "                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m      True \\ Predicted      \u001b[0m\n",
       "                            \n",
       " \u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mPred 0\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mPred 1\u001b[0m\u001b[1m \u001b[0m \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " \u001b[2m \u001b[0m\u001b[2mTrue 0\u001b[0m\u001b[2m \u001b[0m   400       20    \n",
       " \u001b[2m \u001b[0m\u001b[2mTrue 1\u001b[0m\u001b[2m \u001b[0m    16      350    \n",
       "                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────────────────── Target Check ─────────────────────────╮\n",
       "│ ⚠️ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Target Not Met.</span> F1 score for Class 1 (0.9511) is below 0.96. │\n",
       "╰────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭───────────────────────── Target Check ─────────────────────────╮\n",
       "│ ⚠️ \u001b[1;33mTarget Not Met.\u001b[0m F1 score for Class 1 (0.9511) is below 0.96. │\n",
       "╰────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> CV completed. Best models saved in subdirectories within <span style=\"color: #008000; text-decoration-color: #008000\">'./models'</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m CV completed. Best models saved in subdirectories within \u001b[32m'./models'\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> Final evaluation performed on <span style=\"color: #008000; text-decoration-color: #008000\">'valid.csv'</span> using an ensemble and a threshold optimized directly on holdout \n",
       "probabilities.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m Final evaluation performed on \u001b[32m'valid.csv'\u001b[0m using an ensemble and a threshold optimized directly on holdout \n",
       "probabilities.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">🏁 Script finished.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m🏁 Script finished.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep lightweight imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn # Needed for custom loss\n",
    "import os\n",
    "import shutil\n",
    "import sys # For exit on critical errors\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import gc\n",
    "# Defer heavy imports\n",
    "# from transformers import ...\n",
    "# from sklearn.metrics import ...\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration (MODIFIED)\n",
    "# -------------------------------\n",
    "SEED = 42\n",
    "MODEL_NAME = \"allenai/biomed_roberta_base\"\n",
    "N_SPLITS = 5\n",
    "MAX_LENGTH = 512 # << MODIFIED: Increased max sequence length\n",
    "EFFECTIVE_BATCH_SIZE = 32 # Desired effective batch size\n",
    "PER_DEVICE_BATCH_SIZE = 8 # << MODIFIED: Reduced per-device batch size\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 6 # << MODIFIED: Increased max epochs per fold\n",
    "EARLY_STOPPING_PATIENCE = 3 # << MODIFIED: User request\n",
    "WEIGHT_DECAY = 0.01\n",
    "# Output directory for CV models and results (MODIFIED)\n",
    "BASE_OUTPUT_DIR = \"./models\" # << MODIFIED: Output directly to 'models'\n",
    "DATA_DIR = \"./data\" # Directory containing train.csv and valid.csv\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    NUM_GPUS = torch.cuda.device_count()\n",
    "    console.print(f\"[cyan]CUDA available. Using {NUM_GPUS} GPU(s). Device: {torch.cuda.get_device_name(0)}[/]\")\n",
    "else:\n",
    "    NUM_GPUS = 1 # Assume 1 for calculation if CPU\n",
    "    console.print(\"[yellow]⚠️ CUDA not available. Training on CPU (will be very slow).[/]\")\n",
    "\n",
    "# Calculate Gradient Accumulation Steps (MODIFIED)\n",
    "# Ensure it's at least 1\n",
    "GRADIENT_ACCUMULATION_STEPS = max(1, EFFECTIVE_BATCH_SIZE // (PER_DEVICE_BATCH_SIZE * NUM_GPUS))\n",
    "console.print(f\"[cyan]Effective Batch Size: {EFFECTIVE_BATCH_SIZE}, Per-Device Batch Size: {PER_DEVICE_BATCH_SIZE}, Num GPUs: {NUM_GPUS} => Gradient Accumulation Steps: {GRADIENT_ACCUMULATION_STEPS}[/]\")\n",
    "\n",
    "\n",
    "# --- Defer heavy library imports ---\n",
    "console.print(\"[dim]Importing libraries...[/]\")\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSequenceClassification,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        EarlyStoppingCallback\n",
    "    )\n",
    "    from sklearn.metrics import f1_score, precision_recall_curve, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "    from rich.panel import Panel\n",
    "    from rich import box\n",
    "    from rich.progress import track\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from transformers import TrainerCallback # Needed for custom trainer loss\n",
    "except ImportError as e:\n",
    "    console.print(f\"[bold red]Error: Missing required library -> {e}[/]\")\n",
    "    console.print(\"[yellow]Please install all necessary libraries (pandas, torch, transformers[accelerate], scikit-learn, rich, tqdm).[/]\")\n",
    "    sys.exit(1)\n",
    "console.print(\"[green]✓ Libraries imported.[/]\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Data (Separately!)\n",
    "# -------------------------------\n",
    "train_csv_path = os.path.join(DATA_DIR, \"train.csv\")\n",
    "valid_csv_path = os.path.join(DATA_DIR, \"valid.csv\")\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    holdout_valid_df = pd.read_csv(valid_csv_path) # Load original valid set separately\n",
    "\n",
    "    # --- Data Cleaning Function ---\n",
    "    def clean_dataframe(df, name):\n",
    "        console.print(f\"Cleaning {name} Data...\")\n",
    "        initial_count = len(df)\n",
    "        # Standardize column names (handle potential variations)\n",
    "        df.columns = [col.lower().strip() for col in df.columns]\n",
    "        if 'labels' in df.columns and 'label' not in df.columns:\n",
    "            df = df.rename(columns={\"labels\": \"label\"})\n",
    "\n",
    "        # Check required columns\n",
    "        required_cols = ['text', 'label']\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            missing = [col for col in required_cols if col not in df.columns]\n",
    "            raise KeyError(f\"Missing required columns in {name}: {missing}\")\n",
    "\n",
    "        # Add 'id' column if not present (using index)\n",
    "        if 'id' not in df.columns:\n",
    "             console.print(f\"[dim]Adding 'id' column based on index to {name} data.[/]\")\n",
    "             df['id'] = df.index\n",
    "\n",
    "        # Drop rows with NaNs in text or label\n",
    "        nan_rows = df['text'].isnull() | df['label'].isnull()\n",
    "        if nan_rows.any():\n",
    "            console.print(f\"[yellow]⚠️ NaNs found in {name} data. Dropping {nan_rows.sum()} rows...[/]\")\n",
    "            df = df[~nan_rows].copy()\n",
    "\n",
    "        # Ensure label is numeric and then integer\n",
    "        df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
    "        label_nan_rows = df['label'].isnull()\n",
    "        if label_nan_rows.any():\n",
    "            console.print(f\"[yellow]⚠️ Non-numeric labels found after coercion in {name}. Dropping {label_nan_rows.sum()} rows...[/]\")\n",
    "            df = df[~label_nan_rows].copy()\n",
    "\n",
    "        df['label'] = df['label'].astype(int)\n",
    "\n",
    "        # Ensure text is string\n",
    "        df['text'] = df['text'].astype(str)\n",
    "\n",
    "        cleaned_count = len(df)\n",
    "        if cleaned_count < initial_count:\n",
    "            console.print(f\"[dim]Dropped {initial_count - cleaned_count} rows from {name}.[/]\")\n",
    "\n",
    "        # Check for valid labels (0 and 1)\n",
    "        valid_labels = {0, 1}\n",
    "        if not set(df['label'].unique()).issubset(valid_labels):\n",
    "            invalid_labels = set(df['label'].unique()) - valid_labels\n",
    "            console.print(f\"[yellow]⚠️ Invalid labels found in {name}: {invalid_labels}. Keeping only 0 and 1.[/]\")\n",
    "            df = df[df['label'].isin(valid_labels)].copy()\n",
    "            if len(df) < cleaned_count:\n",
    "                 console.print(f\"[dim]Dropped {cleaned_count - len(df)} rows with invalid labels.[/]\")\n",
    "\n",
    "\n",
    "        console.print(f\"[green]✓ {name} data loaded and cleaned. Total: {len(df)} examples.[/]\")\n",
    "        # Check label distribution\n",
    "        label_counts = df['label'].value_counts()\n",
    "        console.print(f\"{name} data label distribution:\\n{label_counts}\")\n",
    "        if len(label_counts) < 2 and name == \"Training\":\n",
    "             console.print(\"[bold red]Error: Training data must contain both labels 0 and 1 for stratified splitting.[/]\")\n",
    "             sys.exit(1)\n",
    "        return df.reset_index(drop=True) # Reset index after cleaning\n",
    "\n",
    "    train_df = clean_dataframe(train_df, \"Training\")\n",
    "    holdout_valid_df = clean_dataframe(holdout_valid_df, \"Holdout Validation\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    console.print(f\"[bold red]Error: CSV file not found - {e}. Check paths '{train_csv_path}' and '{valid_csv_path}'.[/]\")\n",
    "    sys.exit(1)\n",
    "except KeyError as e:\n",
    "    console.print(f\"[bold red]Error: Missing expected column in CSV - {e}. Ensure 'text' and 'label' (or 'labels') exist.[/]\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]Unexpected error loading/cleaning data: {e}[/]\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define Dataset Class (No changes needed here, uses MAX_LENGTH from config)\n",
    "# -------------------------------\n",
    "class VaccineDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, ids=None, tokenizer=None, max_length=512, is_inference=False):\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided.\")\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.ids = ids # Store IDs if provided\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_inference = (labels is None)\n",
    "\n",
    "        if not self.is_inference and (self.labels is None or len(texts) != len(labels)):\n",
    "            raise ValueError(\"Texts and Labels must be provided and have the same length for training/evaluation.\")\n",
    "        if self.ids is not None and len(texts) != len(self.ids):\n",
    "             raise ValueError(\"Texts and IDs must have the same length if IDs are provided.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx]) if idx < len(self.texts) and self.texts[idx] is not None else \"\"\n",
    "        item_id = self.ids[idx] if self.ids is not None and idx < len(self.ids) else idx # Use index as fallback ID\n",
    "\n",
    "        try:\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "            if not self.is_inference:\n",
    "                if idx < len(self.labels):\n",
    "                    label = self.labels[idx]\n",
    "                    item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "                else:\n",
    "                     item['labels'] = torch.tensor(-1, dtype=torch.long) # Should not happen\n",
    "\n",
    "            if self.ids is not None:\n",
    "                item['id'] = item_id # Keep ID as is (numeric or string)\n",
    "\n",
    "            return item\n",
    "\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error in __getitem__ at index {idx} (ID: {item_id}): {e}[/]\")\n",
    "            dummy_item = {\n",
    "                'input_ids': torch.zeros(self.max_length, dtype=torch.long),\n",
    "                'attention_mask': torch.zeros(self.max_length, dtype=torch.long),\n",
    "            }\n",
    "            if not self.is_inference:\n",
    "                dummy_item['labels'] = torch.tensor(-1, dtype=torch.long)\n",
    "            if self.ids is not None:\n",
    "                dummy_item['id'] = item_id # Return the original ID even for dummy item\n",
    "            return dummy_item\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load Tokenizer (once)\n",
    "# -------------------------------\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    console.print(f\"[green]✓ Tokenizer loaded from '{MODEL_NAME}'.[/]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]Error loading tokenizer '{MODEL_NAME}': {e}[/]\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Metric Function (for Trainer, based on argmax)\n",
    "# -------------------------------\n",
    "def compute_metrics_for_trainer(eval_pred):\n",
    "    \"\"\"Calculates F1 based on argmax for checkpoint selection during training.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    valid_indices = labels != -1 # Filter out potential errors from __getitem__\n",
    "    labels = labels[valid_indices]\n",
    "    logits = logits[valid_indices]\n",
    "\n",
    "    if len(labels) == 0: return {\"f1\": 0.0} # No valid labels to compute\n",
    "\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    # Calculate F1 for class 1 specifically, as requested\n",
    "    f1 = f1_score(labels, preds, average='binary', pos_label=1, zero_division=0)\n",
    "    return {\"f1\": f1} # Trainer uses this metric key\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. Threshold Optimization Function (MODIFIED: Target Class 1 F1)\n",
    "# -----------------------------------\n",
    "def find_optimal_threshold(labels, probs, target_label=1):\n",
    "    \"\"\"Finds the threshold that maximizes the F1 score for the target_label.\"\"\"\n",
    "    valid_indices = labels != -1\n",
    "    labels = labels[valid_indices]\n",
    "    probs = probs[valid_indices]\n",
    "\n",
    "    if len(labels) == 0 or len(np.unique(labels)) < 2:\n",
    "         console.print(\"[yellow]⚠️ Not enough valid data or classes for threshold optimization. Returning default 0.5.[/]\")\n",
    "         return 0.5, 0.0\n",
    "\n",
    "    # Ensure probs are for the positive class (target_label)\n",
    "    # Assuming probs are already P(class=1) as calculated later\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, probs, pos_label=target_label)\n",
    "\n",
    "    # Calculate F1 score, avoiding division by zero\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "    f1_scores = f1_scores[:-1] # Drop last value corresponding to no prediction\n",
    "    thresholds = thresholds[:len(f1_scores)] # Align thresholds with scores\n",
    "\n",
    "    f1_scores = np.nan_to_num(f1_scores) # Handle potential NaNs if precision/recall are zero\n",
    "\n",
    "    if len(f1_scores) == 0:\n",
    "         console.print(\"[yellow]⚠️ No valid F1 scores computed during threshold search. Returning default 0.5.[/]\")\n",
    "         return 0.5, 0.0\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    best_f1 = f1_scores[best_f1_idx]\n",
    "    best_thresh = thresholds[best_f1_idx]\n",
    "\n",
    "    # Sanity check against 0.5 threshold F1 for the target class\n",
    "    preds_at_05 = (probs >= 0.5).astype(int)\n",
    "    f1_at_05 = f1_score(labels, preds_at_05, pos_label=target_label, zero_division=0)\n",
    "\n",
    "    # Optionally uncomment to see comparison\n",
    "    # console.print(f\"[dim]Threshold search: Best F1={best_f1:.4f} @ Thr={best_thresh:.4f} vs F1={f1_at_05:.4f} @ Thr=0.5[/]\")\n",
    "\n",
    "    # No need to force 0.5, let the optimization decide\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Custom Trainer for Weighted Loss (CORRECTED)\n",
    "# -------------------------------\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Store weights on CPU initially, move to device in compute_loss\n",
    "        self.class_weights_cpu = class_weights.cpu() if class_weights is not None else None\n",
    "        if self.class_weights_cpu is not None:\n",
    "            console.print(f\"[cyan]Custom Trainer initialized with class weights (stored on CPU): {self.class_weights_cpu.numpy()}[/]\")\n",
    "        else:\n",
    "            console.print(\"[yellow]⚠️ Custom Trainer initialized WITHOUT class weights (will use standard CE loss).[/]\")\n",
    "\n",
    "    # Modify signature to accept **kwargs\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the loss using class weights if provided, otherwise falls back\n",
    "        to the default Trainer loss computation.\n",
    "        Accepts **kwargs to handle potential extra arguments passed by the Trainer internals.\n",
    "        \"\"\"\n",
    "        if self.class_weights_cpu is None:\n",
    "            # No class weights provided, fall back to the default Hugging Face loss.\n",
    "            # Pass along any extra kwargs received.\n",
    "            # console.print(\"[dim]Using default compute_loss (no weights).[/dim]\") # Optional debug print\n",
    "            return super().compute_loss(model, inputs, return_outputs=return_outputs, **kwargs)\n",
    "        else:\n",
    "            # Class weights are provided, compute custom weighted loss.\n",
    "            if \"labels\" not in inputs:\n",
    "                raise ValueError(\"Inputs must contain 'labels' for custom loss calculation.\")\n",
    "\n",
    "            labels = inputs.pop(\"labels\") # Remove labels from inputs to avoid passing them to the model directly if not needed\n",
    "            outputs = model(**inputs)     # Pass remaining inputs to the model\n",
    "            logits = outputs.get(\"logits\")\n",
    "\n",
    "            if logits is None:\n",
    "                 # Handle cases where the model output format might be different\n",
    "                 # If your model returns loss directly, you might need to adjust\n",
    "                 console.print(\"[yellow]⚠️ Model outputs did not contain 'logits'. Falling back to default loss calculation if possible.[/]\")\n",
    "                 # Re-add labels for the potential fallback\n",
    "                 inputs[\"labels\"] = labels\n",
    "                 return super().compute_loss(model, inputs, return_outputs=return_outputs, **kwargs)\n",
    "\n",
    "            # --- Custom Loss Calculation ---\n",
    "            # Move weights to the same device as logits just-in-time\n",
    "            class_weights_on_device = self.class_weights_cpu.to(logits.device)\n",
    "            # console.print(f\"[dim]Using weighted loss on device {logits.device} with weights {class_weights_on_device.cpu().numpy()}.[/dim]\") # Optional debug print\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=class_weights_on_device)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "            # --- End Custom Loss Calculation ---\n",
    "\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Cross-Validation Loop (on train_df ONLY) (MODIFIED)\n",
    "# -------------------------------\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "fold_results = []\n",
    "best_model_paths = [] # Store paths to best model checkpoint of each fold\n",
    "\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "console.rule(\"[bold yellow]Starting Cross-Validation on Training Data[/]\")\n",
    "\n",
    "# Use train_df for splitting\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df['text'], train_df['label'])):\n",
    "    current_fold = fold + 1\n",
    "    console.rule(f\"[bold blue]CV Fold {current_fold}/{N_SPLITS}[/]\")\n",
    "\n",
    "    # --- Get fold data from train_df ---\n",
    "    train_fold_df = train_df.iloc[train_idx].copy().reset_index(drop=True)\n",
    "    fold_valid_df = train_df.iloc[val_idx].copy().reset_index(drop=True) # Validation set FOR THIS FOLD\n",
    "\n",
    "    console.print(f\"Fold Train Size: {len(train_fold_df)}, Fold Validation Size: {len(fold_valid_df)}\")\n",
    "    train_fold_labels_dist = train_fold_df['label'].value_counts(dropna=False).sort_index()\n",
    "    valid_fold_labels_dist = fold_valid_df['label'].value_counts(dropna=False).sort_index()\n",
    "    console.print(f\"Fold Train Labels:\\n{train_fold_labels_dist}\")\n",
    "    console.print(f\"Fold Validation Labels:\\n{valid_fold_labels_dist}\")\n",
    "\n",
    "    if len(train_fold_labels_dist) < 2:\n",
    "        console.print(f\"[bold red]Error: Fold {current_fold} training data only has one class after splitting. Skipping fold.[/]\")\n",
    "        continue\n",
    "\n",
    "    # --- Calculate Class Weights for this fold's training data (NEW) ---\n",
    "    try:\n",
    "        n_samples = len(train_fold_df)\n",
    "        n_classes = 2\n",
    "        class_counts = train_fold_labels_dist.to_dict()\n",
    "        # Ensure both 0 and 1 counts exist, default to 1 if missing (to avoid div by zero, though split should prevent this)\n",
    "        count0 = class_counts.get(0, 1)\n",
    "        count1 = class_counts.get(1, 1)\n",
    "\n",
    "        # Inverse frequency weighting: weight = total_samples / (n_classes * count_for_class)\n",
    "        weight0 = n_samples / (n_classes * count0)\n",
    "        weight1 = n_samples / (n_classes * count1)\n",
    "        # Normalize weights (optional, but can help stability)\n",
    "        # total_weight = weight0 + weight1\n",
    "        # weight0 /= total_weight\n",
    "        # weight1 /= total_weight\n",
    "\n",
    "        class_weights_tensor = torch.tensor([weight0, weight1], dtype=torch.float)\n",
    "        console.print(f\"Calculated class weights for Fold {current_fold}: {class_weights_tensor.numpy()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error calculating class weights for fold {current_fold}: {e}. Proceeding without weights.[/]\")\n",
    "        class_weights_tensor = None # Fallback\n",
    "\n",
    "    # --- Create fold datasets ---\n",
    "    try:\n",
    "        train_dataset = VaccineDataset(\n",
    "            texts=train_fold_df['text'].tolist(),\n",
    "            labels=train_fold_df['label'].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        # This validation dataset is used for evaluating epochs within this fold\n",
    "        fold_eval_dataset = VaccineDataset(\n",
    "            texts=fold_valid_df['text'].tolist(),\n",
    "            labels=fold_valid_df['label'].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        if len(train_dataset) == 0 or len(fold_eval_dataset) == 0:\n",
    "             console.print(f\"[bold red]Error: Empty dataset for fold {current_fold}. Skipping fold.[/]\")\n",
    "             continue\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error creating datasets for fold {current_fold}: {e}[/]\")\n",
    "        continue\n",
    "\n",
    "    # --- Load fresh model ---\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "        # No need to move to device here, Trainer handles it\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error loading model for fold {current_fold}: {e}[/]\")\n",
    "        continue\n",
    "\n",
    "    # --- Training Arguments (MODIFIED) ---\n",
    "    # Use the main BASE_OUTPUT_DIR, Trainer will create subdirs like 'checkpoint-...'\n",
    "    # Let's create a specific subdir for this fold's checkpoints & final model\n",
    "    fold_output_dir = os.path.join(BASE_OUTPUT_DIR, f\"fold_{current_fold}_checkpoints\")\n",
    "    final_fold_model_dir = os.path.join(BASE_OUTPUT_DIR, f\"fold_{current_fold}_best\") # Separate dir for the best model\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=fold_output_dir,             # Directory for checkpoints during training\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=EPOCHS,                # MODIFIED\n",
    "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE, # MODIFIED\n",
    "        per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE * 2, # Keep eval batch size reasonable\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, # MODIFIED\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        load_best_model_at_end=True,            # Keep this\n",
    "        metric_for_best_model=\"f1\",             # Use F1 calculated by compute_metrics_for_trainer (on class 1)\n",
    "        greater_is_better=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        # Adjust logging steps based on effective batch size and dataset size\n",
    "        logging_steps=max(10, len(train_dataset) // (EFFECTIVE_BATCH_SIZE * 4)),\n",
    "        log_level=\"info\",\n",
    "        save_total_limit=2, # Keep best and maybe last checkpoint during training\n",
    "        seed=SEED + fold,   # Vary seed slightly per fold\n",
    "        fp16=torch.cuda.is_available(), # Enable mixed precision if CUDA available\n",
    "        report_to=[], # Disable wandb/tensorboard reporting unless configured\n",
    "        dataloader_num_workers=0, # Safer default\n",
    "        dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        optim=\"adamw_torch\", # Recommended optimizer\n",
    "    )\n",
    "\n",
    "    # --- Trainer Setup (MODIFIED: Use custom trainer) ---\n",
    "    trainer = WeightedLossTrainer( # << MODIFIED\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=fold_eval_dataset, # Use the fold's validation set for epoch evaluation\n",
    "        compute_metrics=compute_metrics_for_trainer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE, # MODIFIED\n",
    "                                         early_stopping_threshold=0.001)], # Stop if F1 improvement is negligible\n",
    "        class_weights=class_weights_tensor # << Pass calculated weights\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    console.print(f\"🚀 Training Fold {current_fold}...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        console.print(f\"[green]✓ Training Fold {current_fold} completed after {train_result.metrics.get('train_runtime', 0):.2f}s.[/]\")\n",
    "        # Log best F1 achieved during training on the fold's validation set\n",
    "        best_metric_val = trainer.state.best_metric\n",
    "        if best_metric_val:\n",
    "             console.print(f\"[cyan]Fold {current_fold} - Best F1 score on internal validation set during training: {best_metric_val:.4f}[/]\")\n",
    "        else:\n",
    "             console.print(\"[yellow]Could not retrieve best metric from trainer state.[/]\")\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error during training for fold {current_fold}: {e}[/]\")\n",
    "        del model, trainer, train_dataset, fold_eval_dataset; gc.collect(); torch.cuda.empty_cache()\n",
    "        continue # Skip to next fold\n",
    "\n",
    "    # --- Evaluate on Fold's Validation Set & Find Optimal Threshold for THIS fold ---\n",
    "    # This threshold is PRIMARILY for reporting/interest, the FINAL threshold will be optimized later on the holdout set\n",
    "    console.print(f\"🔍 Evaluating and finding best threshold for Fold {current_fold} (using its internal validation split)...\")\n",
    "    try:\n",
    "        predictions_output = trainer.predict(fold_eval_dataset)\n",
    "        logits = predictions_output.predictions\n",
    "        labels = predictions_output.label_ids # True labels for this fold's validation split\n",
    "        internal_val_metrics = predictions_output.metrics # Metrics based on argmax (0.5 threshold implicitly)\n",
    "\n",
    "        console.print(f\"Fold {current_fold} Internal Validation Metrics (at threshold 0.5): {internal_val_metrics}\")\n",
    "\n",
    "        if logits is not None and labels is not None:\n",
    "            # Calculate probabilities for the positive class (1)\n",
    "            if logits.shape[1] >= 2:\n",
    "                probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
    "            elif logits.shape[1] == 1: # Handle potential single-logit output (less common for classification)\n",
    "                 probs = torch.sigmoid(torch.tensor(logits)).squeeze(-1).numpy()\n",
    "            else: # Should not happen with num_labels=2\n",
    "                 probs = np.array([])\n",
    "                 console.print(f\"[yellow]⚠️ Unexpected logits shape in fold {current_fold}: {logits.shape}. Cannot calculate probs.[/]\")\n",
    "\n",
    "            if probs.size > 0:\n",
    "                # Find optimal threshold based on this fold's validation split, targeting class 1\n",
    "                optimal_thresh_fold, best_f1_fold_val = find_optimal_threshold(labels, probs, target_label=1)\n",
    "\n",
    "                console.print(f\"[cyan]Fold {current_fold} - Best F1 on Internal Validation: {best_f1_fold_val:.4f} @ Optimized Threshold = {optimal_thresh_fold:.4f}[/]\")\n",
    "                fold_results.append({\n",
    "                    \"fold\": current_fold,\n",
    "                    \"best_f1_internal_val_optimized\": best_f1_fold_val, # F1 on this fold's val set with optimized threshold\n",
    "                    \"optimal_threshold_internal\": optimal_thresh_fold,\n",
    "                    \"f1_internal_val_0.5\": internal_val_metrics.get('test_f1', 0.0) # F1 at 0.5 threshold\n",
    "                })\n",
    "\n",
    "                # --- Save Best Model for Ensemble ---\n",
    "                # Trainer already loaded the best model due to load_best_model_at_end=True\n",
    "                # We just need to save it to a persistent location outside the checkpoints dir\n",
    "                try:\n",
    "                    os.makedirs(final_fold_model_dir, exist_ok=True)\n",
    "                    trainer.save_model(final_fold_model_dir)\n",
    "                    # Also save the tokenizer with the model\n",
    "                    tokenizer.save_pretrained(final_fold_model_dir)\n",
    "                    best_model_paths.append(final_fold_model_dir) # Store path for later ensemble\n",
    "                    console.print(f\"[green]✓ Best model for Fold {current_fold} saved to '{final_fold_model_dir}'[/]\")\n",
    "\n",
    "                    # Optionally, save the fold-specific threshold (though we won't use it for final eval)\n",
    "                    threshold_file_path = os.path.join(final_fold_model_dir, \"threshold_internal.txt\")\n",
    "                    with open(threshold_file_path, \"w\") as f: f.write(f\"{optimal_thresh_fold:.4f}\")\n",
    "\n",
    "                    # --- Clean Checkpoints ---\n",
    "                    console.print(f\"[dim]Cleaning up checkpoints directory: '{fold_output_dir}'[/]\")\n",
    "                    try:\n",
    "                        shutil.rmtree(fold_output_dir)\n",
    "                    except OSError as e:\n",
    "                        console.print(f\"[yellow]⚠️ Error deleting checkpoint directory {fold_output_dir}: {e}[/]\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    console.print(f\"[bold red]Error saving best model or cleaning checkpoints for fold {current_fold}: {e}[/]\")\n",
    "                    # Attempt to find best checkpoint path if saving failed\n",
    "                    best_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "                    if best_checkpoint_path and os.path.isdir(best_checkpoint_path):\n",
    "                         console.print(f\"[yellow]Best checkpoint was at: {best_checkpoint_path}. Consider manually copying.[/]\")\n",
    "                    else:\n",
    "                         console.print(f\"[yellow]⚠️ No best model path found for fold {current_fold}. Cannot use for ensemble.[/]\")\n",
    "\n",
    "            else:\n",
    "                 console.print(f\"[yellow]⚠️ No probabilities calculated for fold {current_fold}. Cannot optimize threshold.[/]\")\n",
    "                 # Try to save the model anyway if training completed\n",
    "                 try:\n",
    "                     os.makedirs(final_fold_model_dir, exist_ok=True)\n",
    "                     trainer.save_model(final_fold_model_dir)\n",
    "                     tokenizer.save_pretrained(final_fold_model_dir)\n",
    "                     best_model_paths.append(final_fold_model_dir)\n",
    "                     console.print(f\"[yellow]✓ Model saved to '{final_fold_model_dir}' despite probability calculation issue.[/]\")\n",
    "                     shutil.rmtree(fold_output_dir)\n",
    "                 except Exception as e_save:\n",
    "                     console.print(f\"[bold red]Error saving model for fold {current_fold} after probability issue: {e_save}[/]\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            console.print(f\"[yellow]⚠️ Prediction output missing logits or labels for fold {current_fold}. Cannot evaluate or save best model.[/]\")\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error during evaluation/optimization/saving for fold {current_fold}: {e}[/]\")\n",
    "        # Ensure memory is freed even if evaluation fails\n",
    "        del model, trainer, train_dataset, fold_eval_dataset; gc.collect(); torch.cuda.empty_cache()\n",
    "        continue # Skip to next fold\n",
    "\n",
    "    # --- Free memory after fold completion ---\n",
    "    console.print(f\"[dim]Cleaning up memory for fold {current_fold}...[/dim]\")\n",
    "    del model, trainer, train_dataset, fold_eval_dataset, predictions_output, logits, labels, probs\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# --- End of CV Loop ---\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Display CV Summary\n",
    "# -------------------------------\n",
    "console.rule(\"[bold green]Cross-Validation Summary (on Internal Validation Splits)[/]\")\n",
    "if fold_results:\n",
    "    results_table = Table(title=\"Fold Performance on Internal Validation Splits\")\n",
    "    results_table.add_column(\"Fold\", style=\"cyan\")\n",
    "    results_table.add_column(\"F1 (Thr=0.5)\", style=\"magenta\", justify=\"right\")\n",
    "    results_table.add_column(\"Optimized F1\", style=\"green\", justify=\"right\")\n",
    "    results_table.add_column(\"Optimal Thr (Internal)\", style=\"yellow\", justify=\"right\")\n",
    "\n",
    "    all_f1s_opt = [res[\"best_f1_internal_val_optimized\"] for res in fold_results]\n",
    "    all_f1s_05 = [res[\"f1_internal_val_0.5\"] for res in fold_results]\n",
    "    all_thresholds = [res[\"optimal_threshold_internal\"] for res in fold_results]\n",
    "\n",
    "    for res in fold_results:\n",
    "        results_table.add_row(\n",
    "            str(res[\"fold\"]),\n",
    "            f\"{res['f1_internal_val_0.5']:.4f}\",\n",
    "            f\"{res['best_f1_internal_val_optimized']:.4f}\",\n",
    "            f\"{res['optimal_threshold_internal']:.4f}\"\n",
    "        )\n",
    "    console.print(results_table)\n",
    "\n",
    "    avg_f1_opt = np.mean(all_f1s_opt)\n",
    "    std_f1_opt = np.std(all_f1s_opt)\n",
    "    avg_f1_05 = np.mean(all_f1s_05)\n",
    "    std_f1_05 = np.std(all_f1s_05)\n",
    "    avg_thresh = np.mean(all_thresholds)\n",
    "    std_thresh = np.std(all_thresholds)\n",
    "\n",
    "    console.print(f\"\\nAvg F1 (Internal Val, Thr=0.5): [bold magenta]{avg_f1_05:.4f} +/- {std_f1_05:.4f}[/]\")\n",
    "    console.print(f\"Avg F1 (Internal Val, Optimized Thr): [bold green]{avg_f1_opt:.4f} +/- {std_f1_opt:.4f}[/]\")\n",
    "    console.print(f\"Avg Optimal Threshold (Internal): [bold yellow]{avg_thresh:.4f} +/- {std_thresh:.4f}[/]\")\n",
    "\n",
    "    if len(best_model_paths) != N_SPLITS:\n",
    "         console.print(f\"[yellow]⚠️ Found {len(best_model_paths)} best models, expected {N_SPLITS}. Ensemble evaluation might be affected.[/]\")\n",
    "\n",
    "else:\n",
    "    console.print(\"[yellow]No fold results recorded. Check for errors during training/evaluation.[/]\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 9. FINAL EVALUATION ON HOLDOUT VALIDATION SET (valid.csv)\n",
    "# Using Ensemble of Best Fold Models\n",
    "# Optimize Threshold DIRECTLY on Holdout Set (NEW)\n",
    "# -----------------------------------------------------\n",
    "console.rule(\"[bold magenta]Final Evaluation on Holdout Set (valid.csv)[/]\")\n",
    "\n",
    "if not best_model_paths:\n",
    "    console.print(\"[bold red]❌ No best models saved from CV folds. Cannot perform final evaluation.[/]\")\n",
    "    sys.exit(1)\n",
    "if len(holdout_valid_df) == 0:\n",
    "    console.print(\"[bold red]❌ Holdout validation data ('valid.csv') is empty. Cannot perform final evaluation.[/]\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Create Dataset & Loader for Holdout Set ---\n",
    "console.print(f\"Preparing holdout validation dataset ({len(holdout_valid_df)} samples)...\")\n",
    "try:\n",
    "    # Include IDs and labels for evaluation\n",
    "    holdout_dataset = VaccineDataset(\n",
    "        texts=holdout_valid_df['text'].tolist(),\n",
    "        labels=holdout_valid_df['label'].tolist(), # Include true labels\n",
    "        ids=holdout_valid_df['id'].tolist(),       # Include IDs\n",
    "        tokenizer=tokenizer, # Use the globally loaded tokenizer\n",
    "        max_length=MAX_LENGTH,\n",
    "        is_inference=False # We have labels for evaluation\n",
    "    )\n",
    "    holdout_loader = DataLoader(holdout_dataset, batch_size=PER_DEVICE_BATCH_SIZE * 2, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "    # Get true labels in the correct order, filtering any potential -1 from dataset errors\n",
    "    holdout_y_true_raw = np.array(holdout_valid_df['label'].tolist())\n",
    "    valid_true_indices = holdout_y_true_raw != -1\n",
    "    holdout_y_true = holdout_y_true_raw[valid_true_indices]\n",
    "    console.print(f\"[dim]Using {len(holdout_y_true)} valid ground truth labels from holdout set for final evaluation.[/]\")\n",
    "\n",
    "except Exception as e:\n",
    "     console.print(f\"[bold red]❌ Error creating holdout dataset/loader: {e}. Cannot perform final evaluation.[/]\")\n",
    "     sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Ensemble Inference on Holdout Set ---\n",
    "console.print(f\"Running ensemble inference on holdout set using {len(best_model_paths)} models...\")\n",
    "all_holdout_probs_np = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Ensure device is set\n",
    "\n",
    "for i, model_p in enumerate(best_model_paths):\n",
    "    console.print(f\"--- Loading model {i+1}/{len(best_model_paths)} from [yellow]{os.path.basename(model_p)}[/] ---\")\n",
    "    try:\n",
    "        # Load model AND tokenizer specific to that fold (best practice)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_p).to(device).eval()\n",
    "        # Re-create dataset/loader with fold-specific tokenizer? Usually not needed if base tokenizer is same.\n",
    "        # We'll stick to the global tokenizer for simplicity, assuming compatibility.\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]❌ Error loading model {model_p}: {e}. Skipping this model for ensemble.[/]\")\n",
    "        continue\n",
    "\n",
    "    fold_holdout_probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in track(holdout_loader, description=f\"Predicting (Holdout, Model {i+1})...\", console=console, transient=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            batch_labels = batch.get(\"labels\", torch.tensor([-1]*len(input_ids))) # Get labels if present\n",
    "\n",
    "            # Identify valid items in the batch (label != -1, which our Dataset uses for errors)\n",
    "            valid_batch_indices_mask = (batch_labels != -1).cpu()\n",
    "            valid_input_ids = input_ids[valid_batch_indices_mask]\n",
    "            valid_attention_mask = attention_mask[valid_batch_indices_mask]\n",
    "\n",
    "            # Skip batch if no valid items (e.g., all errored in __getitem__)\n",
    "            if valid_input_ids.shape[0] == 0:\n",
    "                 # Append placeholder NaNs for invalid items\n",
    "                 batch_probs = np.full(len(input_ids), np.nan)\n",
    "                 fold_holdout_probs_list.extend(batch_probs)\n",
    "                 continue\n",
    "\n",
    "            try:\n",
    "                outputs = model(input_ids=valid_input_ids, attention_mask=valid_attention_mask)\n",
    "                logits = outputs.logits\n",
    "                if logits.shape[1] >= 2:\n",
    "                    probs = torch.softmax(logits, dim=1)[:, 1] # Prob for class 1\n",
    "                elif logits.shape[1] == 1:\n",
    "                    probs = torch.sigmoid(logits).squeeze(-1)\n",
    "                else:\n",
    "                    probs = torch.full((valid_input_ids.shape[0],), 0.5, device=device) # Fallback guess\n",
    "\n",
    "                # Place probabilities back into the original batch structure using NaN for invalid items\n",
    "                batch_probs = np.full(len(input_ids), np.nan) # Initialize with NaNs\n",
    "                batch_probs[valid_batch_indices_mask.numpy()] = probs.cpu().numpy()\n",
    "                fold_holdout_probs_list.extend(batch_probs)\n",
    "\n",
    "\n",
    "            except Exception as pred_e:\n",
    "                console.print(f\"\\n[bold red]❌ Error during holdout prediction batch with model {i+1}: {pred_e}[/]\")\n",
    "                 # Add NaNs for the failed batch to maintain length and indicate failure\n",
    "                fold_holdout_probs_list.extend(np.full(len(input_ids), np.nan))\n",
    "\n",
    "\n",
    "    # Convert list of batch arrays to a single numpy array for the fold\n",
    "    fold_holdout_probs = np.array(fold_holdout_probs_list)\n",
    "\n",
    "    # Ensure length matches dataset size, pad with NaNs if needed (shouldn't happen with current logic)\n",
    "    if len(fold_holdout_probs) != len(holdout_dataset):\n",
    "         console.print(f\"[yellow]⚠️ Length mismatch for fold {i+1} predictions ({len(fold_holdout_probs)}) vs dataset ({len(holdout_dataset)}). Padding with NaN.[/]\")\n",
    "         fold_holdout_probs = np.pad(fold_holdout_probs, (0, len(holdout_dataset) - len(fold_holdout_probs)), constant_values=np.nan)\n",
    "\n",
    "    all_holdout_probs_np.append(fold_holdout_probs[:len(holdout_dataset)]) # Ensure correct length\n",
    "    console.print(f\"[green]✓ Holdout predictions collected for model {i+1}.[/]\")\n",
    "\n",
    "    # Free memory\n",
    "    del model, outputs, logits, probs; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- Aggregate and Evaluate Holdout Predictions ---\n",
    "if not all_holdout_probs_np:\n",
    "    console.print(\"[bold red]❌ No predictions generated for the holdout set by any valid model.[/]\")\n",
    "else:\n",
    "    num_ensemble_models = len(all_holdout_probs_np)\n",
    "    console.print(f\"\\n[bold cyan]Combining holdout probabilities from {num_ensemble_models} models (using nanmean)...[/]\")\n",
    "    # Use nanmean to average probabilities, ignoring NaNs from failed predictions/invalid data\n",
    "    holdout_avg_probs_raw = np.nanmean(np.array(all_holdout_probs_np), axis=0)\n",
    "\n",
    "    # Filter probabilities corresponding to valid true labels\n",
    "    holdout_avg_probs = holdout_avg_probs_raw[valid_true_indices]\n",
    "\n",
    "    # Check if we have valid probabilities and labels to work with\n",
    "    if len(holdout_avg_probs) == 0 or np.all(np.isnan(holdout_avg_probs)):\n",
    "        console.print(\"[bold red]❌ No valid averaged probabilities obtained for the holdout set. Cannot optimize threshold or evaluate.[/]\")\n",
    "    elif len(holdout_y_true) == 0:\n",
    "         console.print(\"[bold red]❌ No valid ground truth labels found in the holdout set. Cannot optimize threshold or evaluate.[/]\")\n",
    "    else:\n",
    "        # --- Optimize Threshold Directly on Holdout Ensemble Probs (NEW) ---\n",
    "        console.print(\"[cyan]Optimizing final threshold directly on holdout ensemble probabilities (targeting F1 for class 1)...[/]\")\n",
    "        final_holdout_threshold, best_f1_on_holdout = find_optimal_threshold(holdout_y_true, holdout_avg_probs, target_label=1)\n",
    "        console.print(f\"[bold green]✓ Optimal threshold for holdout set (Class 1 F1): {final_holdout_threshold:.4f} (yielding F1 score: {best_f1_on_holdout:.4f})[/]\")\n",
    "\n",
    "        # Apply this optimal threshold\n",
    "        holdout_predictions = (holdout_avg_probs >= final_holdout_threshold).astype(int)\n",
    "\n",
    "        # --- Final Report on Holdout Set ---\n",
    "        console.rule(\"[bold magenta]Final Ensemble Performance on Holdout Set (valid.csv) with Optimized Threshold[/]\")\n",
    "        try:\n",
    "            # We already filtered labels (holdout_y_true) and predictions (holdout_predictions)\n",
    "            console.print(f\"Evaluating on {len(holdout_y_true)} holdout samples with valid labels.\")\n",
    "            report = classification_report(holdout_y_true, holdout_predictions, output_dict=True, digits=4, zero_division=0)\n",
    "\n",
    "            # Display Report Table\n",
    "            report_table = Table(show_header=True, header_style=\"bold magenta\", box=box.SIMPLE)\n",
    "            report_table.add_column(\"Class\", style=\"dim\", width=12)\n",
    "            report_table.add_column(\"Precision\", justify=\"right\")\n",
    "            report_table.add_column(\"Recall\", justify=\"right\")\n",
    "            report_table.add_column(\"F1-Score\", justify=\"right\")\n",
    "            report_table.add_column(\"Support\", justify=\"right\")\n",
    "\n",
    "            labels_in_report = [label for label in sorted(report.keys()) if label not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "            for label in labels_in_report:\n",
    "                 metrics = report[label]\n",
    "                 style = \"green\" if label == '1' and metrics['f1-score'] > 0.96 else \"\"\n",
    "                 report_table.add_row(str(label), f\"{metrics['precision']:.4f}\", f\"{metrics['recall']:.4f}\", f\"[bold {style}]{metrics['f1-score']:.4f}[/]\", f\"{int(metrics['support'])}\")\n",
    "\n",
    "            report_table.add_section()\n",
    "            for avg_type in [\"macro avg\", \"weighted avg\"]:\n",
    "                 if avg_type in report:\n",
    "                     metrics = report[avg_type]\n",
    "                     name = avg_type.replace(\" avg\", \" Avg\")\n",
    "                     report_table.add_row(f\"[bold]{name}[/]\", f\"{metrics['precision']:.4f}\", f\"{metrics['recall']:.4f}\", f\"{metrics['f1-score']:.4f}\", f\"{int(metrics['support'])}\")\n",
    "\n",
    "            if \"accuracy\" in report:\n",
    "                 accuracy = report[\"accuracy\"]\n",
    "                 total_support = int(report[\"weighted avg\"][\"support\"]) if \"weighted avg\" in report else len(holdout_y_true)\n",
    "                 report_table.add_section()\n",
    "                 report_table.add_row(\"[bold]Accuracy[/]\", \"\", \"\", f\"[bold]{accuracy:.4f}[/]\", f\"{total_support}\")\n",
    "\n",
    "            console.print(report_table)\n",
    "\n",
    "            # Display Confusion Matrix\n",
    "            console.print(\"\\n🎯 [bold blue]Holdout Confusion Matrix[/bold blue] (using optimized threshold)\\n\")\n",
    "            cm_labels = sorted(list(set(holdout_y_true) | set(holdout_predictions)))\n",
    "            if not cm_labels: cm_labels = [0, 1] # Default if only one class predicted/present\n",
    "            elif len(cm_labels) == 1: cm_labels = [0, 1] # Ensure both 0 and 1 are columns if only one exists\n",
    "\n",
    "            cm = confusion_matrix(holdout_y_true, holdout_predictions, labels=cm_labels)\n",
    "            cm_table = Table(title=\"True \\\\ Predicted\", box=box.SIMPLE_HEAVY, show_header=True, header_style=\"bold\")\n",
    "            cm_table.add_column(\"\", justify=\"center\", style=\"dim\") # Empty top-left corner\n",
    "            for label in cm_labels:\n",
    "                cm_table.add_column(f\"Pred {label}\", justify=\"center\")\n",
    "\n",
    "            for i, true_label in enumerate(cm_labels):\n",
    "                row_data = [f\"True {true_label}\"] + [str(cm[i, j]) for j in range(len(cm_labels))]\n",
    "                cm_table.add_row(*row_data)\n",
    "            console.print(cm_table)\n",
    "\n",
    "            # Explicitly check the target F1 score\n",
    "            f1_class_1 = report.get('1', {}).get('f1-score', 0.0)\n",
    "            if f1_class_1 > 0.96:\n",
    "                 console.print(Panel(f\"🚀 [bold green]Success![/] F1 score for Class 1 ({f1_class_1:.4f}) is above the 0.96 target!\", title=\"Target Check\", expand=False))\n",
    "            else:\n",
    "                 console.print(Panel(f\"⚠️ [bold yellow]Target Not Met.[/] F1 score for Class 1 ({f1_class_1:.4f}) is below 0.96.\", title=\"Target Check\", expand=False))\n",
    "\n",
    "\n",
    "        except Exception as report_e:\n",
    "            console.print(f\"[bold red]❌ Error generating final holdout report: {report_e}[/]\")\n",
    "\n",
    "\n",
    "console.print(f\"\\n[INFO] CV completed. Best models saved in subdirectories within '{BASE_OUTPUT_DIR}'.\")\n",
    "console.print(\"[INFO] Final evaluation performed on 'valid.csv' using an ensemble and a threshold optimized directly on holdout probabilities.\")\n",
    "console.print(\"[bold green]🏁 Script finished.[/]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deberta v3 base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">CUDA available. Using </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">GPU(</span><span style=\"color: #008080; text-decoration-color: #008080\">s</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">. Device: NVIDIA GeForce RTX </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3070</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCUDA available. Using \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m \u001b[0m\u001b[1;36mGPU\u001b[0m\u001b[1;36m(\u001b[0m\u001b[36ms\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m. Device: NVIDIA GeForce RTX \u001b[0m\u001b[1;36m3070\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Effective Batch Size: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #008080; text-decoration-color: #008080\">, Per-Device Batch Size: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #008080; text-decoration-color: #008080\">, Num GPUs: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> =&gt; Gradient Accumulation Steps: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mEffective Batch Size: \u001b[0m\u001b[1;36m8\u001b[0m\u001b[36m, Per-Device Batch Size: \u001b[0m\u001b[1;36m8\u001b[0m\u001b[36m, Num GPUs: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m => Gradient Accumulation Steps: \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Importing libraries</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mImporting libraries\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Libraries imported.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Libraries imported.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cleaning Training Data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cleaning Training Data\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training data loaded and cleaned. Total: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2521</span><span style=\"color: #008000; text-decoration-color: #008000\"> examples.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training data loaded and cleaned. Total: \u001b[0m\u001b[1;32m2521\u001b[0m\u001b[32m examples.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Training data label distribution:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1372</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1149</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Training data label distribution:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1372\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m1149\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cleaning Holdout Validation Data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cleaning Holdout Validation Data\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout Validation data loaded and cleaned. Total: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">786</span><span style=\"color: #008000; text-decoration-color: #008000\"> examples.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout Validation data loaded and cleaned. Total: \u001b[0m\u001b[1;32m786\u001b[0m\u001b[32m examples.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Holdout Validation data label distribution:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">420</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">366</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Holdout Validation data label distribution:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m420\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m366\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Tokenizer loaded from </span><span style=\"color: #008000; text-decoration-color: #008000\">'microsoft/deberta-v3-base'</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Tokenizer loaded from \u001b[0m\u001b[32m'microsoft/deberta-v3-base'\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> CV Run output will be saved under: <span style=\"color: #008000; text-decoration-color: #008000\">'./models\\deberta-v3-base_cv_5folds_ep6_bs8'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m CV Run output will be saved under: \u001b[32m'./models\\deberta-v3-base_cv_5folds_ep6_bs8'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────── </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Starting Cross-Validation on Training Data</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────── \u001b[0m\u001b[1;33mStarting Cross-Validation on Training Data\u001b[0m\u001b[92m ────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">505</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2016\u001b[0m, Fold Validation Size: \u001b[1;36m505\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1097</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1097\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">275</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m275\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculated class weights for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9188697</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0968444</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Calculated class weights for Fold \u001b[1;36m1\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.9188697\u001b[0m \u001b[1;36m1.0968444\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Safetensors PR exists\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Custom Trainer initialized with class weights </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">stored on CPU</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9188697</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0968444</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCustom Trainer initialized with class weights \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mstored on CPU\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m0.9188697\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m1.0968444\u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m1\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,016\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,512\n",
      "  Number of trainable parameters = 184,423,682\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1512' max='1512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1512/1512 2:16:28, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>0.349356</td>\n",
       "      <td>0.906542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.430959</td>\n",
       "      <td>0.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>0.251892</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.255335</td>\n",
       "      <td>0.951579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.240299</td>\n",
       "      <td>0.949153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.260030</td>\n",
       "      <td>0.951168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-252\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-252\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-252\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-504\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-504\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-504\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-756\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-756\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-756\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-252] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1008\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1008\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1008\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-504] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1260\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1260\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1260\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-756] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1512\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1512\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1512\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1260] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1008 (score: 0.9515789473684211).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed after </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">8189.</span><span style=\"color: #008000; text-decoration-color: #008000\">34s.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m completed after \u001b[0m\u001b[1;32m8189.\u001b[0m\u001b[32m34s.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 score on internal validation set during training: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9516</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m - Best F1 score on internal validation set during training: \u001b[0m\u001b[1;36m0.9516\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and finding best threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> <span style=\"font-weight: bold\">(</span>using its internal validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and finding best threshold for Fold \u001b[1;36m1\u001b[0m \u001b[1m(\u001b[0musing its internal validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> Internal Validation Metrics <span style=\"font-weight: bold\">(</span>at threshold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'test_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.25533491373062134</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_f1'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9515789473684211</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_runtime'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.6471</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_samples_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.468</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_steps_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.98</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m1\u001b[0m Internal Validation Metrics \u001b[1m(\u001b[0mat threshold \u001b[1;36m0.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'test_loss'\u001b[0m: \u001b[1;36m0.25533491373062134\u001b[0m, \u001b[32m'test_f1'\u001b[0m: \n",
       "\u001b[1;36m0.9515789473684211\u001b[0m, \u001b[32m'test_runtime'\u001b[0m: \u001b[1;36m32.6471\u001b[0m, \u001b[32m'test_samples_per_second'\u001b[0m: \u001b[1;36m15.468\u001b[0m, \u001b[32m'test_steps_per_second'\u001b[0m: \u001b[1;36m0.98\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Internal Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9556</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Optimized Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8799</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m - Best F1 on Internal Validation: \u001b[0m\u001b[1;36m0.9556\u001b[0m\u001b[36m @ Optimized Threshold = \u001b[0m\u001b[1;36m0.8799\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model\n",
      "Configuration saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model\\config.json\n",
      "Model weights saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model\\model.safetensors\n",
      "tokenizer config file saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Best model for Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Best model for Fold \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Optimal threshold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.8799</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model\\threshold.txt'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Optimal threshold \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m0.8799\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model\\threshold.txt'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up temporary checkpoints directory: </span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up temporary checkpoints directory: \u001b[0m\n",
       "\u001b[2;32m'./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_1'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">1</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m2\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1098</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1098\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">274</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m274\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculated class weights for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span> <span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Calculated class weights for Fold \u001b[1;36m2\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m \u001b[1;36m1.0973885\u001b[0m \u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--microsoft--deberta-v3-base\\snapshots\\8ccc9b6f36199bec6961081d44eb72fb3f7353f3\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--microsoft--deberta-v3-base\\snapshots\\8ccc9b6f36199bec6961081d44eb72fb3f7353f3\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Custom Trainer initialized with class weights </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">stored on CPU</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCustom Trainer initialized with class weights \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mstored on CPU\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m1.0973885\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m2\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,518\n",
      "  Number of trainable parameters = 184,423,682\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1265' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1265/1518 21:52 < 04:22, 0.96 it/s, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.362900</td>\n",
       "      <td>0.188479</td>\n",
       "      <td>0.948276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.209172</td>\n",
       "      <td>0.959488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.174820</td>\n",
       "      <td>0.958606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.223853</td>\n",
       "      <td>0.952586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.245156</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-253\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-253\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-253\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-506\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-506\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-506\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-759\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-759\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-759\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-253] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-1012\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-1012\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-1012\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-759] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-1265\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-1265\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-1265\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-1012] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\checkpoint-506 (score: 0.9594882729211087).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed after </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1313.</span><span style=\"color: #008000; text-decoration-color: #008000\">16s.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m2\u001b[0m\u001b[32m completed after \u001b[0m\u001b[1;32m1313.\u001b[0m\u001b[32m16s.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 score on internal validation set during training: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9595</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m2\u001b[0m\u001b[36m - Best F1 score on internal validation set during training: \u001b[0m\u001b[1;36m0.9595\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and finding best threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> <span style=\"font-weight: bold\">(</span>using its internal validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and finding best threshold for Fold \u001b[1;36m2\u001b[0m \u001b[1m(\u001b[0musing its internal validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> Internal Validation Metrics <span style=\"font-weight: bold\">(</span>at threshold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'test_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2091715931892395</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_f1'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9594882729211087</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_runtime'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.7456</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_samples_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42.91</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_steps_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.724</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m2\u001b[0m Internal Validation Metrics \u001b[1m(\u001b[0mat threshold \u001b[1;36m0.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'test_loss'\u001b[0m: \u001b[1;36m0.2091715931892395\u001b[0m, \u001b[32m'test_f1'\u001b[0m: \n",
       "\u001b[1;36m0.9594882729211087\u001b[0m, \u001b[32m'test_runtime'\u001b[0m: \u001b[1;36m11.7456\u001b[0m, \u001b[32m'test_samples_per_second'\u001b[0m: \u001b[1;36m42.91\u001b[0m, \u001b[32m'test_steps_per_second'\u001b[0m: \u001b[1;36m2.724\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Internal Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9620</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Optimized Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0512</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m2\u001b[0m\u001b[36m - Best F1 on Internal Validation: \u001b[0m\u001b[1;36m0.9620\u001b[0m\u001b[36m @ Optimized Threshold = \u001b[0m\u001b[1;36m0.0512\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model\n",
      "Configuration saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model\\config.json\n",
      "Model weights saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model\\model.safetensors\n",
      "tokenizer config file saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Best model for Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Best model for Fold \u001b[0m\u001b[1;32m2\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Optimal threshold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.0512</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model\\threshold.txt'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Optimal threshold \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m0.0512\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model\\threshold.txt'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up temporary checkpoints directory: </span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up temporary checkpoints directory: \u001b[0m\n",
       "\u001b[2;32m'./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_2'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">2</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m2\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">3</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m3\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1098</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1098\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">274</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m274\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculated class weights for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span> <span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Calculated class weights for Fold \u001b[1;36m3\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m \u001b[1;36m1.0973885\u001b[0m \u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--microsoft--deberta-v3-base\\snapshots\\8ccc9b6f36199bec6961081d44eb72fb3f7353f3\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--microsoft--deberta-v3-base\\snapshots\\8ccc9b6f36199bec6961081d44eb72fb3f7353f3\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Custom Trainer initialized with class weights </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">stored on CPU</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCustom Trainer initialized with class weights \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mstored on CPU\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m1.0973885\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m3\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,518\n",
      "  Number of trainable parameters = 184,423,682\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1518/1518 20:12, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.354100</td>\n",
       "      <td>0.211635</td>\n",
       "      <td>0.946004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.159632</td>\n",
       "      <td>0.963753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.258541</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>0.205654</td>\n",
       "      <td>0.965665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.241276</td>\n",
       "      <td>0.961864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.227276</td>\n",
       "      <td>0.965957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-253\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-253\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-253\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-506\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-506\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-506\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-759\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-759\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-759\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-253] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-1012\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-1012\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-1012\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-506] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-1265\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-1265\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-1265\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-759] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-1518\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-1518\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-1518\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-1012] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\checkpoint-1518 (score: 0.9659574468085106).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed after </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1212.</span><span style=\"color: #008000; text-decoration-color: #008000\">78s.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m completed after \u001b[0m\u001b[1;32m1212.\u001b[0m\u001b[32m78s.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 score on internal validation set during training: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9660</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m3\u001b[0m\u001b[36m - Best F1 score on internal validation set during training: \u001b[0m\u001b[1;36m0.9660\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and finding best threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> <span style=\"font-weight: bold\">(</span>using its internal validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and finding best threshold for Fold \u001b[1;36m3\u001b[0m \u001b[1m(\u001b[0musing its internal validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> Internal Validation Metrics <span style=\"font-weight: bold\">(</span>at threshold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'test_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.22727636992931366</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_f1'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9659574468085106</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_runtime'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.546</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_samples_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58.975</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_steps_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.744</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m3\u001b[0m Internal Validation Metrics \u001b[1m(\u001b[0mat threshold \u001b[1;36m0.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'test_loss'\u001b[0m: \u001b[1;36m0.22727636992931366\u001b[0m, \u001b[32m'test_f1'\u001b[0m: \n",
       "\u001b[1;36m0.9659574468085106\u001b[0m, \u001b[32m'test_runtime'\u001b[0m: \u001b[1;36m8.546\u001b[0m, \u001b[32m'test_samples_per_second'\u001b[0m: \u001b[1;36m58.975\u001b[0m, \u001b[32m'test_steps_per_second'\u001b[0m: \u001b[1;36m3.744\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Internal Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9660</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Optimized Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5448</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m3\u001b[0m\u001b[36m - Best F1 on Internal Validation: \u001b[0m\u001b[1;36m0.9660\u001b[0m\u001b[36m @ Optimized Threshold = \u001b[0m\u001b[1;36m0.5448\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model\n",
      "Configuration saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model\\config.json\n",
      "Model weights saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model\\model.safetensors\n",
      "tokenizer config file saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Best model for Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Best model for Fold \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Optimal threshold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.5448</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model\\threshold.txt'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Optimal threshold \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m0.5448\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model\\threshold.txt'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up temporary checkpoints directory: </span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up temporary checkpoints directory: \u001b[0m\n",
       "\u001b[2;32m'./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_3'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">3</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m3\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">4</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m4\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1098</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1098\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">274</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m274\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculated class weights for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span> <span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Calculated class weights for Fold \u001b[1;36m4\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m \u001b[1;36m1.0973885\u001b[0m \u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--microsoft--deberta-v3-base\\snapshots\\8ccc9b6f36199bec6961081d44eb72fb3f7353f3\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--microsoft--deberta-v3-base\\snapshots\\8ccc9b6f36199bec6961081d44eb72fb3f7353f3\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Custom Trainer initialized with class weights </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">stored on CPU</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCustom Trainer initialized with class weights \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mstored on CPU\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m1.0973885\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m4\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,518\n",
      "  Number of trainable parameters = 184,423,682\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1518/1518 24:19, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.358600</td>\n",
       "      <td>0.262883</td>\n",
       "      <td>0.928425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.191100</td>\n",
       "      <td>0.245104</td>\n",
       "      <td>0.949367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.117300</td>\n",
       "      <td>0.407538</td>\n",
       "      <td>0.917836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.238601</td>\n",
       "      <td>0.948936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.265879</td>\n",
       "      <td>0.951168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.334746</td>\n",
       "      <td>0.949580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-253\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-253\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-253\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-506\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-506\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-506\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-759\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-759\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-759\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-253] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-1012\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-1012\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-1012\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-759] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-1265\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-1265\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-1265\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-506] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-1518\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-1518\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-1518\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-1012] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\checkpoint-1265 (score: 0.9511677282377919).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed after </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1460.</span><span style=\"color: #008000; text-decoration-color: #008000\">18s.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m completed after \u001b[0m\u001b[1;32m1460.\u001b[0m\u001b[32m18s.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 score on internal validation set during training: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9512</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m - Best F1 score on internal validation set during training: \u001b[0m\u001b[1;36m0.9512\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and finding best threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> <span style=\"font-weight: bold\">(</span>using its internal validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and finding best threshold for Fold \u001b[1;36m4\u001b[0m \u001b[1m(\u001b[0musing its internal validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> Internal Validation Metrics <span style=\"font-weight: bold\">(</span>at threshold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'test_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2658790051937103</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_f1'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9511677282377919</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_runtime'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.1972</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_samples_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">54.799</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_steps_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.479</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m4\u001b[0m Internal Validation Metrics \u001b[1m(\u001b[0mat threshold \u001b[1;36m0.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'test_loss'\u001b[0m: \u001b[1;36m0.2658790051937103\u001b[0m, \u001b[32m'test_f1'\u001b[0m: \n",
       "\u001b[1;36m0.9511677282377919\u001b[0m, \u001b[32m'test_runtime'\u001b[0m: \u001b[1;36m9.1972\u001b[0m, \u001b[32m'test_samples_per_second'\u001b[0m: \u001b[1;36m54.799\u001b[0m, \u001b[32m'test_steps_per_second'\u001b[0m: \u001b[1;36m3.479\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Internal Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9581</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Optimized Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9987</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m4\u001b[0m\u001b[36m - Best F1 on Internal Validation: \u001b[0m\u001b[1;36m0.9581\u001b[0m\u001b[36m @ Optimized Threshold = \u001b[0m\u001b[1;36m0.9987\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model\n",
      "Configuration saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model\\config.json\n",
      "Model weights saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model\\model.safetensors\n",
      "tokenizer config file saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Best model for Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Best model for Fold \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Optimal threshold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.9987</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model\\threshold.txt'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Optimal threshold \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m0.9987\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model\\threshold.txt'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up temporary checkpoints directory: </span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up temporary checkpoints directory: \u001b[0m\n",
       "\u001b[2;32m'./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_4'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">4</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m4\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m5\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1097</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">920</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1097\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m920\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">275</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">229</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m275\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m229\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculated class weights for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9193254</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0961957</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Calculated class weights for Fold \u001b[1;36m5\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.9193254\u001b[0m \u001b[1;36m1.0961957\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--microsoft--deberta-v3-base\\snapshots\\8ccc9b6f36199bec6961081d44eb72fb3f7353f3\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--microsoft--deberta-v3-base\\snapshots\\8ccc9b6f36199bec6961081d44eb72fb3f7353f3\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Custom Trainer initialized with class weights </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">stored on CPU</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9193254</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0961957</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCustom Trainer initialized with class weights \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mstored on CPU\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m0.9193254\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m1.0961957\u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m5\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,518\n",
      "  Number of trainable parameters = 184,423,682\n",
      "Attempting to create safetensors variant\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1518/1518 26:39, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.355400</td>\n",
       "      <td>0.521055</td>\n",
       "      <td>0.873077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.247488</td>\n",
       "      <td>0.941935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.213631</td>\n",
       "      <td>0.954839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.223042</td>\n",
       "      <td>0.958606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.263705</td>\n",
       "      <td>0.952991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.287039</td>\n",
       "      <td>0.953191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Safetensors PR exists\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-253\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-253\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-253\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-506\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-506\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-506\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-759\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-759\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-759\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-253] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-1012\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-1012\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-1012\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-506] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-1265\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-1265\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-1265\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-759] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-1518\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-1518\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-1518\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-1265] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\checkpoint-1012 (score: 0.9586056644880174).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed after </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1599.</span><span style=\"color: #008000; text-decoration-color: #008000\">87s.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m5\u001b[0m\u001b[32m completed after \u001b[0m\u001b[1;32m1599.\u001b[0m\u001b[32m87s.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 score on internal validation set during training: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9586</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m5\u001b[0m\u001b[36m - Best F1 score on internal validation set during training: \u001b[0m\u001b[1;36m0.9586\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and finding best threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> <span style=\"font-weight: bold\">(</span>using its internal validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and finding best threshold for Fold \u001b[1;36m5\u001b[0m \u001b[1m(\u001b[0musing its internal validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> Internal Validation Metrics <span style=\"font-weight: bold\">(</span>at threshold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'test_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2230423092842102</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_f1'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9586056644880174</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_runtime'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.5836</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_samples_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47.621</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_steps_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.024</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m5\u001b[0m Internal Validation Metrics \u001b[1m(\u001b[0mat threshold \u001b[1;36m0.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'test_loss'\u001b[0m: \u001b[1;36m0.2230423092842102\u001b[0m, \u001b[32m'test_f1'\u001b[0m: \n",
       "\u001b[1;36m0.9586056644880174\u001b[0m, \u001b[32m'test_runtime'\u001b[0m: \u001b[1;36m10.5836\u001b[0m, \u001b[32m'test_samples_per_second'\u001b[0m: \u001b[1;36m47.621\u001b[0m, \u001b[32m'test_steps_per_second'\u001b[0m: \u001b[1;36m3.024\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Internal Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9625</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Optimized Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9936</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m5\u001b[0m\u001b[36m - Best F1 on Internal Validation: \u001b[0m\u001b[1;36m0.9625\u001b[0m\u001b[36m @ Optimized Threshold = \u001b[0m\u001b[1;36m0.9936\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model\n",
      "Configuration saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model\\config.json\n",
      "Model weights saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model\\model.safetensors\n",
      "tokenizer config file saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Best model for Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Best model for Fold \u001b[0m\u001b[1;32m5\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Optimal threshold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.9936</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model\\threshold.txt'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Optimal threshold \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m0.9936\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model\\threshold.txt'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up temporary checkpoints directory: </span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up temporary checkpoints directory: \u001b[0m\n",
       "\u001b[2;32m'./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8\\fold_5'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">5</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m5\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────── </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Cross-Validation Summary (on Internal Validation Splits)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m──────────────────────────── \u001b[0m\u001b[1;32mCross-Validation Summary \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mon Internal Validation Splits\u001b[0m\u001b[1;32m)\u001b[0m\u001b[92m ─────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Fold Performance on Internal Validation Splits         </span>\n",
       "┏━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Fold </span>┃<span style=\"font-weight: bold\"> F1 (Thr=0.5) </span>┃<span style=\"font-weight: bold\"> Optimized F1 </span>┃<span style=\"font-weight: bold\"> Optimal Thr (Internal) </span>┃\n",
       "┡━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 1    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9516 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       0.9556 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                 0.8799 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 2    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9595 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       0.9620 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                 0.0512 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 3    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9660 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       0.9660 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                 0.5448 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 4    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9512 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       0.9581 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                 0.9987 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> 5    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9586 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       0.9625 </span>│<span style=\"color: #808000; text-decoration-color: #808000\">                 0.9936 </span>│\n",
       "└──────┴──────────────┴──────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m        Fold Performance on Internal Validation Splits         \u001b[0m\n",
       "┏━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mFold\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mF1 (Thr=0.5)\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOptimized F1\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOptimal Thr (Internal)\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m1   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9516\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      0.9556\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                0.8799\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m2   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9595\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      0.9620\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                0.0512\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m3   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9660\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      0.9660\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                0.5448\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m4   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9512\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      0.9581\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                0.9987\u001b[0m\u001b[33m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m5   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9586\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      0.9625\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m                0.9936\u001b[0m\u001b[33m \u001b[0m│\n",
       "└──────┴──────────────┴──────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Avg F1 <span style=\"font-weight: bold\">(</span>Internal Val, <span style=\"color: #808000; text-decoration-color: #808000\">Thr</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">0.9574</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> +/- </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">0.0055</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Avg F1 \u001b[1m(\u001b[0mInternal Val, \u001b[33mThr\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35m0.9574\u001b[0m\u001b[1;35m +\u001b[0m\u001b[1;35m/\u001b[0m\u001b[1;35m-\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m0.0055\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Avg F1 <span style=\"font-weight: bold\">(</span>Internal Val, Optimized Thr<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.9608</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> +/- </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.0036</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Avg F1 \u001b[1m(\u001b[0mInternal Val, Optimized Thr\u001b[1m)\u001b[0m: \u001b[1;32m0.9608\u001b[0m\u001b[1;32m +\u001b[0m\u001b[1;32m/\u001b[0m\u001b[1;32m-\u001b[0m\u001b[1;32m \u001b[0m\u001b[1;32m0.0036\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Avg Optimal Threshold <span style=\"font-weight: bold\">(</span>Internal<span style=\"font-weight: bold\">)</span>: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.6937</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> +/- </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0.3613</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Avg Optimal Threshold \u001b[1m(\u001b[0mInternal\u001b[1m)\u001b[0m: \u001b[1;33m0.6937\u001b[0m\u001b[1;33m +\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m \u001b[0m\u001b[1;33m0.3613\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up base temporary checkpoint directory: </span><span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up base temporary checkpoint directory: \u001b[0m\u001b[2;32m'./models\\_cv_temp_checkpoints_deberta-v3-base_cv_5folds_ep6_bs8'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Final Evaluation on Holdout Set (valid.csv)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────── \u001b[0m\u001b[1;35mFinal Evaluation on Holdout Set \u001b[0m\u001b[1;35m(\u001b[0m\u001b[1;35mvalid.csv\u001b[0m\u001b[1;35m)\u001b[0m\u001b[92m ───────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Preparing holdout validation dataset <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">786</span> samples<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Preparing holdout validation dataset \u001b[1m(\u001b[0m\u001b[1;36m786\u001b[0m samples\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Using </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">786</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> valid ground truth labels from holdout set for final evaluation.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mUsing \u001b[0m\u001b[1;2;36m786\u001b[0m\u001b[2m valid ground truth labels from holdout set for final evaluation.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running ensemble inference on holdout set using <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> models<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running ensemble inference on holdout set using \u001b[1;36m5\u001b[0m models\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m1\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33m.\u001b[0m\u001b[33m/\u001b[0m\u001b[33mmodels\u001b[0m\u001b[33m\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spm.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model\\model.safetensors\n",
      "All model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_1\\best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b358b09de94560bbadb6ec564b0db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m2\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33m.\u001b[0m\u001b[33m/\u001b[0m\u001b[33mmodels\u001b[0m\u001b[33m\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spm.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model\\model.safetensors\n",
      "All model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_2\\best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9946a4acfdcf420f89c92e7417135099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m2\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m3\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33m.\u001b[0m\u001b[33m/\u001b[0m\u001b[33mmodels\u001b[0m\u001b[33m\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spm.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model\\model.safetensors\n",
      "All model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_3\\best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5df300095d04f36aec73d8a9c24928c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m4\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33m.\u001b[0m\u001b[33m/\u001b[0m\u001b[33mmodels\u001b[0m\u001b[33m\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spm.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model\\model.safetensors\n",
      "All model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_4\\best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0bb8584d3f40c5a6258ef02e61433c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Loading model <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> from <span style=\"color: #808000; text-decoration-color: #808000\">./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- Loading model \u001b[1;36m5\u001b[0m/\u001b[1;36m5\u001b[0m from \u001b[33m.\u001b[0m\u001b[33m/\u001b[0m\u001b[33mmodels\u001b[0m\u001b[33m\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spm.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"legacy\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model\\model.safetensors\n",
      "All model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at ./models\\deberta-v3-base_cv_5folds_ep6_bs8\\fold_5\\best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f68d9bab0a040e9aa20b6a58c7d4e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout predictions collected for model </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout predictions collected for model \u001b[0m\u001b[1;32m5\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Combining holdout probabilities from </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> models (using nanmean)...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mCombining holdout probabilities from \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;36m models \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36musing nanmean\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Optimizing final threshold directly on holdout ensemble probabilities </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">targeting F1 for class </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mOptimizing final threshold directly on holdout ensemble probabilities \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mtargeting F1 for class \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">✓ Optimal threshold for holdout set (Class </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> F1): </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.8147</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> (yielding F1 score: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.9656</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m✓ Optimal threshold for holdout set \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mClass \u001b[0m\u001b[1;32m1\u001b[0m\u001b[1;32m F1\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m: \u001b[0m\u001b[1;32m0.8147\u001b[0m\u001b[1;32m \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32myielding F1 score: \u001b[0m\u001b[1;32m0.9656\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Final Ensemble Performance on Holdout Set (valid.csv) with Optimized Threshold</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────── \u001b[0m\u001b[1;35mFinal Ensemble Performance on Holdout Set \u001b[0m\u001b[1;35m(\u001b[0m\u001b[1;35mvalid.csv\u001b[0m\u001b[1;35m)\u001b[0m\u001b[1;35m with Optimized Threshold\u001b[0m\u001b[92m ──────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">786</span> holdout samples with valid labels.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating on \u001b[1;36m786\u001b[0m holdout samples with valid labels.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                          \n",
       " <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Class        </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Precision </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Recall </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> F1-Score </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Support </span> \n",
       " ──────────────────────────────────────────────────────── \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0            </span>     0.9647   0.9762     <span style=\"font-weight: bold\">0.9704</span>       420  \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1            </span>     0.9723   0.9590     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.9656</span>       366  \n",
       "                                                          \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">macro Avg</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">    </span>     0.9685   0.9676     0.9680       786  \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">weighted Avg</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> </span>     0.9682   0.9682     0.9682       786  \n",
       "                                                          \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">Accuracy</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">     </span>                         <span style=\"font-weight: bold\">0.9682</span>       786  \n",
       "                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                          \n",
       " \u001b[1;35m \u001b[0m\u001b[1;35mClass       \u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35mPrecision\u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35mRecall\u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35mF1-Score\u001b[0m\u001b[1;35m \u001b[0m \u001b[1;35m \u001b[0m\u001b[1;35mSupport\u001b[0m\u001b[1;35m \u001b[0m \n",
       " ──────────────────────────────────────────────────────── \n",
       " \u001b[2m \u001b[0m\u001b[2m0           \u001b[0m\u001b[2m \u001b[0m     0.9647   0.9762     \u001b[1m0.9704\u001b[0m       420  \n",
       " \u001b[2m \u001b[0m\u001b[2m1           \u001b[0m\u001b[2m \u001b[0m     0.9723   0.9590     \u001b[1;32m0.9656\u001b[0m       366  \n",
       "                                                          \n",
       " \u001b[2m \u001b[0m\u001b[1;2mmacro Avg\u001b[0m\u001b[2m   \u001b[0m\u001b[2m \u001b[0m     0.9685   0.9676     0.9680       786  \n",
       " \u001b[2m \u001b[0m\u001b[1;2mweighted Avg\u001b[0m\u001b[2m \u001b[0m     0.9682   0.9682     0.9682       786  \n",
       "                                                          \n",
       " \u001b[2m \u001b[0m\u001b[1;2mAccuracy\u001b[0m\u001b[2m    \u001b[0m\u001b[2m \u001b[0m                         \u001b[1m0.9682\u001b[0m       786  \n",
       "                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "🎯 <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Holdout Confusion Matrix</span> <span style=\"font-weight: bold\">(</span>using optimized threshold<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "🎯 \u001b[1;34mHoldout Confusion Matrix\u001b[0m \u001b[1m(\u001b[0musing optimized threshold\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">      True \\ Predicted      </span>\n",
       "                            \n",
       " <span style=\"font-weight: bold\">        </span> <span style=\"font-weight: bold\"> Pred 0 </span> <span style=\"font-weight: bold\"> Pred 1 </span> \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> True 0 </span>   410       10    \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> True 1 </span>    15      351    \n",
       "                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m      True \\ Predicted      \u001b[0m\n",
       "                            \n",
       " \u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mPred 0\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mPred 1\u001b[0m\u001b[1m \u001b[0m \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " \u001b[2m \u001b[0m\u001b[2mTrue 0\u001b[0m\u001b[2m \u001b[0m   410       10    \n",
       " \u001b[2m \u001b[0m\u001b[2mTrue 1\u001b[0m\u001b[2m \u001b[0m    15      351    \n",
       "                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────── Target Check ────────────────────────────╮\n",
       "│ 🚀 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Success!</span> F1 score for Class 1 (0.9656) is above the 0.96 target! │\n",
       "╰─────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────── Target Check ────────────────────────────╮\n",
       "│ 🚀 \u001b[1;32mSuccess!\u001b[0m F1 score for Class 1 (0.9656) is above the 0.96 target! │\n",
       "╰─────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> CV completed. Best models saved under <span style=\"color: #008000; text-decoration-color: #008000\">'./models\\deberta-v3-base_cv_5folds_ep6_bs8'</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m CV completed. Best models saved under \u001b[32m'./models\\deberta-v3-base_cv_5folds_ep6_bs8'\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> Each fold's best model is in <span style=\"color: #008000; text-decoration-color: #008000\">'fold_X/best_model/'</span> including <span style=\"color: #008000; text-decoration-color: #008000\">'threshold.txt'</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m Each fold's best model is in \u001b[32m'fold_X/best_model/'\u001b[0m including \u001b[32m'threshold.txt'\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> Final evaluation performed on <span style=\"color: #008000; text-decoration-color: #008000\">'valid.csv'</span> using an ensemble and a threshold optimized directly on holdout \n",
       "probabilities.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m Final evaluation performed on \u001b[32m'valid.csv'\u001b[0m using an ensemble and a threshold optimized directly on holdout \n",
       "probabilities.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">🏁 Script finished.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m🏁 Script finished.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep lightweight imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn # Needed for custom loss\n",
    "import os\n",
    "import shutil\n",
    "import sys # For exit on critical errors\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import gc\n",
    "# Defer heavy imports\n",
    "# from transformers import ...\n",
    "# from sklearn.metrics import ...\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration (MODIFIED)\n",
    "# -------------------------------\n",
    "SEED = 42\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "N_SPLITS = 5\n",
    "MAX_LENGTH = 512\n",
    "EFFECTIVE_BATCH_SIZE = 8\n",
    "PER_DEVICE_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 6\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "WEIGHT_DECAY = 0.01\n",
    "# --- NEW: Define a name for this specific CV run ---\n",
    "# This will create a subfolder in BASE_OUTPUT_DIR\n",
    "RUN_NAME = f\"{MODEL_NAME.split('/')[-1]}_cv_{N_SPLITS}folds_ep{EPOCHS}_bs{EFFECTIVE_BATCH_SIZE}\"\n",
    "\n",
    "# Output directory for CV models and results\n",
    "BASE_OUTPUT_DIR = \"./models\" # Base directory where run folders will be created\n",
    "DATA_DIR = \"./data\" # Directory containing train.csv and valid.csv\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    NUM_GPUS = torch.cuda.device_count()\n",
    "    console.print(f\"[cyan]CUDA available. Using {NUM_GPUS} GPU(s). Device: {torch.cuda.get_device_name(0)}[/]\")\n",
    "else:\n",
    "    NUM_GPUS = 1 # Assume 1 for calculation if CPU\n",
    "    console.print(\"[yellow]⚠️ CUDA not available. Training on CPU (will be very slow).[/]\")\n",
    "\n",
    "# Calculate Gradient Accumulation Steps\n",
    "GRADIENT_ACCUMULATION_STEPS = max(1, EFFECTIVE_BATCH_SIZE // (PER_DEVICE_BATCH_SIZE * NUM_GPUS))\n",
    "console.print(f\"[cyan]Effective Batch Size: {EFFECTIVE_BATCH_SIZE}, Per-Device Batch Size: {PER_DEVICE_BATCH_SIZE}, Num GPUs: {NUM_GPUS} => Gradient Accumulation Steps: {GRADIENT_ACCUMULATION_STEPS}[/]\")\n",
    "\n",
    "\n",
    "# --- Defer heavy library imports ---\n",
    "console.print(\"[dim]Importing libraries...[/]\")\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSequenceClassification,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        EarlyStoppingCallback\n",
    "    )\n",
    "    from sklearn.metrics import f1_score, precision_recall_curve, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "    from rich.panel import Panel\n",
    "    from rich import box\n",
    "    from rich.progress import track\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from transformers import TrainerCallback # Needed for custom trainer loss\n",
    "except ImportError as e:\n",
    "    console.print(f\"[bold red]Error: Missing required library -> {e}[/]\")\n",
    "    console.print(\"[yellow]Please install all necessary libraries (pandas, torch, transformers[accelerate], scikit-learn, rich, tqdm).[/]\")\n",
    "    sys.exit(1)\n",
    "console.print(\"[green]✓ Libraries imported.[/]\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Data (Separately!)\n",
    "# -------------------------------\n",
    "train_csv_path = os.path.join(DATA_DIR, \"train.csv\")\n",
    "valid_csv_path = os.path.join(DATA_DIR, \"valid.csv\")\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    holdout_valid_df = pd.read_csv(valid_csv_path) # Load original valid set separately\n",
    "\n",
    "    # --- Data Cleaning Function ---\n",
    "    def clean_dataframe(df, name):\n",
    "        console.print(f\"Cleaning {name} Data...\")\n",
    "        initial_count = len(df)\n",
    "        # Standardize column names (handle potential variations)\n",
    "        df.columns = [col.lower().strip() for col in df.columns]\n",
    "        if 'labels' in df.columns and 'label' not in df.columns:\n",
    "            df = df.rename(columns={\"labels\": \"label\"})\n",
    "\n",
    "        # Check required columns\n",
    "        required_cols = ['text', 'label']\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            missing = [col for col in required_cols if col not in df.columns]\n",
    "            raise KeyError(f\"Missing required columns in {name}: {missing}\")\n",
    "\n",
    "        # Add 'id' column if not present (using index)\n",
    "        if 'id' not in df.columns:\n",
    "             console.print(f\"[dim]Adding 'id' column based on index to {name} data.[/]\")\n",
    "             df['id'] = df.index\n",
    "\n",
    "        # Drop rows with NaNs in text or label\n",
    "        nan_rows = df['text'].isnull() | df['label'].isnull()\n",
    "        if nan_rows.any():\n",
    "            console.print(f\"[yellow]⚠️ NaNs found in {name} data. Dropping {nan_rows.sum()} rows...[/]\")\n",
    "            df = df[~nan_rows].copy()\n",
    "\n",
    "        # Ensure label is numeric and then integer\n",
    "        df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
    "        label_nan_rows = df['label'].isnull()\n",
    "        if label_nan_rows.any():\n",
    "            console.print(f\"[yellow]⚠️ Non-numeric labels found after coercion in {name}. Dropping {label_nan_rows.sum()} rows...[/]\")\n",
    "            df = df[~label_nan_rows].copy()\n",
    "\n",
    "        df['label'] = df['label'].astype(int)\n",
    "\n",
    "        # Ensure text is string\n",
    "        df['text'] = df['text'].astype(str)\n",
    "\n",
    "        cleaned_count = len(df)\n",
    "        if cleaned_count < initial_count:\n",
    "            console.print(f\"[dim]Dropped {initial_count - cleaned_count} rows from {name}.[/]\")\n",
    "\n",
    "        # Check for valid labels (0 and 1)\n",
    "        valid_labels = {0, 1}\n",
    "        if not set(df['label'].unique()).issubset(valid_labels):\n",
    "            invalid_labels = set(df['label'].unique()) - valid_labels\n",
    "            console.print(f\"[yellow]⚠️ Invalid labels found in {name}: {invalid_labels}. Keeping only 0 and 1.[/]\")\n",
    "            df = df[df['label'].isin(valid_labels)].copy()\n",
    "            if len(df) < cleaned_count:\n",
    "                 console.print(f\"[dim]Dropped {cleaned_count - len(df)} rows with invalid labels.[/]\")\n",
    "\n",
    "\n",
    "        console.print(f\"[green]✓ {name} data loaded and cleaned. Total: {len(df)} examples.[/]\")\n",
    "        # Check label distribution\n",
    "        label_counts = df['label'].value_counts()\n",
    "        console.print(f\"{name} data label distribution:\\n{label_counts}\")\n",
    "        if len(label_counts) < 2 and name == \"Training\":\n",
    "             console.print(\"[bold red]Error: Training data must contain both labels 0 and 1 for stratified splitting.[/]\")\n",
    "             sys.exit(1)\n",
    "        return df.reset_index(drop=True) # Reset index after cleaning\n",
    "\n",
    "    train_df = clean_dataframe(train_df, \"Training\")\n",
    "    holdout_valid_df = clean_dataframe(holdout_valid_df, \"Holdout Validation\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    console.print(f\"[bold red]Error: CSV file not found - {e}. Check paths '{train_csv_path}' and '{valid_csv_path}'.[/]\")\n",
    "    sys.exit(1)\n",
    "except KeyError as e:\n",
    "    console.print(f\"[bold red]Error: Missing expected column in CSV - {e}. Ensure 'text' and 'label' (or 'labels') exist.[/]\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]Unexpected error loading/cleaning data: {e}[/]\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define Dataset Class (No changes needed here, uses MAX_LENGTH from config)\n",
    "# -------------------------------\n",
    "class VaccineDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, ids=None, tokenizer=None, max_length=512, is_inference=False):\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided.\")\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.ids = ids # Store IDs if provided\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_inference = (labels is None)\n",
    "\n",
    "        if not self.is_inference and (self.labels is None or len(texts) != len(labels)):\n",
    "            raise ValueError(\"Texts and Labels must be provided and have the same length for training/evaluation.\")\n",
    "        if self.ids is not None and len(texts) != len(self.ids):\n",
    "             raise ValueError(\"Texts and IDs must have the same length if IDs are provided.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx]) if idx < len(self.texts) and self.texts[idx] is not None else \"\"\n",
    "        item_id = self.ids[idx] if self.ids is not None and idx < len(self.ids) else idx # Use index as fallback ID\n",
    "\n",
    "        try:\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "            if not self.is_inference:\n",
    "                if idx < len(self.labels):\n",
    "                    label = self.labels[idx]\n",
    "                    item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "                else:\n",
    "                     item['labels'] = torch.tensor(-1, dtype=torch.long) # Should not happen\n",
    "\n",
    "            if self.ids is not None:\n",
    "                item['id'] = item_id # Keep ID as is (numeric or string)\n",
    "\n",
    "            return item\n",
    "\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error in __getitem__ at index {idx} (ID: {item_id}): {e}[/]\")\n",
    "            dummy_item = {\n",
    "                'input_ids': torch.zeros(self.max_length, dtype=torch.long),\n",
    "                'attention_mask': torch.zeros(self.max_length, dtype=torch.long),\n",
    "            }\n",
    "            if not self.is_inference:\n",
    "                dummy_item['labels'] = torch.tensor(-1, dtype=torch.long)\n",
    "            if self.ids is not None:\n",
    "                dummy_item['id'] = item_id # Return the original ID even for dummy item\n",
    "            return dummy_item\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load Tokenizer (once)\n",
    "# -------------------------------\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    console.print(f\"[green]✓ Tokenizer loaded from '{MODEL_NAME}'.[/]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]Error loading tokenizer '{MODEL_NAME}': {e}[/]\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Metric Function (for Trainer, based on argmax)\n",
    "# -------------------------------\n",
    "def compute_metrics_for_trainer(eval_pred):\n",
    "    \"\"\"Calculates F1 based on argmax for checkpoint selection during training.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    valid_indices = labels != -1 # Filter out potential errors from __getitem__\n",
    "    labels = labels[valid_indices]\n",
    "    logits = logits[valid_indices]\n",
    "\n",
    "    if len(labels) == 0: return {\"f1\": 0.0} # No valid labels to compute\n",
    "\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    # Calculate F1 for class 1 specifically, as requested\n",
    "    f1 = f1_score(labels, preds, average='binary', pos_label=1, zero_division=0)\n",
    "    return {\"f1\": f1} # Trainer uses this metric key\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. Threshold Optimization Function (MODIFIED: Target Class 1 F1)\n",
    "# -----------------------------------\n",
    "def find_optimal_threshold(labels, probs, target_label=1):\n",
    "    \"\"\"Finds the threshold that maximizes the F1 score for the target_label.\"\"\"\n",
    "    valid_indices = labels != -1\n",
    "    labels = labels[valid_indices]\n",
    "    probs = probs[valid_indices]\n",
    "\n",
    "    if len(labels) == 0 or len(np.unique(labels)) < 2:\n",
    "         console.print(\"[yellow]⚠️ Not enough valid data or classes for threshold optimization. Returning default 0.5.[/]\")\n",
    "         return 0.5, 0.0\n",
    "\n",
    "    # Ensure probs are for the positive class (target_label)\n",
    "    # Assuming probs are already P(class=1) as calculated later\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, probs, pos_label=target_label)\n",
    "\n",
    "    # Calculate F1 score, avoiding division by zero\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "    f1_scores = f1_scores[:-1] # Drop last value corresponding to no prediction\n",
    "    thresholds = thresholds[:len(f1_scores)] # Align thresholds with scores\n",
    "\n",
    "    f1_scores = np.nan_to_num(f1_scores) # Handle potential NaNs if precision/recall are zero\n",
    "\n",
    "    if len(f1_scores) == 0:\n",
    "         console.print(\"[yellow]⚠️ No valid F1 scores computed during threshold search. Returning default 0.5.[/]\")\n",
    "         return 0.5, 0.0\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    best_f1 = f1_scores[best_f1_idx]\n",
    "    best_thresh = thresholds[best_f1_idx]\n",
    "\n",
    "    # Sanity check against 0.5 threshold F1 for the target class\n",
    "    preds_at_05 = (probs >= 0.5).astype(int)\n",
    "    f1_at_05 = f1_score(labels, preds_at_05, pos_label=target_label, zero_division=0)\n",
    "\n",
    "    # Optionally uncomment to see comparison\n",
    "    # console.print(f\"[dim]Threshold search: Best F1={best_f1:.4f} @ Thr={best_thresh:.4f} vs F1={f1_at_05:.4f} @ Thr=0.5[/]\")\n",
    "\n",
    "    # No need to force 0.5, let the optimization decide\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Custom Trainer for Weighted Loss (CORRECTED)\n",
    "# -------------------------------\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Store weights on CPU initially, move to device in compute_loss\n",
    "        self.class_weights_cpu = class_weights.cpu() if class_weights is not None else None\n",
    "        if self.class_weights_cpu is not None:\n",
    "            console.print(f\"[cyan]Custom Trainer initialized with class weights (stored on CPU): {self.class_weights_cpu.numpy()}[/]\")\n",
    "        else:\n",
    "            console.print(\"[yellow]⚠️ Custom Trainer initialized WITHOUT class weights (will use standard CE loss).[/]\")\n",
    "\n",
    "    # Modify signature to accept **kwargs\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the loss using class weights if provided, otherwise falls back\n",
    "        to the default Trainer loss computation.\n",
    "        Accepts **kwargs to handle potential extra arguments passed by the Trainer internals.\n",
    "        \"\"\"\n",
    "        if self.class_weights_cpu is None:\n",
    "            # No class weights provided, fall back to the default Hugging Face loss.\n",
    "            # Pass along any extra kwargs received.\n",
    "            # console.print(\"[dim]Using default compute_loss (no weights).[/dim]\") # Optional debug print\n",
    "            return super().compute_loss(model, inputs, return_outputs=return_outputs, **kwargs)\n",
    "        else:\n",
    "            # Class weights are provided, compute custom weighted loss.\n",
    "            if \"labels\" not in inputs:\n",
    "                raise ValueError(\"Inputs must contain 'labels' for custom loss calculation.\")\n",
    "\n",
    "            labels = inputs.pop(\"labels\") # Remove labels from inputs to avoid passing them to the model directly if not needed\n",
    "            outputs = model(**inputs)     # Pass remaining inputs to the model\n",
    "            logits = outputs.get(\"logits\")\n",
    "\n",
    "            if logits is None:\n",
    "                 # Handle cases where the model output format might be different\n",
    "                 # If your model returns loss directly, you might need to adjust\n",
    "                 console.print(\"[yellow]⚠️ Model outputs did not contain 'logits'. Falling back to default loss calculation if possible.[/]\")\n",
    "                 # Re-add labels for the potential fallback\n",
    "                 inputs[\"labels\"] = labels\n",
    "                 return super().compute_loss(model, inputs, return_outputs=return_outputs, **kwargs)\n",
    "\n",
    "            # --- Custom Loss Calculation ---\n",
    "            # Move weights to the same device as logits just-in-time\n",
    "            class_weights_on_device = self.class_weights_cpu.to(logits.device)\n",
    "            # console.print(f\"[dim]Using weighted loss on device {logits.device} with weights {class_weights_on_device.cpu().numpy()}.[/dim]\") # Optional debug print\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=class_weights_on_device)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "            # --- End Custom Loss Calculation ---\n",
    "\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Cross-Validation Loop (on train_df ONLY) (MODIFIED PATHS)\n",
    "# -------------------------------\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "fold_results = []\n",
    "best_model_paths = [] # Store paths to best model dir (e.g., ./models/RUN_NAME/fold_X/best_model)\n",
    "\n",
    "# --- Create the main output directory for this run ---\n",
    "RUN_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, RUN_NAME)\n",
    "os.makedirs(RUN_OUTPUT_DIR, exist_ok=True)\n",
    "console.print(f\"[INFO] CV Run output will be saved under: '{RUN_OUTPUT_DIR}'\")\n",
    "# --- Create a directory for temporary checkpoints ---\n",
    "TEMP_CHECKPOINT_BASE_DIR = os.path.join(BASE_OUTPUT_DIR, f\"_cv_temp_checkpoints_{RUN_NAME}\")\n",
    "os.makedirs(TEMP_CHECKPOINT_BASE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "console.rule(\"[bold yellow]Starting Cross-Validation on Training Data[/]\")\n",
    "\n",
    "# Use train_df for splitting\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df['text'], train_df['label'])):\n",
    "    current_fold = fold + 1\n",
    "    console.rule(f\"[bold blue]CV Fold {current_fold}/{N_SPLITS}[/]\")\n",
    "\n",
    "    # --- Get fold data from train_df ---\n",
    "    train_fold_df = train_df.iloc[train_idx].copy().reset_index(drop=True)\n",
    "    fold_valid_df = train_df.iloc[val_idx].copy().reset_index(drop=True) # Validation set FOR THIS FOLD\n",
    "\n",
    "    console.print(f\"Fold Train Size: {len(train_fold_df)}, Fold Validation Size: {len(fold_valid_df)}\")\n",
    "    train_fold_labels_dist = train_fold_df['label'].value_counts(dropna=False).sort_index()\n",
    "    valid_fold_labels_dist = fold_valid_df['label'].value_counts(dropna=False).sort_index()\n",
    "    console.print(f\"Fold Train Labels:\\n{train_fold_labels_dist}\")\n",
    "    console.print(f\"Fold Validation Labels:\\n{valid_fold_labels_dist}\")\n",
    "\n",
    "    if len(train_fold_labels_dist) < 2:\n",
    "        console.print(f\"[bold red]Error: Fold {current_fold} training data only has one class after splitting. Skipping fold.[/]\")\n",
    "        continue\n",
    "\n",
    "    # --- Calculate Class Weights for this fold's training data ---\n",
    "    try:\n",
    "        n_samples = len(train_fold_df)\n",
    "        n_classes = 2\n",
    "        class_counts = train_fold_labels_dist.to_dict()\n",
    "        # Ensure both 0 and 1 counts exist, default to 1 if missing (to avoid div by zero, though split should prevent this)\n",
    "        count0 = class_counts.get(0, 1)\n",
    "        count1 = class_counts.get(1, 1)\n",
    "\n",
    "        # Inverse frequency weighting: weight = total_samples / (n_classes * count_for_class)\n",
    "        weight0 = n_samples / (n_classes * count0)\n",
    "        weight1 = n_samples / (n_classes * count1)\n",
    "\n",
    "        class_weights_tensor = torch.tensor([weight0, weight1], dtype=torch.float)\n",
    "        console.print(f\"Calculated class weights for Fold {current_fold}: {class_weights_tensor.numpy()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error calculating class weights for fold {current_fold}: {e}. Proceeding without weights.[/]\")\n",
    "        class_weights_tensor = None # Fallback\n",
    "\n",
    "    # --- Create fold datasets ---\n",
    "    try:\n",
    "        train_dataset = VaccineDataset(\n",
    "            texts=train_fold_df['text'].tolist(),\n",
    "            labels=train_fold_df['label'].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        fold_eval_dataset = VaccineDataset(\n",
    "            texts=fold_valid_df['text'].tolist(),\n",
    "            labels=fold_valid_df['label'].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        if len(train_dataset) == 0 or len(fold_eval_dataset) == 0:\n",
    "             console.print(f\"[bold red]Error: Empty dataset for fold {current_fold}. Skipping fold.[/]\")\n",
    "             continue\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error creating datasets for fold {current_fold}: {e}[/]\")\n",
    "        continue\n",
    "\n",
    "    # --- Load fresh model ---\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error loading model for fold {current_fold}: {e}[/]\")\n",
    "        continue\n",
    "\n",
    "    # --- Define Training Paths (MODIFIED STRUCTURE) ---\n",
    "    # Temporary directory for checkpoints during this fold's training\n",
    "    fold_temp_checkpoint_dir = os.path.join(TEMP_CHECKPOINT_BASE_DIR, f\"fold_{current_fold}\")\n",
    "    # Final directory structure for the *best* saved model of this fold\n",
    "    final_fold_output_basedir = os.path.join(RUN_OUTPUT_DIR, f\"fold_{current_fold}\")\n",
    "    final_best_model_dir = os.path.join(final_fold_output_basedir, \"best_model\") # <<< CHANGED STRUCTURE\n",
    "\n",
    "    # --- Training Arguments ---\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=fold_temp_checkpoint_dir,    # <<< Use temporary dir for checkpoints\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE * 2,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        disable_tqdm=False,\n",
    "        load_best_model_at_end=True,            # Crucial for getting the best model\n",
    "        metric_for_best_model=\"f1\",             # Use F1 on fold's validation set\n",
    "        greater_is_better=True,\n",
    "        logging_strategy=\"epoch\",\n",
    "        logging_steps=max(10, len(train_dataset) // (EFFECTIVE_BATCH_SIZE * 4)),\n",
    "        log_level=\"info\",\n",
    "        save_total_limit=2,                     # Limit checkpoints saved in temp dir\n",
    "        seed=SEED + fold,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=[],\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        optim=\"adamw_torch\",\n",
    "    )\n",
    "\n",
    "    # --- Trainer Setup ---\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=fold_eval_dataset,\n",
    "        compute_metrics=compute_metrics_for_trainer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "                                         early_stopping_threshold=0.001)],\n",
    "        class_weights=class_weights_tensor\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    console.print(f\"🚀 Training Fold {current_fold}...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        console.print(f\"[green]✓ Training Fold {current_fold} completed after {train_result.metrics.get('train_runtime', 0):.2f}s.[/]\")\n",
    "        best_metric_val = trainer.state.best_metric\n",
    "        if best_metric_val:\n",
    "             console.print(f\"[cyan]Fold {current_fold} - Best F1 score on internal validation set during training: {best_metric_val:.4f}[/]\")\n",
    "        else:\n",
    "             console.print(\"[yellow]Could not retrieve best metric from trainer state.[/]\")\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error during training for fold {current_fold}: {e}[/]\")\n",
    "        del model, trainer, train_dataset, fold_eval_dataset; gc.collect(); torch.cuda.empty_cache()\n",
    "        continue # Skip to next fold\n",
    "\n",
    "    # --- Evaluate on Fold's Validation Set & Find Optimal Threshold for THIS fold ---\n",
    "    console.print(f\"🔍 Evaluating and finding best threshold for Fold {current_fold} (using its internal validation split)...\")\n",
    "    try:\n",
    "        predictions_output = trainer.predict(fold_eval_dataset)\n",
    "        logits = predictions_output.predictions\n",
    "        labels = predictions_output.label_ids\n",
    "        internal_val_metrics = predictions_output.metrics\n",
    "\n",
    "        console.print(f\"Fold {current_fold} Internal Validation Metrics (at threshold 0.5): {internal_val_metrics}\")\n",
    "\n",
    "        if logits is not None and labels is not None:\n",
    "            # Calculate probabilities for the positive class (1)\n",
    "            if logits.shape[1] >= 2:\n",
    "                probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
    "            elif logits.shape[1] == 1:\n",
    "                 probs = torch.sigmoid(torch.tensor(logits)).squeeze(-1).numpy()\n",
    "            else:\n",
    "                 probs = np.array([])\n",
    "                 console.print(f\"[yellow]⚠️ Unexpected logits shape in fold {current_fold}: {logits.shape}. Cannot calculate probs.[/]\")\n",
    "\n",
    "            if probs.size > 0:\n",
    "                # Find optimal threshold based on this fold's validation split, targeting class 1\n",
    "                optimal_thresh_fold, best_f1_fold_val = find_optimal_threshold(labels, probs, target_label=1)\n",
    "\n",
    "                console.print(f\"[cyan]Fold {current_fold} - Best F1 on Internal Validation: {best_f1_fold_val:.4f} @ Optimized Threshold = {optimal_thresh_fold:.4f}[/]\")\n",
    "                fold_results.append({\n",
    "                    \"fold\": current_fold,\n",
    "                    \"best_f1_internal_val_optimized\": best_f1_fold_val,\n",
    "                    \"optimal_threshold_internal\": optimal_thresh_fold,\n",
    "                    \"f1_internal_val_0.5\": internal_val_metrics.get('test_f1', 0.0)\n",
    "                })\n",
    "\n",
    "                # --- Save Best Model for Ensemble (MODIFIED PATH & FILENAME) ---\n",
    "                try:\n",
    "                    # Create the specific final directory structure: RUN_OUTPUT_DIR/fold_X/best_model/\n",
    "                    os.makedirs(final_best_model_dir, exist_ok=True) # <<< ENSURE FINAL DIR EXISTS\n",
    "                    trainer.save_model(final_best_model_dir) # Save model files here\n",
    "                    tokenizer.save_pretrained(final_best_model_dir) # Save tokenizer here\n",
    "                    best_model_paths.append(final_best_model_dir) # Store path for later ensemble\n",
    "                    console.print(f\"[green]✓ Best model for Fold {current_fold} saved to '{final_best_model_dir}'[/]\")\n",
    "\n",
    "                    # --- Save the optimal threshold file (MODIFIED FILENAME) ---\n",
    "                    threshold_file_path = os.path.join(final_best_model_dir, \"threshold.txt\") # <<< CHANGED FILENAME\n",
    "                    with open(threshold_file_path, \"w\") as f: f.write(f\"{optimal_thresh_fold:.4f}\")\n",
    "                    console.print(f\"[green]✓ Optimal threshold ({optimal_thresh_fold:.4f}) saved to '{threshold_file_path}'[/]\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    console.print(f\"[bold red]Error saving best model or threshold for fold {current_fold}: {e}[/]\")\n",
    "                    best_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "                    if best_checkpoint_path and os.path.isdir(best_checkpoint_path):\n",
    "                         console.print(f\"[yellow]Best checkpoint was at: {best_checkpoint_path}. Consider manually copying.[/]\")\n",
    "                    else:\n",
    "                         console.print(f\"[yellow]⚠️ No best model path found for fold {current_fold}. Cannot use for ensemble.[/]\")\n",
    "\n",
    "            else:\n",
    "                 console.print(f\"[yellow]⚠️ No probabilities calculated for fold {current_fold}. Cannot optimize threshold.[/]\")\n",
    "                 # Try to save the model anyway if training completed, but without threshold\n",
    "                 try:\n",
    "                     os.makedirs(final_best_model_dir, exist_ok=True)\n",
    "                     trainer.save_model(final_best_model_dir)\n",
    "                     tokenizer.save_pretrained(final_best_model_dir)\n",
    "                     best_model_paths.append(final_best_model_dir)\n",
    "                     console.print(f\"[yellow]✓ Model saved to '{final_best_model_dir}' despite probability calculation issue (NO threshold file saved).[/]\")\n",
    "                 except Exception as e_save:\n",
    "                     console.print(f\"[bold red]Error saving model for fold {current_fold} after probability issue: {e_save}[/]\")\n",
    "\n",
    "        else:\n",
    "            console.print(f\"[yellow]⚠️ Prediction output missing logits or labels for fold {current_fold}. Cannot evaluate or save best model.[/]\")\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error during evaluation/optimization/saving for fold {current_fold}: {e}[/]\")\n",
    "        del model, trainer, train_dataset, fold_eval_dataset; gc.collect(); torch.cuda.empty_cache()\n",
    "        continue # Skip to next fold\n",
    "\n",
    "    # --- Clean Temporary Checkpoints (MODIFIED PATH) ---\n",
    "    console.print(f\"[dim]Cleaning up temporary checkpoints directory: '{fold_temp_checkpoint_dir}'[/]\")\n",
    "    try:\n",
    "        shutil.rmtree(fold_temp_checkpoint_dir) # <<< Use correct temp path\n",
    "    except OSError as e:\n",
    "        console.print(f\"[yellow]⚠️ Error deleting checkpoint directory {fold_temp_checkpoint_dir}: {e}[/]\")\n",
    "\n",
    "\n",
    "    # --- Free memory after fold completion ---\n",
    "    console.print(f\"[dim]Cleaning up memory for fold {current_fold}...[/dim]\")\n",
    "    del model, trainer, train_dataset, fold_eval_dataset, predictions_output, logits, labels, probs\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# --- End of CV Loop ---\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Display CV Summary\n",
    "# -------------------------------\n",
    "console.rule(\"[bold green]Cross-Validation Summary (on Internal Validation Splits)[/]\")\n",
    "if fold_results:\n",
    "    results_table = Table(title=\"Fold Performance on Internal Validation Splits\")\n",
    "    results_table.add_column(\"Fold\", style=\"cyan\")\n",
    "    results_table.add_column(\"F1 (Thr=0.5)\", style=\"magenta\", justify=\"right\")\n",
    "    results_table.add_column(\"Optimized F1\", style=\"green\", justify=\"right\")\n",
    "    results_table.add_column(\"Optimal Thr (Internal)\", style=\"yellow\", justify=\"right\")\n",
    "\n",
    "    all_f1s_opt = [res[\"best_f1_internal_val_optimized\"] for res in fold_results]\n",
    "    all_f1s_05 = [res[\"f1_internal_val_0.5\"] for res in fold_results]\n",
    "    all_thresholds = [res[\"optimal_threshold_internal\"] for res in fold_results]\n",
    "\n",
    "    for res in fold_results:\n",
    "        results_table.add_row(\n",
    "            str(res[\"fold\"]),\n",
    "            f\"{res['f1_internal_val_0.5']:.4f}\",\n",
    "            f\"{res['best_f1_internal_val_optimized']:.4f}\",\n",
    "            f\"{res['optimal_threshold_internal']:.4f}\"\n",
    "        )\n",
    "    console.print(results_table)\n",
    "\n",
    "    avg_f1_opt = np.mean(all_f1s_opt)\n",
    "    std_f1_opt = np.std(all_f1s_opt)\n",
    "    avg_f1_05 = np.mean(all_f1s_05)\n",
    "    std_f1_05 = np.std(all_f1s_05)\n",
    "    avg_thresh = np.mean(all_thresholds)\n",
    "    std_thresh = np.std(all_thresholds)\n",
    "\n",
    "    console.print(f\"\\nAvg F1 (Internal Val, Thr=0.5): [bold magenta]{avg_f1_05:.4f} +/- {std_f1_05:.4f}[/]\")\n",
    "    console.print(f\"Avg F1 (Internal Val, Optimized Thr): [bold green]{avg_f1_opt:.4f} +/- {std_f1_opt:.4f}[/]\")\n",
    "    console.print(f\"Avg Optimal Threshold (Internal): [bold yellow]{avg_thresh:.4f} +/- {std_thresh:.4f}[/]\")\n",
    "\n",
    "    if len(best_model_paths) != N_SPLITS:\n",
    "         console.print(f\"[yellow]⚠️ Found {len(best_model_paths)} best models, expected {N_SPLITS}. Ensemble evaluation might be affected.[/]\")\n",
    "\n",
    "else:\n",
    "    console.print(\"[yellow]No fold results recorded. Check for errors during training/evaluation.[/]\")\n",
    "\n",
    "# --- Cleanup Overall Temp Checkpoint Dir ---\n",
    "console.print(f\"[dim]Cleaning up base temporary checkpoint directory: '{TEMP_CHECKPOINT_BASE_DIR}'[/]\")\n",
    "try:\n",
    "    shutil.rmtree(TEMP_CHECKPOINT_BASE_DIR)\n",
    "except OSError as e:\n",
    "    console.print(f\"[yellow]⚠️ Error deleting base temp checkpoint directory {TEMP_CHECKPOINT_BASE_DIR}: {e}[/]\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 9. FINAL EVALUATION ON HOLDOUT VALIDATION SET (valid.csv)\n",
    "# Using Ensemble of Best Fold Models\n",
    "# Optimize Threshold DIRECTLY on Holdout Set\n",
    "# -----------------------------------------------------\n",
    "console.rule(\"[bold magenta]Final Evaluation on Holdout Set (valid.csv)[/]\")\n",
    "\n",
    "if not best_model_paths:\n",
    "    console.print(\"[bold red]❌ No best models saved from CV folds. Cannot perform final evaluation.[/]\")\n",
    "    sys.exit(1)\n",
    "if len(holdout_valid_df) == 0:\n",
    "    console.print(\"[bold red]❌ Holdout validation data ('valid.csv') is empty. Cannot perform final evaluation.[/]\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Create Dataset & Loader for Holdout Set ---\n",
    "console.print(f\"Preparing holdout validation dataset ({len(holdout_valid_df)} samples)...\")\n",
    "try:\n",
    "    # Include IDs and labels for evaluation\n",
    "    holdout_dataset = VaccineDataset(\n",
    "        texts=holdout_valid_df['text'].tolist(),\n",
    "        labels=holdout_valid_df['label'].tolist(), # Include true labels\n",
    "        ids=holdout_valid_df['id'].tolist(),       # Include IDs\n",
    "        tokenizer=tokenizer, # Use the globally loaded tokenizer\n",
    "        max_length=MAX_LENGTH,\n",
    "        is_inference=False # We have labels for evaluation\n",
    "    )\n",
    "    holdout_loader = DataLoader(holdout_dataset, batch_size=PER_DEVICE_BATCH_SIZE * 2, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "    # Get true labels in the correct order, filtering any potential -1 from dataset errors\n",
    "    holdout_y_true_raw = np.array(holdout_valid_df['label'].tolist())\n",
    "    valid_true_indices = holdout_y_true_raw != -1\n",
    "    holdout_y_true = holdout_y_true_raw[valid_true_indices]\n",
    "    console.print(f\"[dim]Using {len(holdout_y_true)} valid ground truth labels from holdout set for final evaluation.[/]\")\n",
    "\n",
    "except Exception as e:\n",
    "     console.print(f\"[bold red]❌ Error creating holdout dataset/loader: {e}. Cannot perform final evaluation.[/]\")\n",
    "     sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Ensemble Inference on Holdout Set ---\n",
    "console.print(f\"Running ensemble inference on holdout set using {len(best_model_paths)} models...\")\n",
    "all_holdout_probs_np = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Ensure device is set\n",
    "\n",
    "for i, model_p in enumerate(best_model_paths):\n",
    "    console.print(f\"--- Loading model {i+1}/{len(best_model_paths)} from [yellow]{model_p}[/] ---\") # Print full path now\n",
    "    try:\n",
    "        # Load model AND tokenizer specific to that fold (best practice)\n",
    "        # Although we saved the global tokenizer, loading from the model dir ensures consistency if needed\n",
    "        fold_tokenizer = AutoTokenizer.from_pretrained(model_p)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_p).to(device).eval()\n",
    "        # Re-create dataset/loader with fold-specific tokenizer? Could be more robust but slower.\n",
    "        # Let's stick to the global tokenizer for inference speed, assuming compatibility.\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]❌ Error loading model {model_p}: {e}. Skipping this model for ensemble.[/]\")\n",
    "        continue\n",
    "\n",
    "    fold_holdout_probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in track(holdout_loader, description=f\"Predicting (Holdout, Model {i+1})...\", console=console, transient=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            batch_labels = batch.get(\"labels\", torch.tensor([-1]*len(input_ids))) # Get labels if present\n",
    "\n",
    "            # Identify valid items in the batch (label != -1, which our Dataset uses for errors)\n",
    "            valid_batch_indices_mask = (batch_labels != -1).cpu()\n",
    "            valid_input_ids = input_ids[valid_batch_indices_mask]\n",
    "            valid_attention_mask = attention_mask[valid_batch_indices_mask]\n",
    "\n",
    "            # Skip batch if no valid items (e.g., all errored in __getitem__)\n",
    "            if valid_input_ids.shape[0] == 0:\n",
    "                 # Append placeholder NaNs for invalid items\n",
    "                 batch_probs = np.full(len(input_ids), np.nan)\n",
    "                 fold_holdout_probs_list.extend(batch_probs)\n",
    "                 continue\n",
    "\n",
    "            try:\n",
    "                outputs = model(input_ids=valid_input_ids, attention_mask=valid_attention_mask)\n",
    "                logits = outputs.logits\n",
    "                if logits.shape[1] >= 2:\n",
    "                    probs = torch.softmax(logits, dim=1)[:, 1] # Prob for class 1\n",
    "                elif logits.shape[1] == 1:\n",
    "                    probs = torch.sigmoid(logits).squeeze(-1)\n",
    "                else:\n",
    "                    probs = torch.full((valid_input_ids.shape[0],), 0.5, device=device) # Fallback guess\n",
    "\n",
    "                # Place probabilities back into the original batch structure using NaN for invalid items\n",
    "                batch_probs = np.full(len(input_ids), np.nan) # Initialize with NaNs\n",
    "                batch_probs[valid_batch_indices_mask.numpy()] = probs.cpu().numpy()\n",
    "                fold_holdout_probs_list.extend(batch_probs)\n",
    "\n",
    "\n",
    "            except Exception as pred_e:\n",
    "                console.print(f\"\\n[bold red]❌ Error during holdout prediction batch with model {i+1}: {pred_e}[/]\")\n",
    "                 # Add NaNs for the failed batch to maintain length and indicate failure\n",
    "                fold_holdout_probs_list.extend(np.full(len(input_ids), np.nan))\n",
    "\n",
    "\n",
    "    # Convert list of batch arrays to a single numpy array for the fold\n",
    "    fold_holdout_probs = np.array(fold_holdout_probs_list)\n",
    "\n",
    "    # Ensure length matches dataset size, pad with NaNs if needed (shouldn't happen with current logic)\n",
    "    if len(fold_holdout_probs) != len(holdout_dataset):\n",
    "         console.print(f\"[yellow]⚠️ Length mismatch for fold {i+1} predictions ({len(fold_holdout_probs)}) vs dataset ({len(holdout_dataset)}). Padding with NaN.[/]\")\n",
    "         fold_holdout_probs = np.pad(fold_holdout_probs, (0, len(holdout_dataset) - len(fold_holdout_probs)), constant_values=np.nan)\n",
    "\n",
    "    all_holdout_probs_np.append(fold_holdout_probs[:len(holdout_dataset)]) # Ensure correct length\n",
    "    console.print(f\"[green]✓ Holdout predictions collected for model {i+1}.[/]\")\n",
    "\n",
    "    # Free memory\n",
    "    del model, outputs, logits, probs, fold_tokenizer; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- Aggregate and Evaluate Holdout Predictions ---\n",
    "if not all_holdout_probs_np:\n",
    "    console.print(\"[bold red]❌ No predictions generated for the holdout set by any valid model.[/]\")\n",
    "else:\n",
    "    num_ensemble_models = len(all_holdout_probs_np)\n",
    "    console.print(f\"\\n[bold cyan]Combining holdout probabilities from {num_ensemble_models} models (using nanmean)...[/]\")\n",
    "    # Use nanmean to average probabilities, ignoring NaNs from failed predictions/invalid data\n",
    "    holdout_avg_probs_raw = np.nanmean(np.array(all_holdout_probs_np), axis=0)\n",
    "\n",
    "    # Filter probabilities corresponding to valid true labels\n",
    "    holdout_avg_probs = holdout_avg_probs_raw[valid_true_indices]\n",
    "\n",
    "    # Check if we have valid probabilities and labels to work with\n",
    "    if len(holdout_avg_probs) == 0 or np.all(np.isnan(holdout_avg_probs)):\n",
    "        console.print(\"[bold red]❌ No valid averaged probabilities obtained for the holdout set. Cannot optimize threshold or evaluate.[/]\")\n",
    "    elif len(holdout_y_true) == 0:\n",
    "         console.print(\"[bold red]❌ No valid ground truth labels found in the holdout set. Cannot optimize threshold or evaluate.[/]\")\n",
    "    else:\n",
    "        # --- Optimize Threshold Directly on Holdout Ensemble Probs ---\n",
    "        console.print(\"[cyan]Optimizing final threshold directly on holdout ensemble probabilities (targeting F1 for class 1)...[/]\")\n",
    "        final_holdout_threshold, best_f1_on_holdout = find_optimal_threshold(holdout_y_true, holdout_avg_probs, target_label=1)\n",
    "        console.print(f\"[bold green]✓ Optimal threshold for holdout set (Class 1 F1): {final_holdout_threshold:.4f} (yielding F1 score: {best_f1_on_holdout:.4f})[/]\")\n",
    "\n",
    "        # Apply this optimal threshold\n",
    "        holdout_predictions = (holdout_avg_probs >= final_holdout_threshold).astype(int)\n",
    "\n",
    "        # --- Final Report on Holdout Set ---\n",
    "        console.rule(\"[bold magenta]Final Ensemble Performance on Holdout Set (valid.csv) with Optimized Threshold[/]\")\n",
    "        try:\n",
    "            # We already filtered labels (holdout_y_true) and predictions (holdout_predictions)\n",
    "            console.print(f\"Evaluating on {len(holdout_y_true)} holdout samples with valid labels.\")\n",
    "            report = classification_report(holdout_y_true, holdout_predictions, output_dict=True, digits=4, zero_division=0)\n",
    "\n",
    "            # Display Report Table\n",
    "            report_table = Table(show_header=True, header_style=\"bold magenta\", box=box.SIMPLE)\n",
    "            report_table.add_column(\"Class\", style=\"dim\", width=12)\n",
    "            report_table.add_column(\"Precision\", justify=\"right\")\n",
    "            report_table.add_column(\"Recall\", justify=\"right\")\n",
    "            report_table.add_column(\"F1-Score\", justify=\"right\")\n",
    "            report_table.add_column(\"Support\", justify=\"right\")\n",
    "\n",
    "            labels_in_report = [label for label in sorted(report.keys()) if label not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "            for label in labels_in_report:\n",
    "                 metrics = report[label]\n",
    "                 style = \"green\" if label == '1' and metrics['f1-score'] > 0.96 else \"\"\n",
    "                 report_table.add_row(str(label), f\"{metrics['precision']:.4f}\", f\"{metrics['recall']:.4f}\", f\"[bold {style}]{metrics['f1-score']:.4f}[/]\", f\"{int(metrics['support'])}\")\n",
    "\n",
    "            report_table.add_section()\n",
    "            for avg_type in [\"macro avg\", \"weighted avg\"]:\n",
    "                 if avg_type in report:\n",
    "                     metrics = report[avg_type]\n",
    "                     name = avg_type.replace(\" avg\", \" Avg\")\n",
    "                     report_table.add_row(f\"[bold]{name}[/]\", f\"{metrics['precision']:.4f}\", f\"{metrics['recall']:.4f}\", f\"{metrics['f1-score']:.4f}\", f\"{int(metrics['support'])}\")\n",
    "\n",
    "            if \"accuracy\" in report:\n",
    "                 accuracy = report[\"accuracy\"]\n",
    "                 total_support = int(report[\"weighted avg\"][\"support\"]) if \"weighted avg\" in report else len(holdout_y_true)\n",
    "                 report_table.add_section()\n",
    "                 report_table.add_row(\"[bold]Accuracy[/]\", \"\", \"\", f\"[bold]{accuracy:.4f}[/]\", f\"{total_support}\")\n",
    "\n",
    "            console.print(report_table)\n",
    "\n",
    "            # Display Confusion Matrix\n",
    "            console.print(\"\\n🎯 [bold blue]Holdout Confusion Matrix[/bold blue] (using optimized threshold)\\n\")\n",
    "            cm_labels = sorted(list(set(holdout_y_true) | set(holdout_predictions)))\n",
    "            if not cm_labels: cm_labels = [0, 1] # Default if only one class predicted/present\n",
    "            elif len(cm_labels) == 1: cm_labels = [0, 1] # Ensure both 0 and 1 are columns if only one exists\n",
    "\n",
    "            cm = confusion_matrix(holdout_y_true, holdout_predictions, labels=cm_labels)\n",
    "            cm_table = Table(title=\"True \\\\ Predicted\", box=box.SIMPLE_HEAVY, show_header=True, header_style=\"bold\")\n",
    "            cm_table.add_column(\"\", justify=\"center\", style=\"dim\") # Empty top-left corner\n",
    "            for label in cm_labels:\n",
    "                cm_table.add_column(f\"Pred {label}\", justify=\"center\")\n",
    "\n",
    "            for i, true_label in enumerate(cm_labels):\n",
    "                row_data = [f\"True {true_label}\"] + [str(cm[i, j]) for j in range(len(cm_labels))]\n",
    "                cm_table.add_row(*row_data)\n",
    "            console.print(cm_table)\n",
    "\n",
    "            # Explicitly check the target F1 score\n",
    "            f1_class_1 = report.get('1', {}).get('f1-score', 0.0)\n",
    "            if f1_class_1 > 0.96:\n",
    "                 console.print(Panel(f\"🚀 [bold green]Success![/] F1 score for Class 1 ({f1_class_1:.4f}) is above the 0.96 target!\", title=\"Target Check\", expand=False))\n",
    "            else:\n",
    "                 console.print(Panel(f\"⚠️ [bold yellow]Target Not Met.[/] F1 score for Class 1 ({f1_class_1:.4f}) is below 0.96.\", title=\"Target Check\", expand=False))\n",
    "\n",
    "\n",
    "        except Exception as report_e:\n",
    "            console.print(f\"[bold red]❌ Error generating final holdout report: {report_e}[/]\")\n",
    "\n",
    "\n",
    "console.print(f\"\\n[INFO] CV completed. Best models saved under '{RUN_OUTPUT_DIR}'.\") # <-- MODIFIED Log Message\n",
    "console.print(\"[INFO] Each fold's best model is in 'fold_X/best_model/' including 'threshold.txt'.\")\n",
    "console.print(\"[INFO] Final evaluation performed on 'valid.csv' using an ensemble and a threshold optimized directly on holdout probabilities.\")\n",
    "console.print(\"[bold green]🏁 Script finished.[/]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter XLM Roberta Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">CUDA available. Using </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">GPU(</span><span style=\"color: #008080; text-decoration-color: #008080\">s</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">. Device: NVIDIA GeForce RTX </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3070</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCUDA available. Using \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m \u001b[0m\u001b[1;36mGPU\u001b[0m\u001b[1;36m(\u001b[0m\u001b[36ms\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m. Device: NVIDIA GeForce RTX \u001b[0m\u001b[1;36m3070\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Effective Batch Size: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #008080; text-decoration-color: #008080\">, Per-Device Batch Size: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #008080; text-decoration-color: #008080\">, Num GPUs: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> =&gt; Gradient Accumulation Steps: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mEffective Batch Size: \u001b[0m\u001b[1;36m8\u001b[0m\u001b[36m, Per-Device Batch Size: \u001b[0m\u001b[1;36m8\u001b[0m\u001b[36m, Num GPUs: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m => Gradient Accumulation Steps: \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Importing libraries</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mImporting libraries\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Libraries imported.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Libraries imported.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cleaning Training Data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cleaning Training Data\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training data loaded and cleaned. Total: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2521</span><span style=\"color: #008000; text-decoration-color: #008000\"> examples.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training data loaded and cleaned. Total: \u001b[0m\u001b[1;32m2521\u001b[0m\u001b[32m examples.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Training data label distribution:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1372</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1149</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Training data label distribution:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1372\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m1149\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Cleaning Holdout Validation Data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Cleaning Holdout Validation Data\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Holdout Validation data loaded and cleaned. Total: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">786</span><span style=\"color: #008000; text-decoration-color: #008000\"> examples.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Holdout Validation data loaded and cleaned. Total: \u001b[0m\u001b[1;32m786\u001b[0m\u001b[32m examples.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Holdout Validation data label distribution:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">420</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">366</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Holdout Validation data label distribution:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m420\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m366\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file sentencepiece.bpe.model from cache at None\n",
      "loading file tokenizer.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-xlm-roberta-large-2022\\snapshots\\675531e4195704e77aff71dbde57202746c182c9\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-xlm-roberta-large-2022\\snapshots\\675531e4195704e77aff71dbde57202746c182c9\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-xlm-roberta-large-2022\\snapshots\\675531e4195704e77aff71dbde57202746c182c9\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Tokenizer loaded from </span><span style=\"color: #008000; text-decoration-color: #008000\">'cardiffnlp/twitter-xlm-roberta-large-2022'</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Tokenizer loaded from \u001b[0m\u001b[32m'cardiffnlp/twitter-xlm-roberta-large-2022'\u001b[0m\u001b[32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>INFO<span style=\"font-weight: bold\">]</span> CV Run output will be saved under: <span style=\"color: #008000; text-decoration-color: #008000\">'./models\\twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mINFO\u001b[1m]\u001b[0m CV Run output will be saved under: \u001b[32m'./models\\twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────── </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Starting Cross-Validation on Training Data</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────── \u001b[0m\u001b[1;33mStarting Cross-Validation on Training Data\u001b[0m\u001b[92m ────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m1\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">505</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2016\u001b[0m, Fold Validation Size: \u001b[1;36m505\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1097</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1097\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">275</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m275\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculated class weights for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9188697</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0968444</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Calculated class weights for Fold \u001b[1;36m1\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.9188697\u001b[0m \u001b[1;36m1.0968444\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-xlm-roberta-large-2022\\snapshots\\675531e4195704e77aff71dbde57202746c182c9\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-xlm-roberta-large-2022\\snapshots\\675531e4195704e77aff71dbde57202746c182c9\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Safetensors PR exists\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Custom Trainer initialized with class weights </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">stored on CPU</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9188697</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0968444</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCustom Trainer initialized with class weights \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mstored on CPU\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m0.9188697\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m1.0968444\u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m1\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,016\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,512\n",
      "  Number of trainable parameters = 559,892,482\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1512' max='1512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1512/1512 2:04:56, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.393600</td>\n",
       "      <td>0.293952</td>\n",
       "      <td>0.922018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.196500</td>\n",
       "      <td>0.583138</td>\n",
       "      <td>0.882466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.283994</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.261795</td>\n",
       "      <td>0.951374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.264785</td>\n",
       "      <td>0.946921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.277952</td>\n",
       "      <td>0.949580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-252\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-252\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-252\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-504\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-504\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-504\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-756\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-756\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-756\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-252] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1008\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1008\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1008\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-504] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1260\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1260\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1260\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-756] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1512\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1512\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1512\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1260] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\checkpoint-1008 (score: 0.9513742071881607).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Training Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\"> completed after </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">7497.</span><span style=\"color: #008000; text-decoration-color: #008000\">50s.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Training Fold \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m completed after \u001b[0m\u001b[1;32m7497.\u001b[0m\u001b[32m50s.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 score on internal validation set during training: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9514</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m - Best F1 score on internal validation set during training: \u001b[0m\u001b[1;36m0.9514\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔍 Evaluating and finding best threshold for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> <span style=\"font-weight: bold\">(</span>using its internal validation split<span style=\"font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔍 Evaluating and finding best threshold for Fold \u001b[1;36m1\u001b[0m \u001b[1m(\u001b[0musing its internal validation split\u001b[1m)\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 505\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> Internal Validation Metrics <span style=\"font-weight: bold\">(</span>at threshold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'test_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2617945969104767</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_f1'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9513742071881607</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_runtime'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.159</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_samples_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21.806</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_steps_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.382</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold \u001b[1;36m1\u001b[0m Internal Validation Metrics \u001b[1m(\u001b[0mat threshold \u001b[1;36m0.5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'test_loss'\u001b[0m: \u001b[1;36m0.2617945969104767\u001b[0m, \u001b[32m'test_f1'\u001b[0m: \n",
       "\u001b[1;36m0.9513742071881607\u001b[0m, \u001b[32m'test_runtime'\u001b[0m: \u001b[1;36m23.159\u001b[0m, \u001b[32m'test_samples_per_second'\u001b[0m: \u001b[1;36m21.806\u001b[0m, \u001b[32m'test_steps_per_second'\u001b[0m: \u001b[1;36m1.382\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Fold </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> - Best F1 on Internal Validation: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9675</span><span style=\"color: #008080; text-decoration-color: #008080\"> @ Optimized Threshold = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9973</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFold \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m - Best F1 on Internal Validation: \u001b[0m\u001b[1;36m0.9675\u001b[0m\u001b[36m @ Optimized Threshold = \u001b[0m\u001b[1;36m0.9973\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\\twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\best_model\n",
      "Configuration saved in ./models\\twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\best_model\\config.json\n",
      "Model weights saved in ./models\\twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\best_model\\model.safetensors\n",
      "tokenizer config file saved in ./models\\twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\best_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./models\\twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\best_model\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Best model for Fold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span><span style=\"color: #008000; text-decoration-color: #008000\">'./models\\twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\best_model'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Best model for Fold \u001b[0m\u001b[1;32m1\u001b[0m\u001b[32m saved to \u001b[0m\u001b[32m'./models\\twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\best_model'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Optimal threshold </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.9973</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\"> saved to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'./models\\twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\best_model\\threshold.txt'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m✓ Optimal threshold \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m0.9973\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m saved to \u001b[0m\n",
       "\u001b[32m'./models\\twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1\\best_model\\threshold.txt'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up temporary checkpoints directory: </span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">'./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up temporary checkpoints directory: \u001b[0m\n",
       "\u001b[2;32m'./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_1'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Cleaning up memory for fold </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">1</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mCleaning up memory for fold \u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[2;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">CV Fold </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">5</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─────────────────────────────────────────────────── \u001b[0m\u001b[1;34mCV Fold \u001b[0m\u001b[1;34m2\u001b[0m\u001b[1;34m/\u001b[0m\u001b[1;34m5\u001b[0m\u001b[92m ───────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>, Fold Validation Size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">504</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Size: \u001b[1;36m2017\u001b[0m, Fold Validation Size: \u001b[1;36m504\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Train Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1098</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">919</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Train Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m1098\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m     \u001b[1;36m919\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fold Validation Labels:\n",
       "label\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">274</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "Name: count, dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Fold Validation Labels:\n",
       "label\n",
       "\u001b[1;36m0\u001b[0m    \u001b[1;36m274\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m    \u001b[1;36m230\u001b[0m\n",
       "Name: count, dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculated class weights for Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span> <span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Calculated class weights for Fold \u001b[1;36m2\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m \u001b[1;36m1.0973885\u001b[0m \u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-xlm-roberta-large-2022\\snapshots\\675531e4195704e77aff71dbde57202746c182c9\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Olivier\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-xlm-roberta-large-2022\\snapshots\\675531e4195704e77aff71dbde57202746c182c9\\pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Safetensors PR exists\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-large-2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Custom Trainer initialized with class weights </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">stored on CPU</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.91848814</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0973885</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCustom Trainer initialized with class weights \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mstored on CPU\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m[\u001b[0m\u001b[1;36m0.91848814\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m1.0973885\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Training Fold <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Training Fold \u001b[1;36m2\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,017\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,518\n",
      "  Number of trainable parameters = 559,892,482\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='828' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 828/1518 6:35:39 < 5:30:30, 0.03 it/s, Epoch 3.27/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.361118</td>\n",
       "      <td>0.914513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.277878</td>\n",
       "      <td>0.948052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.112200</td>\n",
       "      <td>0.244196</td>\n",
       "      <td>0.949451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_2\\checkpoint-253\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_2\\checkpoint-253\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_2\\checkpoint-253\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_2\\checkpoint-506\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_2\\checkpoint-506\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_2\\checkpoint-506\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 504\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_2\\checkpoint-759\n",
      "Configuration saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_2\\checkpoint-759\\config.json\n",
      "Model weights saved in ./models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_2\\checkpoint-759\\model.safetensors\n",
      "Deleting older checkpoint [models\\_cv_temp_checkpoints_twitter-xlm-roberta-large-2022_cv_5folds_ep6_bs8\\fold_2\\checkpoint-253] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "# Keep lightweight imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn # Needed for custom loss\n",
    "import os\n",
    "import shutil\n",
    "import sys # For exit on critical errors\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import gc\n",
    "# Defer heavy imports\n",
    "# from transformers import ...\n",
    "# from sklearn.metrics import ...\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration (MODIFIED)\n",
    "# -------------------------------\n",
    "SEED = 42\n",
    "MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-large-2022\"\n",
    "N_SPLITS = 5\n",
    "MAX_LENGTH = 512\n",
    "EFFECTIVE_BATCH_SIZE = 8\n",
    "PER_DEVICE_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 6\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "WEIGHT_DECAY = 0.01\n",
    "# --- NEW: Define a name for this specific CV run ---\n",
    "# This will create a subfolder in BASE_OUTPUT_DIR\n",
    "RUN_NAME = f\"{MODEL_NAME.split('/')[-1]}_cv_{N_SPLITS}folds_ep{EPOCHS}_bs{EFFECTIVE_BATCH_SIZE}\"\n",
    "\n",
    "# Output directory for CV models and results\n",
    "BASE_OUTPUT_DIR = \"./models\" # Base directory where run folders will be created\n",
    "DATA_DIR = \"./data\" # Directory containing train.csv and valid.csv\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    NUM_GPUS = torch.cuda.device_count()\n",
    "    console.print(f\"[cyan]CUDA available. Using {NUM_GPUS} GPU(s). Device: {torch.cuda.get_device_name(0)}[/]\")\n",
    "else:\n",
    "    NUM_GPUS = 1 # Assume 1 for calculation if CPU\n",
    "    console.print(\"[yellow]⚠️ CUDA not available. Training on CPU (will be very slow).[/]\")\n",
    "\n",
    "# Calculate Gradient Accumulation Steps\n",
    "GRADIENT_ACCUMULATION_STEPS = max(1, EFFECTIVE_BATCH_SIZE // (PER_DEVICE_BATCH_SIZE * NUM_GPUS))\n",
    "console.print(f\"[cyan]Effective Batch Size: {EFFECTIVE_BATCH_SIZE}, Per-Device Batch Size: {PER_DEVICE_BATCH_SIZE}, Num GPUs: {NUM_GPUS} => Gradient Accumulation Steps: {GRADIENT_ACCUMULATION_STEPS}[/]\")\n",
    "\n",
    "\n",
    "# --- Defer heavy library imports ---\n",
    "console.print(\"[dim]Importing libraries...[/]\")\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSequenceClassification,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        EarlyStoppingCallback\n",
    "    )\n",
    "    from sklearn.metrics import f1_score, precision_recall_curve, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "    from rich.panel import Panel\n",
    "    from rich import box\n",
    "    from rich.progress import track\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from transformers import TrainerCallback # Needed for custom trainer loss\n",
    "except ImportError as e:\n",
    "    console.print(f\"[bold red]Error: Missing required library -> {e}[/]\")\n",
    "    console.print(\"[yellow]Please install all necessary libraries (pandas, torch, transformers[accelerate], scikit-learn, rich, tqdm).[/]\")\n",
    "    sys.exit(1)\n",
    "console.print(\"[green]✓ Libraries imported.[/]\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Data (Separately!)\n",
    "# -------------------------------\n",
    "train_csv_path = os.path.join(DATA_DIR, \"train.csv\")\n",
    "valid_csv_path = os.path.join(DATA_DIR, \"valid.csv\")\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    holdout_valid_df = pd.read_csv(valid_csv_path) # Load original valid set separately\n",
    "\n",
    "    # --- Data Cleaning Function ---\n",
    "    def clean_dataframe(df, name):\n",
    "        console.print(f\"Cleaning {name} Data...\")\n",
    "        initial_count = len(df)\n",
    "        # Standardize column names (handle potential variations)\n",
    "        df.columns = [col.lower().strip() for col in df.columns]\n",
    "        if 'labels' in df.columns and 'label' not in df.columns:\n",
    "            df = df.rename(columns={\"labels\": \"label\"})\n",
    "\n",
    "        # Check required columns\n",
    "        required_cols = ['text', 'label']\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            missing = [col for col in required_cols if col not in df.columns]\n",
    "            raise KeyError(f\"Missing required columns in {name}: {missing}\")\n",
    "\n",
    "        # Add 'id' column if not present (using index)\n",
    "        if 'id' not in df.columns:\n",
    "             console.print(f\"[dim]Adding 'id' column based on index to {name} data.[/]\")\n",
    "             df['id'] = df.index\n",
    "\n",
    "        # Drop rows with NaNs in text or label\n",
    "        nan_rows = df['text'].isnull() | df['label'].isnull()\n",
    "        if nan_rows.any():\n",
    "            console.print(f\"[yellow]⚠️ NaNs found in {name} data. Dropping {nan_rows.sum()} rows...[/]\")\n",
    "            df = df[~nan_rows].copy()\n",
    "\n",
    "        # Ensure label is numeric and then integer\n",
    "        df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
    "        label_nan_rows = df['label'].isnull()\n",
    "        if label_nan_rows.any():\n",
    "            console.print(f\"[yellow]⚠️ Non-numeric labels found after coercion in {name}. Dropping {label_nan_rows.sum()} rows...[/]\")\n",
    "            df = df[~label_nan_rows].copy()\n",
    "\n",
    "        df['label'] = df['label'].astype(int)\n",
    "\n",
    "        # Ensure text is string\n",
    "        df['text'] = df['text'].astype(str)\n",
    "\n",
    "        cleaned_count = len(df)\n",
    "        if cleaned_count < initial_count:\n",
    "            console.print(f\"[dim]Dropped {initial_count - cleaned_count} rows from {name}.[/]\")\n",
    "\n",
    "        # Check for valid labels (0 and 1)\n",
    "        valid_labels = {0, 1}\n",
    "        if not set(df['label'].unique()).issubset(valid_labels):\n",
    "            invalid_labels = set(df['label'].unique()) - valid_labels\n",
    "            console.print(f\"[yellow]⚠️ Invalid labels found in {name}: {invalid_labels}. Keeping only 0 and 1.[/]\")\n",
    "            df = df[df['label'].isin(valid_labels)].copy()\n",
    "            if len(df) < cleaned_count:\n",
    "                 console.print(f\"[dim]Dropped {cleaned_count - len(df)} rows with invalid labels.[/]\")\n",
    "\n",
    "\n",
    "        console.print(f\"[green]✓ {name} data loaded and cleaned. Total: {len(df)} examples.[/]\")\n",
    "        # Check label distribution\n",
    "        label_counts = df['label'].value_counts()\n",
    "        console.print(f\"{name} data label distribution:\\n{label_counts}\")\n",
    "        if len(label_counts) < 2 and name == \"Training\":\n",
    "             console.print(\"[bold red]Error: Training data must contain both labels 0 and 1 for stratified splitting.[/]\")\n",
    "             sys.exit(1)\n",
    "        return df.reset_index(drop=True) # Reset index after cleaning\n",
    "\n",
    "    train_df = clean_dataframe(train_df, \"Training\")\n",
    "    holdout_valid_df = clean_dataframe(holdout_valid_df, \"Holdout Validation\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    console.print(f\"[bold red]Error: CSV file not found - {e}. Check paths '{train_csv_path}' and '{valid_csv_path}'.[/]\")\n",
    "    sys.exit(1)\n",
    "except KeyError as e:\n",
    "    console.print(f\"[bold red]Error: Missing expected column in CSV - {e}. Ensure 'text' and 'label' (or 'labels') exist.[/]\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]Unexpected error loading/cleaning data: {e}[/]\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define Dataset Class (No changes needed here, uses MAX_LENGTH from config)\n",
    "# -------------------------------\n",
    "class VaccineDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, ids=None, tokenizer=None, max_length=512, is_inference=False):\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer must be provided.\")\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.ids = ids # Store IDs if provided\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_inference = (labels is None)\n",
    "\n",
    "        if not self.is_inference and (self.labels is None or len(texts) != len(labels)):\n",
    "            raise ValueError(\"Texts and Labels must be provided and have the same length for training/evaluation.\")\n",
    "        if self.ids is not None and len(texts) != len(self.ids):\n",
    "             raise ValueError(\"Texts and IDs must have the same length if IDs are provided.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx]) if idx < len(self.texts) and self.texts[idx] is not None else \"\"\n",
    "        item_id = self.ids[idx] if self.ids is not None and idx < len(self.ids) else idx # Use index as fallback ID\n",
    "\n",
    "        try:\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "            if not self.is_inference:\n",
    "                if idx < len(self.labels):\n",
    "                    label = self.labels[idx]\n",
    "                    item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "                else:\n",
    "                     item['labels'] = torch.tensor(-1, dtype=torch.long) # Should not happen\n",
    "\n",
    "            if self.ids is not None:\n",
    "                item['id'] = item_id # Keep ID as is (numeric or string)\n",
    "\n",
    "            return item\n",
    "\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error in __getitem__ at index {idx} (ID: {item_id}): {e}[/]\")\n",
    "            dummy_item = {\n",
    "                'input_ids': torch.zeros(self.max_length, dtype=torch.long),\n",
    "                'attention_mask': torch.zeros(self.max_length, dtype=torch.long),\n",
    "            }\n",
    "            if not self.is_inference:\n",
    "                dummy_item['labels'] = torch.tensor(-1, dtype=torch.long)\n",
    "            if self.ids is not None:\n",
    "                dummy_item['id'] = item_id # Return the original ID even for dummy item\n",
    "            return dummy_item\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load Tokenizer (once)\n",
    "# -------------------------------\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    console.print(f\"[green]✓ Tokenizer loaded from '{MODEL_NAME}'.[/]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]Error loading tokenizer '{MODEL_NAME}': {e}[/]\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Metric Function (for Trainer, based on argmax)\n",
    "# -------------------------------\n",
    "def compute_metrics_for_trainer(eval_pred):\n",
    "    \"\"\"Calculates F1 based on argmax for checkpoint selection during training.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    valid_indices = labels != -1 # Filter out potential errors from __getitem__\n",
    "    labels = labels[valid_indices]\n",
    "    logits = logits[valid_indices]\n",
    "\n",
    "    if len(labels) == 0: return {\"f1\": 0.0} # No valid labels to compute\n",
    "\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    # Calculate F1 for class 1 specifically, as requested\n",
    "    f1 = f1_score(labels, preds, average='binary', pos_label=1, zero_division=0)\n",
    "    return {\"f1\": f1} # Trainer uses this metric key\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. Threshold Optimization Function (MODIFIED: Target Class 1 F1)\n",
    "# -----------------------------------\n",
    "def find_optimal_threshold(labels, probs, target_label=1):\n",
    "    \"\"\"Finds the threshold that maximizes the F1 score for the target_label.\"\"\"\n",
    "    valid_indices = labels != -1\n",
    "    labels = labels[valid_indices]\n",
    "    probs = probs[valid_indices]\n",
    "\n",
    "    if len(labels) == 0 or len(np.unique(labels)) < 2:\n",
    "         console.print(\"[yellow]⚠️ Not enough valid data or classes for threshold optimization. Returning default 0.5.[/]\")\n",
    "         return 0.5, 0.0\n",
    "\n",
    "    # Ensure probs are for the positive class (target_label)\n",
    "    # Assuming probs are already P(class=1) as calculated later\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, probs, pos_label=target_label)\n",
    "\n",
    "    # Calculate F1 score, avoiding division by zero\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "    f1_scores = f1_scores[:-1] # Drop last value corresponding to no prediction\n",
    "    thresholds = thresholds[:len(f1_scores)] # Align thresholds with scores\n",
    "\n",
    "    f1_scores = np.nan_to_num(f1_scores) # Handle potential NaNs if precision/recall are zero\n",
    "\n",
    "    if len(f1_scores) == 0:\n",
    "         console.print(\"[yellow]⚠️ No valid F1 scores computed during threshold search. Returning default 0.5.[/]\")\n",
    "         return 0.5, 0.0\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    best_f1 = f1_scores[best_f1_idx]\n",
    "    best_thresh = thresholds[best_f1_idx]\n",
    "\n",
    "    # Sanity check against 0.5 threshold F1 for the target class\n",
    "    preds_at_05 = (probs >= 0.5).astype(int)\n",
    "    f1_at_05 = f1_score(labels, preds_at_05, pos_label=target_label, zero_division=0)\n",
    "\n",
    "    # Optionally uncomment to see comparison\n",
    "    # console.print(f\"[dim]Threshold search: Best F1={best_f1:.4f} @ Thr={best_thresh:.4f} vs F1={f1_at_05:.4f} @ Thr=0.5[/]\")\n",
    "\n",
    "    # No need to force 0.5, let the optimization decide\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Custom Trainer for Weighted Loss (CORRECTED)\n",
    "# -------------------------------\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Store weights on CPU initially, move to device in compute_loss\n",
    "        self.class_weights_cpu = class_weights.cpu() if class_weights is not None else None\n",
    "        if self.class_weights_cpu is not None:\n",
    "            console.print(f\"[cyan]Custom Trainer initialized with class weights (stored on CPU): {self.class_weights_cpu.numpy()}[/]\")\n",
    "        else:\n",
    "            console.print(\"[yellow]⚠️ Custom Trainer initialized WITHOUT class weights (will use standard CE loss).[/]\")\n",
    "\n",
    "    # Modify signature to accept **kwargs\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the loss using class weights if provided, otherwise falls back\n",
    "        to the default Trainer loss computation.\n",
    "        Accepts **kwargs to handle potential extra arguments passed by the Trainer internals.\n",
    "        \"\"\"\n",
    "        if self.class_weights_cpu is None:\n",
    "            # No class weights provided, fall back to the default Hugging Face loss.\n",
    "            # Pass along any extra kwargs received.\n",
    "            # console.print(\"[dim]Using default compute_loss (no weights).[/dim]\") # Optional debug print\n",
    "            return super().compute_loss(model, inputs, return_outputs=return_outputs, **kwargs)\n",
    "        else:\n",
    "            # Class weights are provided, compute custom weighted loss.\n",
    "            if \"labels\" not in inputs:\n",
    "                raise ValueError(\"Inputs must contain 'labels' for custom loss calculation.\")\n",
    "\n",
    "            labels = inputs.pop(\"labels\") # Remove labels from inputs to avoid passing them to the model directly if not needed\n",
    "            outputs = model(**inputs)     # Pass remaining inputs to the model\n",
    "            logits = outputs.get(\"logits\")\n",
    "\n",
    "            if logits is None:\n",
    "                 # Handle cases where the model output format might be different\n",
    "                 # If your model returns loss directly, you might need to adjust\n",
    "                 console.print(\"[yellow]⚠️ Model outputs did not contain 'logits'. Falling back to default loss calculation if possible.[/]\")\n",
    "                 # Re-add labels for the potential fallback\n",
    "                 inputs[\"labels\"] = labels\n",
    "                 return super().compute_loss(model, inputs, return_outputs=return_outputs, **kwargs)\n",
    "\n",
    "            # --- Custom Loss Calculation ---\n",
    "            # Move weights to the same device as logits just-in-time\n",
    "            class_weights_on_device = self.class_weights_cpu.to(logits.device)\n",
    "            # console.print(f\"[dim]Using weighted loss on device {logits.device} with weights {class_weights_on_device.cpu().numpy()}.[/dim]\") # Optional debug print\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=class_weights_on_device)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "            # --- End Custom Loss Calculation ---\n",
    "\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Cross-Validation Loop (on train_df ONLY) (MODIFIED PATHS)\n",
    "# -------------------------------\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "fold_results = []\n",
    "best_model_paths = [] # Store paths to best model dir (e.g., ./models/RUN_NAME/fold_X/best_model)\n",
    "\n",
    "# --- Create the main output directory for this run ---\n",
    "RUN_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, RUN_NAME)\n",
    "os.makedirs(RUN_OUTPUT_DIR, exist_ok=True)\n",
    "console.print(f\"[INFO] CV Run output will be saved under: '{RUN_OUTPUT_DIR}'\")\n",
    "# --- Create a directory for temporary checkpoints ---\n",
    "TEMP_CHECKPOINT_BASE_DIR = os.path.join(BASE_OUTPUT_DIR, f\"_cv_temp_checkpoints_{RUN_NAME}\")\n",
    "os.makedirs(TEMP_CHECKPOINT_BASE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "console.rule(\"[bold yellow]Starting Cross-Validation on Training Data[/]\")\n",
    "\n",
    "# Use train_df for splitting\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df['text'], train_df['label'])):\n",
    "    current_fold = fold + 1\n",
    "    console.rule(f\"[bold blue]CV Fold {current_fold}/{N_SPLITS}[/]\")\n",
    "\n",
    "    # --- Get fold data from train_df ---\n",
    "    train_fold_df = train_df.iloc[train_idx].copy().reset_index(drop=True)\n",
    "    fold_valid_df = train_df.iloc[val_idx].copy().reset_index(drop=True) # Validation set FOR THIS FOLD\n",
    "\n",
    "    console.print(f\"Fold Train Size: {len(train_fold_df)}, Fold Validation Size: {len(fold_valid_df)}\")\n",
    "    train_fold_labels_dist = train_fold_df['label'].value_counts(dropna=False).sort_index()\n",
    "    valid_fold_labels_dist = fold_valid_df['label'].value_counts(dropna=False).sort_index()\n",
    "    console.print(f\"Fold Train Labels:\\n{train_fold_labels_dist}\")\n",
    "    console.print(f\"Fold Validation Labels:\\n{valid_fold_labels_dist}\")\n",
    "\n",
    "    if len(train_fold_labels_dist) < 2:\n",
    "        console.print(f\"[bold red]Error: Fold {current_fold} training data only has one class after splitting. Skipping fold.[/]\")\n",
    "        continue\n",
    "\n",
    "    # --- Calculate Class Weights for this fold's training data ---\n",
    "    try:\n",
    "        n_samples = len(train_fold_df)\n",
    "        n_classes = 2\n",
    "        class_counts = train_fold_labels_dist.to_dict()\n",
    "        # Ensure both 0 and 1 counts exist, default to 1 if missing (to avoid div by zero, though split should prevent this)\n",
    "        count0 = class_counts.get(0, 1)\n",
    "        count1 = class_counts.get(1, 1)\n",
    "\n",
    "        # Inverse frequency weighting: weight = total_samples / (n_classes * count_for_class)\n",
    "        weight0 = n_samples / (n_classes * count0)\n",
    "        weight1 = n_samples / (n_classes * count1)\n",
    "\n",
    "        class_weights_tensor = torch.tensor([weight0, weight1], dtype=torch.float)\n",
    "        console.print(f\"Calculated class weights for Fold {current_fold}: {class_weights_tensor.numpy()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error calculating class weights for fold {current_fold}: {e}. Proceeding without weights.[/]\")\n",
    "        class_weights_tensor = None # Fallback\n",
    "\n",
    "    # --- Create fold datasets ---\n",
    "    try:\n",
    "        train_dataset = VaccineDataset(\n",
    "            texts=train_fold_df['text'].tolist(),\n",
    "            labels=train_fold_df['label'].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        fold_eval_dataset = VaccineDataset(\n",
    "            texts=fold_valid_df['text'].tolist(),\n",
    "            labels=fold_valid_df['label'].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        if len(train_dataset) == 0 or len(fold_eval_dataset) == 0:\n",
    "             console.print(f\"[bold red]Error: Empty dataset for fold {current_fold}. Skipping fold.[/]\")\n",
    "             continue\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error creating datasets for fold {current_fold}: {e}[/]\")\n",
    "        continue\n",
    "\n",
    "    # --- Load fresh model ---\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error loading model for fold {current_fold}: {e}[/]\")\n",
    "        continue\n",
    "\n",
    "    # --- Define Training Paths (MODIFIED STRUCTURE) ---\n",
    "    # Temporary directory for checkpoints during this fold's training\n",
    "    fold_temp_checkpoint_dir = os.path.join(TEMP_CHECKPOINT_BASE_DIR, f\"fold_{current_fold}\")\n",
    "    # Final directory structure for the *best* saved model of this fold\n",
    "    final_fold_output_basedir = os.path.join(RUN_OUTPUT_DIR, f\"fold_{current_fold}\")\n",
    "    final_best_model_dir = os.path.join(final_fold_output_basedir, \"best_model\") # <<< CHANGED STRUCTURE\n",
    "\n",
    "    # --- Training Arguments ---\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=fold_temp_checkpoint_dir,    # <<< Use temporary dir for checkpoints\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE * 2,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        disable_tqdm=False,\n",
    "        load_best_model_at_end=True,            # Crucial for getting the best model\n",
    "        metric_for_best_model=\"f1\",             # Use F1 on fold's validation set\n",
    "        greater_is_better=True,\n",
    "        logging_strategy=\"epoch\",\n",
    "        logging_steps=max(10, len(train_dataset) // (EFFECTIVE_BATCH_SIZE * 4)),\n",
    "        log_level=\"info\",\n",
    "        save_total_limit=2,                     # Limit checkpoints saved in temp dir\n",
    "        seed=SEED + fold,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=[],\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        optim=\"adamw_torch\",\n",
    "    )\n",
    "\n",
    "    # --- Trainer Setup ---\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=fold_eval_dataset,\n",
    "        compute_metrics=compute_metrics_for_trainer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "                                         early_stopping_threshold=0.001)],\n",
    "        class_weights=class_weights_tensor\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    console.print(f\"🚀 Training Fold {current_fold}...\")\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        console.print(f\"[green]✓ Training Fold {current_fold} completed after {train_result.metrics.get('train_runtime', 0):.2f}s.[/]\")\n",
    "        best_metric_val = trainer.state.best_metric\n",
    "        if best_metric_val:\n",
    "             console.print(f\"[cyan]Fold {current_fold} - Best F1 score on internal validation set during training: {best_metric_val:.4f}[/]\")\n",
    "        else:\n",
    "             console.print(\"[yellow]Could not retrieve best metric from trainer state.[/]\")\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error during training for fold {current_fold}: {e}[/]\")\n",
    "        del model, trainer, train_dataset, fold_eval_dataset; gc.collect(); torch.cuda.empty_cache()\n",
    "        continue # Skip to next fold\n",
    "\n",
    "    # --- Evaluate on Fold's Validation Set & Find Optimal Threshold for THIS fold ---\n",
    "    console.print(f\"🔍 Evaluating and finding best threshold for Fold {current_fold} (using its internal validation split)...\")\n",
    "    try:\n",
    "        predictions_output = trainer.predict(fold_eval_dataset)\n",
    "        logits = predictions_output.predictions\n",
    "        labels = predictions_output.label_ids\n",
    "        internal_val_metrics = predictions_output.metrics\n",
    "\n",
    "        console.print(f\"Fold {current_fold} Internal Validation Metrics (at threshold 0.5): {internal_val_metrics}\")\n",
    "\n",
    "        if logits is not None and labels is not None:\n",
    "            # Calculate probabilities for the positive class (1)\n",
    "            if logits.shape[1] >= 2:\n",
    "                probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
    "            elif logits.shape[1] == 1:\n",
    "                 probs = torch.sigmoid(torch.tensor(logits)).squeeze(-1).numpy()\n",
    "            else:\n",
    "                 probs = np.array([])\n",
    "                 console.print(f\"[yellow]⚠️ Unexpected logits shape in fold {current_fold}: {logits.shape}. Cannot calculate probs.[/]\")\n",
    "\n",
    "            if probs.size > 0:\n",
    "                # Find optimal threshold based on this fold's validation split, targeting class 1\n",
    "                optimal_thresh_fold, best_f1_fold_val = find_optimal_threshold(labels, probs, target_label=1)\n",
    "\n",
    "                console.print(f\"[cyan]Fold {current_fold} - Best F1 on Internal Validation: {best_f1_fold_val:.4f} @ Optimized Threshold = {optimal_thresh_fold:.4f}[/]\")\n",
    "                fold_results.append({\n",
    "                    \"fold\": current_fold,\n",
    "                    \"best_f1_internal_val_optimized\": best_f1_fold_val,\n",
    "                    \"optimal_threshold_internal\": optimal_thresh_fold,\n",
    "                    \"f1_internal_val_0.5\": internal_val_metrics.get('test_f1', 0.0)\n",
    "                })\n",
    "\n",
    "                # --- Save Best Model for Ensemble (MODIFIED PATH & FILENAME) ---\n",
    "                try:\n",
    "                    # Create the specific final directory structure: RUN_OUTPUT_DIR/fold_X/best_model/\n",
    "                    os.makedirs(final_best_model_dir, exist_ok=True) # <<< ENSURE FINAL DIR EXISTS\n",
    "                    trainer.save_model(final_best_model_dir) # Save model files here\n",
    "                    tokenizer.save_pretrained(final_best_model_dir) # Save tokenizer here\n",
    "                    best_model_paths.append(final_best_model_dir) # Store path for later ensemble\n",
    "                    console.print(f\"[green]✓ Best model for Fold {current_fold} saved to '{final_best_model_dir}'[/]\")\n",
    "\n",
    "                    # --- Save the optimal threshold file (MODIFIED FILENAME) ---\n",
    "                    threshold_file_path = os.path.join(final_best_model_dir, \"threshold.txt\") # <<< CHANGED FILENAME\n",
    "                    with open(threshold_file_path, \"w\") as f: f.write(f\"{optimal_thresh_fold:.4f}\")\n",
    "                    console.print(f\"[green]✓ Optimal threshold ({optimal_thresh_fold:.4f}) saved to '{threshold_file_path}'[/]\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    console.print(f\"[bold red]Error saving best model or threshold for fold {current_fold}: {e}[/]\")\n",
    "                    best_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "                    if best_checkpoint_path and os.path.isdir(best_checkpoint_path):\n",
    "                         console.print(f\"[yellow]Best checkpoint was at: {best_checkpoint_path}. Consider manually copying.[/]\")\n",
    "                    else:\n",
    "                         console.print(f\"[yellow]⚠️ No best model path found for fold {current_fold}. Cannot use for ensemble.[/]\")\n",
    "\n",
    "            else:\n",
    "                 console.print(f\"[yellow]⚠️ No probabilities calculated for fold {current_fold}. Cannot optimize threshold.[/]\")\n",
    "                 # Try to save the model anyway if training completed, but without threshold\n",
    "                 try:\n",
    "                     os.makedirs(final_best_model_dir, exist_ok=True)\n",
    "                     trainer.save_model(final_best_model_dir)\n",
    "                     tokenizer.save_pretrained(final_best_model_dir)\n",
    "                     best_model_paths.append(final_best_model_dir)\n",
    "                     console.print(f\"[yellow]✓ Model saved to '{final_best_model_dir}' despite probability calculation issue (NO threshold file saved).[/]\")\n",
    "                 except Exception as e_save:\n",
    "                     console.print(f\"[bold red]Error saving model for fold {current_fold} after probability issue: {e_save}[/]\")\n",
    "\n",
    "        else:\n",
    "            console.print(f\"[yellow]⚠️ Prediction output missing logits or labels for fold {current_fold}. Cannot evaluate or save best model.[/]\")\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]Error during evaluation/optimization/saving for fold {current_fold}: {e}[/]\")\n",
    "        del model, trainer, train_dataset, fold_eval_dataset; gc.collect(); torch.cuda.empty_cache()\n",
    "        continue # Skip to next fold\n",
    "\n",
    "    # --- Clean Temporary Checkpoints (MODIFIED PATH) ---\n",
    "    console.print(f\"[dim]Cleaning up temporary checkpoints directory: '{fold_temp_checkpoint_dir}'[/]\")\n",
    "    try:\n",
    "        shutil.rmtree(fold_temp_checkpoint_dir) # <<< Use correct temp path\n",
    "    except OSError as e:\n",
    "        console.print(f\"[yellow]⚠️ Error deleting checkpoint directory {fold_temp_checkpoint_dir}: {e}[/]\")\n",
    "\n",
    "\n",
    "    # --- Free memory after fold completion ---\n",
    "    console.print(f\"[dim]Cleaning up memory for fold {current_fold}...[/dim]\")\n",
    "    del model, trainer, train_dataset, fold_eval_dataset, predictions_output, logits, labels, probs\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# --- End of CV Loop ---\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Display CV Summary\n",
    "# -------------------------------\n",
    "console.rule(\"[bold green]Cross-Validation Summary (on Internal Validation Splits)[/]\")\n",
    "if fold_results:\n",
    "    results_table = Table(title=\"Fold Performance on Internal Validation Splits\")\n",
    "    results_table.add_column(\"Fold\", style=\"cyan\")\n",
    "    results_table.add_column(\"F1 (Thr=0.5)\", style=\"magenta\", justify=\"right\")\n",
    "    results_table.add_column(\"Optimized F1\", style=\"green\", justify=\"right\")\n",
    "    results_table.add_column(\"Optimal Thr (Internal)\", style=\"yellow\", justify=\"right\")\n",
    "\n",
    "    all_f1s_opt = [res[\"best_f1_internal_val_optimized\"] for res in fold_results]\n",
    "    all_f1s_05 = [res[\"f1_internal_val_0.5\"] for res in fold_results]\n",
    "    all_thresholds = [res[\"optimal_threshold_internal\"] for res in fold_results]\n",
    "\n",
    "    for res in fold_results:\n",
    "        results_table.add_row(\n",
    "            str(res[\"fold\"]),\n",
    "            f\"{res['f1_internal_val_0.5']:.4f}\",\n",
    "            f\"{res['best_f1_internal_val_optimized']:.4f}\",\n",
    "            f\"{res['optimal_threshold_internal']:.4f}\"\n",
    "        )\n",
    "    console.print(results_table)\n",
    "\n",
    "    avg_f1_opt = np.mean(all_f1s_opt)\n",
    "    std_f1_opt = np.std(all_f1s_opt)\n",
    "    avg_f1_05 = np.mean(all_f1s_05)\n",
    "    std_f1_05 = np.std(all_f1s_05)\n",
    "    avg_thresh = np.mean(all_thresholds)\n",
    "    std_thresh = np.std(all_thresholds)\n",
    "\n",
    "    console.print(f\"\\nAvg F1 (Internal Val, Thr=0.5): [bold magenta]{avg_f1_05:.4f} +/- {std_f1_05:.4f}[/]\")\n",
    "    console.print(f\"Avg F1 (Internal Val, Optimized Thr): [bold green]{avg_f1_opt:.4f} +/- {std_f1_opt:.4f}[/]\")\n",
    "    console.print(f\"Avg Optimal Threshold (Internal): [bold yellow]{avg_thresh:.4f} +/- {std_thresh:.4f}[/]\")\n",
    "\n",
    "    if len(best_model_paths) != N_SPLITS:\n",
    "         console.print(f\"[yellow]⚠️ Found {len(best_model_paths)} best models, expected {N_SPLITS}. Ensemble evaluation might be affected.[/]\")\n",
    "\n",
    "else:\n",
    "    console.print(\"[yellow]No fold results recorded. Check for errors during training/evaluation.[/]\")\n",
    "\n",
    "# --- Cleanup Overall Temp Checkpoint Dir ---\n",
    "console.print(f\"[dim]Cleaning up base temporary checkpoint directory: '{TEMP_CHECKPOINT_BASE_DIR}'[/]\")\n",
    "try:\n",
    "    shutil.rmtree(TEMP_CHECKPOINT_BASE_DIR)\n",
    "except OSError as e:\n",
    "    console.print(f\"[yellow]⚠️ Error deleting base temp checkpoint directory {TEMP_CHECKPOINT_BASE_DIR}: {e}[/]\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 9. FINAL EVALUATION ON HOLDOUT VALIDATION SET (valid.csv)\n",
    "# Using Ensemble of Best Fold Models\n",
    "# Optimize Threshold DIRECTLY on Holdout Set\n",
    "# -----------------------------------------------------\n",
    "console.rule(\"[bold magenta]Final Evaluation on Holdout Set (valid.csv)[/]\")\n",
    "\n",
    "if not best_model_paths:\n",
    "    console.print(\"[bold red]❌ No best models saved from CV folds. Cannot perform final evaluation.[/]\")\n",
    "    sys.exit(1)\n",
    "if len(holdout_valid_df) == 0:\n",
    "    console.print(\"[bold red]❌ Holdout validation data ('valid.csv') is empty. Cannot perform final evaluation.[/]\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Create Dataset & Loader for Holdout Set ---\n",
    "console.print(f\"Preparing holdout validation dataset ({len(holdout_valid_df)} samples)...\")\n",
    "try:\n",
    "    # Include IDs and labels for evaluation\n",
    "    holdout_dataset = VaccineDataset(\n",
    "        texts=holdout_valid_df['text'].tolist(),\n",
    "        labels=holdout_valid_df['label'].tolist(), # Include true labels\n",
    "        ids=holdout_valid_df['id'].tolist(),       # Include IDs\n",
    "        tokenizer=tokenizer, # Use the globally loaded tokenizer\n",
    "        max_length=MAX_LENGTH,\n",
    "        is_inference=False # We have labels for evaluation\n",
    "    )\n",
    "    holdout_loader = DataLoader(holdout_dataset, batch_size=PER_DEVICE_BATCH_SIZE * 2, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "    # Get true labels in the correct order, filtering any potential -1 from dataset errors\n",
    "    holdout_y_true_raw = np.array(holdout_valid_df['label'].tolist())\n",
    "    valid_true_indices = holdout_y_true_raw != -1\n",
    "    holdout_y_true = holdout_y_true_raw[valid_true_indices]\n",
    "    console.print(f\"[dim]Using {len(holdout_y_true)} valid ground truth labels from holdout set for final evaluation.[/]\")\n",
    "\n",
    "except Exception as e:\n",
    "     console.print(f\"[bold red]❌ Error creating holdout dataset/loader: {e}. Cannot perform final evaluation.[/]\")\n",
    "     sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Ensemble Inference on Holdout Set ---\n",
    "console.print(f\"Running ensemble inference on holdout set using {len(best_model_paths)} models...\")\n",
    "all_holdout_probs_np = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Ensure device is set\n",
    "\n",
    "for i, model_p in enumerate(best_model_paths):\n",
    "    console.print(f\"--- Loading model {i+1}/{len(best_model_paths)} from [yellow]{model_p}[/] ---\") # Print full path now\n",
    "    try:\n",
    "        # Load model AND tokenizer specific to that fold (best practice)\n",
    "        # Although we saved the global tokenizer, loading from the model dir ensures consistency if needed\n",
    "        fold_tokenizer = AutoTokenizer.from_pretrained(model_p)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_p).to(device).eval()\n",
    "        # Re-create dataset/loader with fold-specific tokenizer? Could be more robust but slower.\n",
    "        # Let's stick to the global tokenizer for inference speed, assuming compatibility.\n",
    "    except Exception as e:\n",
    "        console.print(f\"[bold red]❌ Error loading model {model_p}: {e}. Skipping this model for ensemble.[/]\")\n",
    "        continue\n",
    "\n",
    "    fold_holdout_probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in track(holdout_loader, description=f\"Predicting (Holdout, Model {i+1})...\", console=console, transient=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            batch_labels = batch.get(\"labels\", torch.tensor([-1]*len(input_ids))) # Get labels if present\n",
    "\n",
    "            # Identify valid items in the batch (label != -1, which our Dataset uses for errors)\n",
    "            valid_batch_indices_mask = (batch_labels != -1).cpu()\n",
    "            valid_input_ids = input_ids[valid_batch_indices_mask]\n",
    "            valid_attention_mask = attention_mask[valid_batch_indices_mask]\n",
    "\n",
    "            # Skip batch if no valid items (e.g., all errored in __getitem__)\n",
    "            if valid_input_ids.shape[0] == 0:\n",
    "                 # Append placeholder NaNs for invalid items\n",
    "                 batch_probs = np.full(len(input_ids), np.nan)\n",
    "                 fold_holdout_probs_list.extend(batch_probs)\n",
    "                 continue\n",
    "\n",
    "            try:\n",
    "                outputs = model(input_ids=valid_input_ids, attention_mask=valid_attention_mask)\n",
    "                logits = outputs.logits\n",
    "                if logits.shape[1] >= 2:\n",
    "                    probs = torch.softmax(logits, dim=1)[:, 1] # Prob for class 1\n",
    "                elif logits.shape[1] == 1:\n",
    "                    probs = torch.sigmoid(logits).squeeze(-1)\n",
    "                else:\n",
    "                    probs = torch.full((valid_input_ids.shape[0],), 0.5, device=device) # Fallback guess\n",
    "\n",
    "                # Place probabilities back into the original batch structure using NaN for invalid items\n",
    "                batch_probs = np.full(len(input_ids), np.nan) # Initialize with NaNs\n",
    "                batch_probs[valid_batch_indices_mask.numpy()] = probs.cpu().numpy()\n",
    "                fold_holdout_probs_list.extend(batch_probs)\n",
    "\n",
    "\n",
    "            except Exception as pred_e:\n",
    "                console.print(f\"\\n[bold red]❌ Error during holdout prediction batch with model {i+1}: {pred_e}[/]\")\n",
    "                 # Add NaNs for the failed batch to maintain length and indicate failure\n",
    "                fold_holdout_probs_list.extend(np.full(len(input_ids), np.nan))\n",
    "\n",
    "\n",
    "    # Convert list of batch arrays to a single numpy array for the fold\n",
    "    fold_holdout_probs = np.array(fold_holdout_probs_list)\n",
    "\n",
    "    # Ensure length matches dataset size, pad with NaNs if needed (shouldn't happen with current logic)\n",
    "    if len(fold_holdout_probs) != len(holdout_dataset):\n",
    "         console.print(f\"[yellow]⚠️ Length mismatch for fold {i+1} predictions ({len(fold_holdout_probs)}) vs dataset ({len(holdout_dataset)}). Padding with NaN.[/]\")\n",
    "         fold_holdout_probs = np.pad(fold_holdout_probs, (0, len(holdout_dataset) - len(fold_holdout_probs)), constant_values=np.nan)\n",
    "\n",
    "    all_holdout_probs_np.append(fold_holdout_probs[:len(holdout_dataset)]) # Ensure correct length\n",
    "    console.print(f\"[green]✓ Holdout predictions collected for model {i+1}.[/]\")\n",
    "\n",
    "    # Free memory\n",
    "    del model, outputs, logits, probs, fold_tokenizer; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- Aggregate and Evaluate Holdout Predictions ---\n",
    "if not all_holdout_probs_np:\n",
    "    console.print(\"[bold red]❌ No predictions generated for the holdout set by any valid model.[/]\")\n",
    "else:\n",
    "    num_ensemble_models = len(all_holdout_probs_np)\n",
    "    console.print(f\"\\n[bold cyan]Combining holdout probabilities from {num_ensemble_models} models (using nanmean)...[/]\")\n",
    "    # Use nanmean to average probabilities, ignoring NaNs from failed predictions/invalid data\n",
    "    holdout_avg_probs_raw = np.nanmean(np.array(all_holdout_probs_np), axis=0)\n",
    "\n",
    "    # Filter probabilities corresponding to valid true labels\n",
    "    holdout_avg_probs = holdout_avg_probs_raw[valid_true_indices]\n",
    "\n",
    "    # Check if we have valid probabilities and labels to work with\n",
    "    if len(holdout_avg_probs) == 0 or np.all(np.isnan(holdout_avg_probs)):\n",
    "        console.print(\"[bold red]❌ No valid averaged probabilities obtained for the holdout set. Cannot optimize threshold or evaluate.[/]\")\n",
    "    elif len(holdout_y_true) == 0:\n",
    "         console.print(\"[bold red]❌ No valid ground truth labels found in the holdout set. Cannot optimize threshold or evaluate.[/]\")\n",
    "    else:\n",
    "        # --- Optimize Threshold Directly on Holdout Ensemble Probs ---\n",
    "        console.print(\"[cyan]Optimizing final threshold directly on holdout ensemble probabilities (targeting F1 for class 1)...[/]\")\n",
    "        final_holdout_threshold, best_f1_on_holdout = find_optimal_threshold(holdout_y_true, holdout_avg_probs, target_label=1)\n",
    "        console.print(f\"[bold green]✓ Optimal threshold for holdout set (Class 1 F1): {final_holdout_threshold:.4f} (yielding F1 score: {best_f1_on_holdout:.4f})[/]\")\n",
    "\n",
    "        # Apply this optimal threshold\n",
    "        holdout_predictions = (holdout_avg_probs >= final_holdout_threshold).astype(int)\n",
    "\n",
    "        # --- Final Report on Holdout Set ---\n",
    "        console.rule(\"[bold magenta]Final Ensemble Performance on Holdout Set (valid.csv) with Optimized Threshold[/]\")\n",
    "        try:\n",
    "            # We already filtered labels (holdout_y_true) and predictions (holdout_predictions)\n",
    "            console.print(f\"Evaluating on {len(holdout_y_true)} holdout samples with valid labels.\")\n",
    "            report = classification_report(holdout_y_true, holdout_predictions, output_dict=True, digits=4, zero_division=0)\n",
    "\n",
    "            # Display Report Table\n",
    "            report_table = Table(show_header=True, header_style=\"bold magenta\", box=box.SIMPLE)\n",
    "            report_table.add_column(\"Class\", style=\"dim\", width=12)\n",
    "            report_table.add_column(\"Precision\", justify=\"right\")\n",
    "            report_table.add_column(\"Recall\", justify=\"right\")\n",
    "            report_table.add_column(\"F1-Score\", justify=\"right\")\n",
    "            report_table.add_column(\"Support\", justify=\"right\")\n",
    "\n",
    "            labels_in_report = [label for label in sorted(report.keys()) if label not in ['accuracy', 'macro avg', 'weighted avg']]\n",
    "            for label in labels_in_report:\n",
    "                 metrics = report[label]\n",
    "                 style = \"green\" if label == '1' and metrics['f1-score'] > 0.96 else \"\"\n",
    "                 report_table.add_row(str(label), f\"{metrics['precision']:.4f}\", f\"{metrics['recall']:.4f}\", f\"[bold {style}]{metrics['f1-score']:.4f}[/]\", f\"{int(metrics['support'])}\")\n",
    "\n",
    "            report_table.add_section()\n",
    "            for avg_type in [\"macro avg\", \"weighted avg\"]:\n",
    "                 if avg_type in report:\n",
    "                     metrics = report[avg_type]\n",
    "                     name = avg_type.replace(\" avg\", \" Avg\")\n",
    "                     report_table.add_row(f\"[bold]{name}[/]\", f\"{metrics['precision']:.4f}\", f\"{metrics['recall']:.4f}\", f\"{metrics['f1-score']:.4f}\", f\"{int(metrics['support'])}\")\n",
    "\n",
    "            if \"accuracy\" in report:\n",
    "                 accuracy = report[\"accuracy\"]\n",
    "                 total_support = int(report[\"weighted avg\"][\"support\"]) if \"weighted avg\" in report else len(holdout_y_true)\n",
    "                 report_table.add_section()\n",
    "                 report_table.add_row(\"[bold]Accuracy[/]\", \"\", \"\", f\"[bold]{accuracy:.4f}[/]\", f\"{total_support}\")\n",
    "\n",
    "            console.print(report_table)\n",
    "\n",
    "            # Display Confusion Matrix\n",
    "            console.print(\"\\n🎯 [bold blue]Holdout Confusion Matrix[/bold blue] (using optimized threshold)\\n\")\n",
    "            cm_labels = sorted(list(set(holdout_y_true) | set(holdout_predictions)))\n",
    "            if not cm_labels: cm_labels = [0, 1] # Default if only one class predicted/present\n",
    "            elif len(cm_labels) == 1: cm_labels = [0, 1] # Ensure both 0 and 1 are columns if only one exists\n",
    "\n",
    "            cm = confusion_matrix(holdout_y_true, holdout_predictions, labels=cm_labels)\n",
    "            cm_table = Table(title=\"True \\\\ Predicted\", box=box.SIMPLE_HEAVY, show_header=True, header_style=\"bold\")\n",
    "            cm_table.add_column(\"\", justify=\"center\", style=\"dim\") # Empty top-left corner\n",
    "            for label in cm_labels:\n",
    "                cm_table.add_column(f\"Pred {label}\", justify=\"center\")\n",
    "\n",
    "            for i, true_label in enumerate(cm_labels):\n",
    "                row_data = [f\"True {true_label}\"] + [str(cm[i, j]) for j in range(len(cm_labels))]\n",
    "                cm_table.add_row(*row_data)\n",
    "            console.print(cm_table)\n",
    "\n",
    "            # Explicitly check the target F1 score\n",
    "            f1_class_1 = report.get('1', {}).get('f1-score', 0.0)\n",
    "            if f1_class_1 > 0.96:\n",
    "                 console.print(Panel(f\"🚀 [bold green]Success![/] F1 score for Class 1 ({f1_class_1:.4f}) is above the 0.96 target!\", title=\"Target Check\", expand=False))\n",
    "            else:\n",
    "                 console.print(Panel(f\"⚠️ [bold yellow]Target Not Met.[/] F1 score for Class 1 ({f1_class_1:.4f}) is below 0.96.\", title=\"Target Check\", expand=False))\n",
    "\n",
    "\n",
    "        except Exception as report_e:\n",
    "            console.print(f\"[bold red]❌ Error generating final holdout report: {report_e}[/]\")\n",
    "\n",
    "\n",
    "console.print(f\"\\n[INFO] CV completed. Best models saved under '{RUN_OUTPUT_DIR}'.\") # <-- MODIFIED Log Message\n",
    "console.print(\"[INFO] Each fold's best model is in 'fold_X/best_model/' including 'threshold.txt'.\")\n",
    "console.print(\"[INFO] Final evaluation performed on 'valid.csv' using an ensemble and a threshold optimized directly on holdout probabilities.\")\n",
    "console.print(\"[bold green]🏁 Script finished.[/]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
