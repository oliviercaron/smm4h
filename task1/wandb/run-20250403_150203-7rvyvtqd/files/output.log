WandB initialized.
Loading data...
Applying text cleaning (config: APPLY_TEXT_CLEANING=True)...
Data loaded. Train size: 31187, Dev size: 4624
Train labels distribution:
label
0    0.92141
1    0.07859
Name: proportion, dtype: float64
Dev labels distribution:
label
0    0.913927
1    0.086073
Name: proportion, dtype: float64
Tokenizing data with max_length = 256...
Tokenization complete.
Cleaning dataset columns...
Removing columns: ['text', 'file_name', 'origin', 'language', 'split', 'type']
Columns after cleaning and rename: ['id', 'labels', 'input_ids', 'attention_mask']
Setting up loss function...
Class Weights (for CrossEntropy): tensor([0.5426, 6.3621], device='cuda:0')
Loading model...
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Model loaded with 2 labels.
Setting training arguments...
c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
C:\Users\Olivier\AppData\Local\Temp\ipykernel_29208\2564574165.py:381: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
Trainer configured.
Starting Training...
Training finished.

Evaluating on Development Set (Best Model loaded, using argmax)...
Evaluation Results (argmax):
{'eval_loss': 0.8648844361305237, 'eval_accuracy': 0.9433391003460208, 'eval_f1_pos': 0.6675126903553299, 'eval_precision_pos': 0.6743589743589744, 'eval_recall_pos': 0.6608040201005025, 'eval_f1_neg': 0.9690307328605201, 'eval_runtime': 37.8751, 'eval_samples_per_second': 122.086, 'eval_steps_per_second': 15.261, 'epoch': 7.999358727715788}

--- Performing Threshold Tuning on Validation Set ---
Optimizing threshold for Macro F1...
Best threshold found: 0.7367 (yielding Macro F1: 0.6522 on Dev)

--- Detailed Evaluation on Development Set (Using Optimal Threshold) ---
Final Threshold Applied: 0.7367

Metrics for language: DE
  Precision-de (Pos): 0.5135
  Recall-de    (Pos): 0.5429
  F1-de        (Pos): 0.5278
  Accuracy-de:        0.9464

Metrics for language: EN
  Precision-en (Pos): 0.7500
  Recall-en    (Pos): 0.7377
  F1-en        (Pos): 0.7438
  Accuracy-en:        0.9656

Metrics for language: FR
  Precision-fr (Pos): 0.5897
  Recall-fr    (Pos): 0.7667
  F1-fr        (Pos): 0.6667
  Accuracy-fr:        0.9450

Metrics for language: RU
  Precision-ru (Pos): 0.7137
  Recall-ru    (Pos): 0.6324
  F1-ru        (Pos): 0.6706
  Accuracy-ru:        0.9367

--- Overall Evaluation Summary (Final) ---
Macro F1-score across languages (Pos Class): 0.6522  <--- *** TARGET METRIC ***
F1-score across all languages (Positive Class): 0.6684
Overall Precision (Positive Class):             0.6870
Overall Recall (Positive Class):                0.6508
Overall Accuracy across all languages:          0.9444

Overall Confusion Matrix (All Languages - Final):
[[TN=4108  FP=118]
 [FN=139  TP=259]]
Final metrics logged to WandB.

Saving predictions for submission...
Predictions saved to results_xlm_roberta_base_256_tuned\predictions_task1_tuned.csv
predictions_task1_tuned.csv has been zipped into results_xlm_roberta_base_256_tuned\submission_task1_tuned.zip

Script finished.
