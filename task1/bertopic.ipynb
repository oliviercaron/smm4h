{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36538a48",
   "metadata": {},
   "source": [
    "## on fait un BERTopic pour voir les topics et réduire ceux qui sont en trop grand nombre\n",
    "l'idée c'est de réduire l'imbalance de classe 1 et 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c72113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Distribution des classes dans les données d'entraînement:\n",
      "label\n",
      "0    28736\n",
      "1     2451\n",
      "Name: count, dtype: int64\n",
      "Pourcentage de positifs: 7.86%\n",
      "Création des embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e895483e4041d59cdad58d7fe5fbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88904b8ebfa14753a4d5af91b247382c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c975a00e4946bea6467fdaeba5fdc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d005761491554b359d587e63d7e15812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f9e76e7b7c4d93ac0914883cac9e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0532c8e77fec4e1b94b58c00e20babcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939a9eccb414480087ab12331b98f809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635b879272bd4baa9db0f7f3c658f9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3fe18cb206436ea1261b33b2ef72bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b88e7acedb4680864b29433bb2723d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf10645644c543d2aaa5943f71c1957c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4e216a948b4c3480d52322a1b68cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f619c9c63545eeb37dcb6d4786bcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:55:56,355 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration du modèle BERTopic...\n",
      "Entraînement du modèle BERTopic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 10:56:32,203 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-09 10:56:32,204 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-09 10:59:24,703 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-09 10:59:24,710 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-09 10:59:25,231 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réduction du déséquilibre des classes...\n",
      "Avant réduction: Positifs=1961, Négatifs=22988\n",
      "Ratio de réduction global: 0.2559\n",
      "Après réduction: Positifs=1961, Négatifs=21533\n",
      "Nouveau ratio négatif/positif: 10.98\n",
      "✅ Dataset équilibré sauvegardé.\n",
      "\n",
      "Exemples de textes par topic:\n",
      "\n",
      "Topic 0:\n",
      "  Exemple 1: i took my trazodone like an hour ago and i can barely keep my eyes open and i need to finish homewor...\n",
      "  Exemple 2: fuck seroquel. i need to be awake...\n",
      "\n",
      "Topic 1:\n",
      "  Exemple 1: Этот препарат рассчитан на детский возраст начиная от двух лет....\n",
      "  Exemple 2: Нам порекомендовал его на комиссии логопед-дефектолог....\n",
      "\n",
      "Topic 2:\n",
      "  Exemple 1: @USER_________ I'm nervous about Remicade because my veins are horrible. But nervous about Humira be...\n",
      "  Exemple 2: Early night for me, going to be a long day tomo with my first humira injections! Time to finally bea...\n",
      "\n",
      "Topic 3:\n",
      "  Exemple 1: Oui, tu viens certainement, pour moi aussi, les cycles ont changé. Au début, ils arrivaient tous les...\n",
      "  Exemple 2: Salut <user>, il y a un long post de <user> à ce sujet, qu'elle a compilé spécialement. Regarde le p...\n",
      "\n",
      "Préparation des données de test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05d4d0afc9a4862b1bd2842e4511028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/726 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 11:00:00,791 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\sparse\\_index.py:145: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "2025-04-09 11:00:24,620 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-09 11:00:24,621 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-04-09 11:00:26,617 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
      "2025-04-09 11:03:05,887 - BERTopic - Probabilities - Completed ✓\n",
      "2025-04-09 11:03:05,888 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Données de test avec topics sauvegardées.\n",
      "\n",
      "Terminé ! Les données équilibrées sont prêtes pour l'entraînement d'un modèle de classification.\n"
     ]
    }
   ],
   "source": [
    "# 1. Installation des dépendances nécessaires\n",
    "# pip install bertopic sentence-transformers transformers umap-learn hdbscan scikit-learn pandas numpy tqdm matplotlib\n",
    "\n",
    "# 2. Importation des bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# 3. Chargement des données\n",
    "print(\"Chargement des données...\")\n",
    "train_df = pd.read_csv(\"data/train_data_SMM4H_2025_Task_1.csv\")\n",
    "test_df = pd.read_csv(\"data/test_data_SMM4H_2025_Task_1_no_labels.csv\")\n",
    "\n",
    "# Vérification de l'équilibre des classes dans les données d'entraînement\n",
    "print(f\"Distribution des classes dans les données d'entraînement:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(f\"Pourcentage de positifs: {100 * train_df['label'].mean():.2f}%\")\n",
    "\n",
    "# 4. Division des données d'entraînement en train et validation\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=42)\n",
    "\n",
    "# 5. Création des embeddings avec SentenceTransformer (modèle multilingue)\n",
    "print(\"Création des embeddings...\")\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Fonction pour traiter les données par lots\n",
    "def create_embeddings_batch(texts, batch_size=32):\n",
    "    return model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "\n",
    "# Création des embeddings pour les ensembles d'entraînement et de validation\n",
    "train_embeddings = create_embeddings_batch(train_data['text'].tolist())\n",
    "val_embeddings = create_embeddings_batch(val_data['text'].tolist())\n",
    "\n",
    "# 6. Configuration et entraînement du modèle BERTopic\n",
    "print(\"Configuration du modèle BERTopic...\")\n",
    "# Paramètres UMAP plus adaptés aux petits datasets multilingues\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Configuration HDBSCAN pour détecter plus de clusters\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=10,\n",
    "    min_samples=5,\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# Initialisation du modèle BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=None,  # On utilise nos propres embeddings\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Entraînement du modèle BERTopic\n",
    "print(\"Entraînement du modèle BERTopic...\")\n",
    "topics, probs = topic_model.fit_transform(train_data['text'].tolist(), train_embeddings)\n",
    "\n",
    "# 7. Ajout des topics aux données d'entraînement\n",
    "train_data['topic'] = topics\n",
    "\n",
    "# 8. Stratégie de réduction du déséquilibre des classes\n",
    "print(\"Réduction du déséquilibre des classes...\")\n",
    "df_pos = train_data[train_data['label'] == 1]\n",
    "df_neg = train_data[train_data['label'] == 0]\n",
    "\n",
    "print(f\"Avant réduction: Positifs={len(df_pos)}, Négatifs={len(df_neg)}\")\n",
    "\n",
    "# Calcul du ratio cible pour un meilleur équilibrage\n",
    "# Viser un ratio de 1:3 (par exemple) au lieu de 1:100\n",
    "target_ratio = 3  # 3 négatifs pour 1 positif\n",
    "total_neg_needed = len(df_pos) * target_ratio\n",
    "\n",
    "# Détermination dynamique du pourcentage à conserver\n",
    "overall_reduction_ratio = min(1.0, total_neg_needed / len(df_neg))\n",
    "print(f\"Ratio de réduction global: {overall_reduction_ratio:.4f}\")\n",
    "\n",
    "reduced_neg = []\n",
    "for topic in df_neg['topic'].unique():\n",
    "    # Sous-ensemble des exemples négatifs pour ce topic\n",
    "    sub_df = df_neg[df_neg['topic'] == topic]\n",
    "    \n",
    "    # Pour les très petits clusters, on garde tout\n",
    "    if len(sub_df) < 5:\n",
    "        reduced_neg.append(sub_df)\n",
    "        continue\n",
    "    \n",
    "    # Récupération des embeddings pour ce topic\n",
    "    indices = sub_df.index.tolist()\n",
    "    topic_indices = [train_data.index.get_loc(idx) for idx in indices]\n",
    "    topic_embeddings = train_embeddings[topic_indices]\n",
    "    \n",
    "    # Calcul du centroïde du topic\n",
    "    centroid = np.mean(topic_embeddings, axis=0, keepdims=True)\n",
    "    \n",
    "    # Calcul des distances au centroïde\n",
    "    distances = cosine_distances(topic_embeddings, centroid).flatten()\n",
    "    \n",
    "    # Tri des indices par distance\n",
    "    sorted_indices = np.argsort(distances)\n",
    "    \n",
    "    # Ajustement du nombre d'exemples à conserver en fonction de la taille du topic\n",
    "    # Les topics plus grands contribuent plus à la réduction\n",
    "    topic_size_ratio = len(sub_df) / len(df_neg)\n",
    "    topic_reduction_ratio = overall_reduction_ratio / topic_size_ratio\n",
    "    keep_ratio = min(1.0, topic_reduction_ratio * 1.5)  # Légère préférence pour garder plus d'exemples des petits topics\n",
    "    \n",
    "    keep_n = max(1, int(keep_ratio * len(sorted_indices)))\n",
    "    \n",
    "    # Sélection des exemples les plus proches du centroïde\n",
    "    selected = sub_df.iloc[sorted_indices[:keep_n]]\n",
    "    reduced_neg.append(selected)\n",
    "\n",
    "# Combinaison de tous les exemples négatifs réduits\n",
    "df_neg_reduced = pd.concat(reduced_neg)\n",
    "\n",
    "# Création du dataset final équilibré\n",
    "df_final = pd.concat([df_pos, df_neg_reduced]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Après réduction: Positifs={len(df_pos)}, Négatifs={len(df_neg_reduced)}\")\n",
    "print(f\"Nouveau ratio négatif/positif: {len(df_neg_reduced)/len(df_pos):.2f}\")\n",
    "\n",
    "# 9. Sauvegarde du dataset équilibré\n",
    "df_final.to_csv(\"data/balanced_dataset.csv\", index=False)\n",
    "print(\"✅ Dataset équilibré sauvegardé.\")\n",
    "\n",
    "# 10. Visualisation de la distribution des topics\n",
    "plt.figure(figsize=(12, 6))\n",
    "topic_counts = train_data['topic'].value_counts()\n",
    "topic_counts = topic_counts[topic_counts.index != -1]  # Exclure les outliers (-1)\n",
    "topic_counts.sort_index().plot(kind='bar')\n",
    "plt.title('Distribution des topics')\n",
    "plt.xlabel('Topic ID')\n",
    "plt.ylabel('Nombre de documents')\n",
    "plt.savefig('topic_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# 11. Exemples de chaque topic pour vérification\n",
    "print(\"\\nExemples de textes par topic:\")\n",
    "for topic_id in sorted(train_data['topic'].unique())[:5]:  # Afficher les 5 premiers topics\n",
    "    if topic_id == -1:\n",
    "        continue  # Sauter les outliers\n",
    "    topic_texts = train_data[train_data['topic'] == topic_id]['text'].tolist()[:2]\n",
    "    print(f\"\\nTopic {topic_id}:\")\n",
    "    for i, text in enumerate(topic_texts):\n",
    "        print(f\"  Exemple {i+1}: {text[:100]}...\")  # Afficher les 100 premiers caractères\n",
    "\n",
    "# 12. Préparation des données pour le test\n",
    "print(\"\\nPréparation des données de test...\")\n",
    "test_embeddings = create_embeddings_batch(test_df['text'].tolist())\n",
    "\n",
    "# Assignation des topics aux données de test\n",
    "test_topics, test_probs = topic_model.transform(test_df['text'].tolist(), test_embeddings)\n",
    "test_df['topic'] = test_topics\n",
    "\n",
    "# Sauvegarde des données de test avec leurs topics\n",
    "test_df.to_csv(\"data/test_data_with_topics.csv\", index=False)\n",
    "print(\"✅ Données de test avec topics sauvegardées.\")\n",
    "\n",
    "print(\"\\nTerminé ! Les données équilibrées sont prêtes pour l'entraînement d'un modèle de classification.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
