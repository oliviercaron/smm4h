Loading data...
Data loaded.
Tokenizing data...
Tokenization complete.
Cleaning dataset columns...
Columns before removal: ['id', 'text', 'label', 'file_name', 'origin', 'type', 'language', 'split', 'input_ids', 'attention_mask']
Removing columns: ['text', 'id', 'file_name', 'origin', 'language', 'split', 'type']
Columns after cleaning and rename: ['labels', 'input_ids', 'attention_mask']
Computing class weights...
Class Weights: tensor([0.5426, 6.3621], device='cuda:0')
Loading model...
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Model loaded.
Dropout increased: hidden_dropout=0.3, attention_dropout=0.3
Setting training arguments...
C:\Users\Olivier\AppData\Local\Temp\ipykernel_20904\1573148678.py:170: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.
  trainer = WeightedTrainer(
Trainer configured.
Starting Training...
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
Training finished.

Evaluating on Development Set (Best Model)...
Evaluation Results:
{'eval_loss': 0.3056026101112366, 'eval_accuracy': 0.9528648648648649, 'eval_f1_pos': 0.7315270935960592, 'eval_precision_pos': 0.717391304347826, 'eval_recall_pos': 0.7462311557788944, 'eval_f1_neg': 0.9741644939559138, 'eval_runtime': 47.3617, 'eval_samples_per_second': 97.653, 'eval_steps_per_second': 48.837, 'epoch': 5.999358727715788}

Generating predictions and detailed metrics for Dev Set...

Running threshold tuning on positive class...
Baseline F1 (argmax): 0.7315
Best threshold found: 0.6000 → Tuned F1: 0.7338
Threshold saved to easy_xlmr_large_256\threshold.txt

--- Detailed Evaluation on Development Set ---

Metrics for language: DE
  Precision-de (Pos): 0.6053
  Recall-de    (Pos): 0.6571
  F1-de        (Pos): 0.6301
  Accuracy-de:        0.9574

Metrics for language: EN
  Precision-en (Pos): 0.7727
  Recall-en    (Pos): 0.8361
  F1-en        (Pos): 0.8031
  Accuracy-en:        0.9723

Metrics for language: FR
  Precision-fr (Pos): 0.7297
  Recall-fr    (Pos): 0.9000
  F1-fr        (Pos): 0.8060
  Accuracy-fr:        0.9690

Metrics for language: RU
  Precision-ru (Pos): 0.7179
  Recall-ru    (Pos): 0.7206
  F1-ru        (Pos): 0.7193
  Accuracy-ru:        0.9427

--- Overall Evaluation Summary (Positive Class Focus) ---
F1-score across all languages (Positive Class): 0.7315  <-- Primary Metric
Macro F1-score across all languages (Pos Class):0.7396
Overall Precision (Positive Class):             0.7174
Overall Recall (Positive Class):                0.7462
Overall Accuracy across all languages:          0.9529

Overall Confusion Matrix (All Languages):
[[TN=4110  FP=117]
 [FN=101  TP=297]]
Metrics logged to WandB.

Saving predictions for submission...
Predictions saved to easy_xlmr_large_256\predictions_task1.csv
predictions_task1.csv has been zipped into easy_xlmr_large_256\submission_task1.zip

Script finished.

Nombre d'erreurs en allemand : 27
          id                                               text  label  \
52    de_241  Hallo <user>, Ich bin mit 1 Kapsel( 1000 mg) v...      1
332    de_17  Bin schon wieder zurück! Ich habe gebackene Ca...      0
485   de_540  Guten Morgen liebe <user> Ich bin jetzt 48 in ...      0
705   de_537  Hi meine Lieben.. ich bin jetzt <pi> und hatte...      1
730   de_462  <user> Hallo, an alle, habe jetzt wieder Cipra...      1
1047  de_566  Liebe <user>, danke für deine Nachricht. nun i...      1
1330  de_124  Liebe <user>, Danke für deine Antwort und Vers...      1
1416  de_450  Hallo <doc>,ich mache keine systemischer Hormo...      1
1445  de_102  Vor einiger Zeit erschien im Focus-online ein ...      0
1765  de_220  Hallo <user> Es freut mich dass es dir in der ...      0

      predicted_label
52                  0
332                 1
485                 1
705                 0
730                 0
1047                0
1330                0
1416                0
1445                1
1765                1

Nombre d'erreurs en allemand : 27
          id                                               text  label  \
52    de_241  Hallo <user>, Ich bin mit 1 Kapsel( 1000 mg) v...      1
332    de_17  Bin schon wieder zurück! Ich habe gebackene Ca...      0
485   de_540  Guten Morgen liebe <user> Ich bin jetzt 48 in ...      0
705   de_537  Hi meine Lieben.. ich bin jetzt <pi> und hatte...      1
730   de_462  <user> Hallo, an alle, habe jetzt wieder Cipra...      1
1047  de_566  Liebe <user>, danke für deine Nachricht. nun i...      1
1330  de_124  Liebe <user>, Danke für deine Antwort und Vers...      1
1416  de_450  Hallo <doc>,ich mache keine systemischer Hormo...      1
1445  de_102  Vor einiger Zeit erschien im Focus-online ein ...      0
1765  de_220  Hallo <user> Es freut mich dass es dir in der ...      0

      predicted_label
52                  0
332                 1
485                 1
705                 0
730                 0
1047                0
1330                0
1416                0
1445                1
1765                1
Loading training and development data...
Training data: 1482 samples
Development data: 634 samples
Class distribution in training data:
label
0    0.943995
1    0.056005
Name: proportion, dtype: float64

Class distribution in development data:
label
0    0.944795
1    0.055205
Name: proportion, dtype: float64

Vectorizing text using TF-IDF...
TF-IDF vectorization complete.

Applying RandomOverSampler to balance training classes...
Oversampling complete. New training samples: 2798
Class distribution after oversampling:
label
0    0.5
1    0.5
Name: proportion, dtype: float64

Training Random Forest classifier...
Training complete.

Predicting probabilities on development data...
Predictions made using threshold 0.3.

----- Detailed Evaluation on Development Set -----

Metrics for language: DE
  Precision-de: 0.5238
  Recall-de:    0.3143
  F1-de:        0.3929
  Accuracy-de:  0.9464

----- Overall Evaluation Summary -----
F1-score across all languages (Positive Class): 0.3929  <-- Primary Metric
Macro F1-score across all languages:            0.3929
Overall Accuracy across all languages:          0.9464

Overall Confusion Matrix (All Languages):
[[TN=589  FP=10]
 [FN=24  TP=11]]
Loading training and development data...
Training data: 31187 samples
Development data: 4625 samples
Class distribution in training data:
0    0.92141
1    0.07859
Name: proportion, dtype: float64

Class distribution in development data:
0    0.913946
1    0.086054
Name: proportion, dtype: float64
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\Olivier\AppData\Local\Temp\ipykernel_20904\750989214.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\Olivier\AppData\Local\Temp\ipykernel_20904\1940634954.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.
  trainer = WeightedTrainer(
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\Olivier\AppData\Local\Temp\ipykernel_20904\1940634954.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.
  trainer = WeightedTrainer(
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\Olivier\AppData\Local\Temp\ipykernel_20904\530991367.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.
  trainer = WeightedTrainer(
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\Olivier\AppData\Local\Temp\ipykernel_20904\1758150867.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.
  trainer = WeightedTrainer(
