Loading data...
Cleaning text data (replacing patterns with '')...
Data loaded. Train size: 31186, Dev size: 4624
Tokenizing data with max_length = 128...
Tokenization complete.
Cleaning dataset columns...
Removing columns: ['text', 'file_name', 'origin', 'language', 'split', 'type', '__index_level_0__']
Columns after cleaning and rename: ['id', 'labels', 'input_ids', 'attention_mask']
Computing class weights...
Class Weights (for CrossEntropy): tensor([0.5426, 6.3619], device='cuda:0')
Focal Loss Alpha (derived): tensor([0.5426, 6.3619], device='cuda:0')
Loading model...
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Model loaded with 2 labels.
Setting training arguments...
c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
C:\Users\Olivier\AppData\Local\Temp\ipykernel_24412\2686947070.py:252: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
Trainer configured.
Starting Training...
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
Loading data...
Cleaning text data (replacing patterns with '')...
Loading data...
Cleaning text data (replacing patterns with '')...
Data loaded. Train size: 31177, Dev size: 4624
Tokenizing data with max_length = 128...
c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Tokenization complete.
Cleaning dataset columns...
Removing columns: ['text', 'file_name', 'origin', 'language', 'split', 'type', '__index_level_0__']
Columns after cleaning and rename: ['id', 'labels', 'input_ids', 'token_type_ids', 'attention_mask']
Computing class weights...
Class Weights (for CrossEntropy): tensor([0.5426, 6.3627], device='cuda:0')
Focal Loss Alpha (derived): tensor([0.5426, 6.3627], device='cuda:0')
Loading model...
Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Model loaded with 2 labels.
Setting training arguments...
c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
C:\Users\Olivier\AppData\Local\Temp\ipykernel_24412\2914803416.py:277: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
Trainer configured.
Starting Training...
Training finished.

Evaluating on Development Set (Best Model with argmax)...
Evaluation Results (using argmax):
{'eval_loss': 0.3287891149520874, 'eval_accuracy': 0.9346885813148789, 'eval_f1_pos': 0.616751269035533, 'eval_precision_pos': 0.6230769230769231, 'eval_recall_pos': 0.6105527638190955, 'eval_f1_neg': 0.9643026004728132, 'eval_runtime': 10.8724, 'eval_samples_per_second': 425.296, 'eval_steps_per_second': 26.581, 'epoch': 6.999486916367368}

--- Performing Threshold Tuning on Validation Set ---
Best threshold found: 0.5266 (yielding F1-score: 0.6178)

--- Detailed Evaluation on Development Set (Using Optimal Threshold) ---

Metrics for language: DE (Threshold: 0.5266)
  Precision-de (Pos): 0.5667
  Recall-de    (Pos): 0.4857
  F1-de        (Pos): 0.5231
  Accuracy-de:        0.9511

Metrics for language: EN (Threshold: 0.5266)
  Precision-en (Pos): 0.6721
  Recall-en    (Pos): 0.6721
  F1-en        (Pos): 0.6721
  Accuracy-en:        0.9557

Metrics for language: FR (Threshold: 0.5266)
  Precision-fr (Pos): 0.5385
  Recall-fr    (Pos): 0.4667
  F1-fr        (Pos): 0.5000
  Accuracy-fr:        0.9330

Metrics for language: RU (Threshold: 0.5266)
  Precision-ru (Pos): 0.6412
  Recall-ru    (Pos): 0.6176
  F1-ru        (Pos): 0.6292
  Accuracy-ru:        0.9258

--- Overall Evaluation Summary (Using Optimal Threshold) ---
Optimal Threshold Applied: 0.5266
F1-score across all languages (Positive Class): 0.6178  <-- Primary Metric
Macro F1-score across all languages (Pos Class):0.5811
Overall Precision (Positive Class):             0.6332
Overall Recall (Positive Class):                0.6030
Overall Accuracy across all languages:          0.9358

Overall Confusion Matrix (All Languages - Tuned):
[[TN=4087  FP=139]
 [FN=158  TP=240]]
Tuned metrics logged to WandB.

Saving predictions for submission (using optimal threshold)...
Predictions saved to results_mdeberta_focal_thresh_tuning_opt\predictions_task1_tuned.csv
predictions_task1_tuned.csv has been zipped into results_mdeberta_focal_thresh_tuning_opt\submission_task1_tuned.zip

Script finished.
