
An error occurred during training: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['labels']
--------------------- TRACEBACK ---------------------
Traceback (most recent call last):
  File "C:\Users\Olivier\AppData\Local\Temp\ipykernel_24720\1857661549.py", line 347, in <module>
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\trainer.py", line 2514, in _inner_training_loop
    batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\trainer.py", line 5243, in get_batch_samples
    batch_samples.append(next(epoch_iterator))
                         ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\accelerate\data_loader.py", line 566, in __iter__
    current_batch = next(dataloader_iter)
                    ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\data\dataloader.py", line 708, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\data\dataloader.py", line 764, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\data\_utils\fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\data\data_collator.py", line 272, in __call__
    batch = pad_without_fast_tokenizer_warning(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\data\data_collator.py", line 67, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\tokenization_utils_base.py", line 3324, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['labels']
---------------------------------------------------

Skipping evaluation because training did not complete successfully.

Skipping detailed evaluation because training did not complete successfully.

Saving final predictions for submission...

Cleaning up resources...
Loading data...
Loaded 31187 train and 4625 dev examples.
Data loaded.
ACTION REQUIRED: Please ensure you have cleared the Hugging Face datasets cache (e.g., ~/.cache/huggingface/datasets) before this run if you suspect caching issues.
Tokenizing data...
Using 6 processes for tokenization...
Tokenization complete.
Loading main tokenizer 'xlm-roberta-large' for the Trainer...
Cleaning dataset columns...
Columns after cleaning and rename: ['labels']
Computing class weights and determining label info...
Model will be configured with num_labels=2
Class Weights (for classes [0, 1]): [0.5426468 6.3620973]

Setting up QLoRA configuration...
Loading model 'xlm-roberta-large' (num_labels=2)...
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Preparing model for k-bit training...

Inspecting/casting classifier...
Casting classifier to float32...
Inspecting after cast:
  dense.weight                   | torch.float32
  dense.bias                     | torch.float32
  out_proj.weight                | torch.float32
  out_proj.bias                  | torch.float32

Setting up LoRA configuration...
Applying LoRA adapter...
LoRA applied successfully.
trainable params: 2,624,514 || all params: 562,516,996 || trainable%: 0.4666

Verifying dataset columns before Trainer initialization...
Train dataset columns: ['labels']
Validation dataset columns: ['labels']
ERROR: Training dataset is MISSING required columns! Expected: ['input_ids', 'attention_mask', 'labels']
