Loading data...
Data loaded.
Tokenizing data...
Tokenization complete.
Cleaning dataset columns...
Columns before removal: ['id', 'text', 'label', 'file_name', 'origin', 'type', 'language', 'split', 'input_ids', 'attention_mask']
Removing columns: ['text', 'id', 'file_name', 'origin', 'language', 'split', 'type']
Columns after cleaning and rename: ['labels', 'input_ids', 'attention_mask']
Computing class weights...
Class Weights: tensor([0.5426, 6.3621], device='cuda:0')
Loading model...
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Model loaded.
Setting training arguments...
c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
C:\Users\Olivier\AppData\Local\Temp\ipykernel_24768\3082492132.py:140: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.
  trainer = WeightedTrainer(
Trainer configured.
Starting Training...
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
Training finished.

Evaluating on Development Set (Best Model)...
Evaluation Results:
{'eval_loss': 0.8237790465354919, 'eval_accuracy': 0.9446486486486486, 'eval_f1_pos': 0.6717948717948717, 'eval_precision_pos': 0.6858638743455497, 'eval_recall_pos': 0.6582914572864321, 'eval_f1_neg': 0.969775678866588, 'eval_runtime': 10.571, 'eval_samples_per_second': 437.518, 'eval_steps_per_second': 27.434, 'epoch': 7.992049243395742}

Generating predictions and detailed metrics for Dev Set...

--- Detailed Evaluation on Development Set ---

Metrics for language: DE
  Precision-de (Pos): 0.6250
  Recall-de    (Pos): 0.4286
  F1-de        (Pos): 0.5085
  Accuracy-de:        0.9543

Metrics for language: EN
  Precision-en (Pos): 0.7424
  Recall-en    (Pos): 0.8033
  F1-en        (Pos): 0.7717
  Accuracy-en:        0.9678

Metrics for language: FR
  Precision-fr (Pos): 0.4878
  Recall-fr    (Pos): 0.6667
  F1-fr        (Pos): 0.5634
  Accuracy-fr:        0.9260

Metrics for language: RU
  Precision-ru (Pos): 0.7092
  Recall-ru    (Pos): 0.6544
  F1-ru        (Pos): 0.6807
  Accuracy-ru:        0.9375

--- Overall Evaluation Summary (Positive Class Focus) ---
F1-score across all languages (Positive Class): 0.6718  <-- Primary Metric
Macro F1-score across all languages (Pos Class):0.6310
Overall Precision (Positive Class):             0.6859
Overall Recall (Positive Class):                0.6583
Overall Accuracy across all languages:          0.9446

Overall Confusion Matrix (All Languages):
[[TN=4107  FP=120]
 [FN=136  TP=262]]
Metrics logged to WandB.

Saving predictions for submission...
Predictions saved to results_xlmr_longer_train\predictions_task1.csv
predictions_task1.csv has been zipped into results_xlmr_longer_train\submission_task1.zip

Script finished.
Loading training and development data...
Training data: 31187 samples
Development data: 4625 samples
Class distribution in training data:
label
0    0.92141
1    0.07859
Name: proportion, dtype: float64

Class distribution in development data:
label
0    0.913946
1    0.086054
Name: proportion, dtype: float64

Vectorizing text using TF-IDF...
TF-IDF vectorization complete.

Applying RandomOverSampler to balance training classes...
Oversampling complete. New training samples: 57472
Class distribution after oversampling:
label
1    0.5
0    0.5
Name: proportion, dtype: float64

Training Random Forest classifier...
Loading training and development data...
Training data: 31187 samples
Development data: 4625 samples
Class distribution in training data:
0    0.92141
1    0.07859
Name: proportion, dtype: float64

Class distribution in development data:
0    0.913946
1    0.086054
Name: proportion, dtype: float64

Loading E5 model: intfloat/multilingual-e5-large...
Using device: cuda
Preparing texts with 'passage: ' prefix for E5...
Generating E5 embeddings for training data...
