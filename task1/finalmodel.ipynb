{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mdeberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "Tokenizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83501aca827d44449c0f312df42a2455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061ce433d80249bdbd76ba2c1db4c982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n",
      "Cleaning dataset columns...\n",
      "Columns before removal: ['id', 'text', 'label', 'file_name', 'origin', 'type', 'language', 'split', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Removing columns: ['text', 'id', 'file_name', 'origin', 'language', 'split', 'type']\n",
      "Columns after cleaning and rename: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Computing class weights...\n",
      "Class Weights: tensor([0.5426, 6.3621], device='cuda:0')\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Dropout increased: hidden_dropout=0.3, attention_dropout=0.3\n",
      "Setting training arguments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_21464\\754797739.py:170: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer configured.\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5844' max='5844' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5844/5844 2:37:08, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Pos</th>\n",
       "      <th>Precision Pos</th>\n",
       "      <th>Recall Pos</th>\n",
       "      <th>F1 Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.246800</td>\n",
       "      <td>0.258236</td>\n",
       "      <td>0.928432</td>\n",
       "      <td>0.561589</td>\n",
       "      <td>0.593838</td>\n",
       "      <td>0.532663</td>\n",
       "      <td>0.961036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.217694</td>\n",
       "      <td>0.932757</td>\n",
       "      <td>0.633687</td>\n",
       "      <td>0.596452</td>\n",
       "      <td>0.675879</td>\n",
       "      <td>0.962981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.133400</td>\n",
       "      <td>0.221850</td>\n",
       "      <td>0.931892</td>\n",
       "      <td>0.654226</td>\n",
       "      <td>0.580897</td>\n",
       "      <td>0.748744</td>\n",
       "      <td>0.962226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.252743</td>\n",
       "      <td>0.937081</td>\n",
       "      <td>0.661234</td>\n",
       "      <td>0.616052</td>\n",
       "      <td>0.713568</td>\n",
       "      <td>0.965320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.255575</td>\n",
       "      <td>0.935351</td>\n",
       "      <td>0.662147</td>\n",
       "      <td>0.601643</td>\n",
       "      <td>0.736181</td>\n",
       "      <td>0.964256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>0.256488</td>\n",
       "      <td>0.932973</td>\n",
       "      <td>0.654788</td>\n",
       "      <td>0.588000</td>\n",
       "      <td>0.738693</td>\n",
       "      <td>0.962883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "\n",
      "Evaluating on Development Set (Best Model)...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "{'eval_loss': 0.25557541847229004, 'eval_accuracy': 0.9353513513513514, 'eval_f1_pos': 0.6621468926553672, 'eval_precision_pos': 0.6016427104722792, 'eval_recall_pos': 0.7361809045226131, 'eval_f1_neg': 0.9642558278541542, 'eval_runtime': 51.7866, 'eval_samples_per_second': 89.309, 'eval_steps_per_second': 44.664, 'epoch': 5.999358727715788}\n",
      "\n",
      "Generating predictions and detailed metrics for Dev Set...\n",
      "\n",
      "Running threshold tuning on positive class...\n",
      "Baseline F1 (argmax): 0.6621\n",
      "Best threshold found: 0.6300 → Tuned F1: 0.6713\n",
      "Threshold saved to results_deberta-v3-large\\threshold.txt\n",
      "\n",
      "--- Detailed Evaluation on Development Set ---\n",
      "\n",
      "Metrics for language: DE\n",
      "  Precision-de (Pos): 0.6216\n",
      "  Recall-de    (Pos): 0.6571\n",
      "  F1-de        (Pos): 0.6389\n",
      "  Accuracy-de:        0.9590\n",
      "\n",
      "Metrics for language: EN\n",
      "  Precision-en (Pos): 0.6220\n",
      "  Recall-en    (Pos): 0.8361\n",
      "  F1-en        (Pos): 0.7133\n",
      "  Accuracy-en:        0.9545\n",
      "\n",
      "Metrics for language: FR\n",
      "  Precision-fr (Pos): 0.5750\n",
      "  Recall-fr    (Pos): 0.7667\n",
      "  F1-fr        (Pos): 0.6571\n",
      "  Accuracy-fr:        0.9427\n",
      "\n",
      "Metrics for language: RU\n",
      "  Precision-ru (Pos): 0.5976\n",
      "  Recall-ru    (Pos): 0.7206\n",
      "  F1-ru        (Pos): 0.6533\n",
      "  Accuracy-ru:        0.9221\n",
      "\n",
      "--- Overall Evaluation Summary (Positive Class Focus) ---\n",
      "F1-score across all languages (Positive Class): 0.6621  <-- Primary Metric\n",
      "Macro F1-score across all languages (Pos Class):0.6657\n",
      "Overall Precision (Positive Class):             0.6016\n",
      "Overall Recall (Positive Class):                0.7362\n",
      "Overall Accuracy across all languages:          0.9354\n",
      "\n",
      "Overall Confusion Matrix (All Languages):\n",
      "[[TN=4033  FP=194]\n",
      " [FN=105  TP=293]]\n",
      "Metrics logged to WandB.\n",
      "\n",
      "Saving predictions for submission...\n",
      "Predictions saved to results_deberta-v3-large\\predictions_task1.csv\n",
      "predictions_task1.csv has been zipped into results_deberta-v3-large\\submission_task1.zip\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback # Importer le callback pour l'arrêt précoce\n",
    ")\n",
    "# Assurez-vous que wandb est initialisé si vous l'utilisez, sinon commentez/supprimez les lignes wandb.log\n",
    "import wandb\n",
    "wandb.init(project=\"ade-classification-mdeberta\") # Exemple d'initialisation\n",
    "\n",
    "# CORRECTION ICI: Ajout de precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "import zipfile # Pour la sauvegarde finale\n",
    "import os # Pour la sauvegarde finale\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"microsoft/mdeberta-v3-base\"  # Passer à la version Large\n",
    "TRAIN_CSV = \"data/train_data_SMM4H_2025_Task_1.csv\" # Utiliser le CSV d'entraînement augmenté\n",
    "DEV_CSV = \"data/dev_data_SMM4H_2025_Task_1.csv\"\n",
    "OUTPUT_DIR = \"results_deberta-v3-large\"  # Répertoire de sortie pour le modèle et les résultats\n",
    "NUM_EPOCHS = 6  # Réduire légèrement les epochs\n",
    "BATCH_SIZE = 2  # Plus petit batch size\n",
    "LEARNING_RATE = 1e-5  # Learning rate plus bas pour le large\n",
    "GRADIENT_ACCUMULATION_STEPS = 16  # Accumulation plus importante\n",
    "EARLY_STOPPING_PATIENCE = 1  # Arrêt plus rapide si stagnation\n",
    "MAX_LENGTH = 256  # Réduire la longueur maximale des séquences\n",
    "WEIGHT_DECAY = 0.05  # Poids de régularisation\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_CSV).dropna(subset=['text'])\n",
    "    dev_df = pd.read_csv(DEV_CSV).dropna(subset=['text'])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading CSV files: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "dataset_dict = DatasetDict({'train': train_dataset, 'validation': dev_dataset})\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "# --- 2. Tokenization ---\n",
    "print(\"Tokenizing data...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- Modifications de la fonction de tokenization ---\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True  # Pour éviter de dépasser les limites du modèle\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# --- Clean up columns ---\n",
    "print(\"Cleaning dataset columns...\")\n",
    "print(\"Columns before removal:\", tokenized_datasets['train'].column_names)\n",
    "columns_to_remove = [\"text\", \"id\", \"file_name\", \"origin\", \"language\", \"split\", \"type\"]\n",
    "actual_columns_to_remove = [col for col in columns_to_remove if col in tokenized_datasets['train'].column_names]\n",
    "print(\"Removing columns:\", actual_columns_to_remove)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(actual_columns_to_remove)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Rename 'label' to 'labels'\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "print(\"Columns after cleaning and rename:\", tokenized_datasets['train'].column_names)\n",
    "\n",
    "# --- 3. Compute Class Weights ---\n",
    "print(\"Computing class weights...\")\n",
    "labels_train = train_df['label'].values\n",
    "if len(np.unique(labels_train)) > 1: # Ensure there are at least two classes\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels_train), y=labels_train)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Class Weights: {class_weights_tensor}\")\n",
    "else:\n",
    "    print(\"Warning: Only one class found in training data. Cannot compute class weights.\")\n",
    "    class_weights_tensor = None # Handle this case in the loss function if needed\n",
    "\n",
    "# --- Custom Trainer for Weighted Loss ---\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Use weights only if they were computed\n",
    "        if class_weights_tensor is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss() # Default unweighted loss\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --- 4. Model & Metrics ---\n",
    "print(\"Loading model...\")\n",
    "# --- Chargement du modèle avec optimisations ---\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# --- Increase dropout to help regularization ---\n",
    "model.config.hidden_dropout_prob = 0.3  # Dropout on fully connected layers\n",
    "model.config.attention_probs_dropout_prob = 0.3  # Dropout on attention probabilities\n",
    "print(\"Dropout increased: hidden_dropout=0.3, attention_dropout=0.3\")\n",
    "\n",
    "# Activer le gradient checkpointing pour économiser la mémoire\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Fonction compute_metrics qui utilise la fonction importée\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Utilisation de precision_recall_fscore_support\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None, labels=[0, 1], zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_pos': f1[1], # F1 for class 1\n",
    "        'precision_pos': precision[1],\n",
    "        'recall_pos': recall[1],\n",
    "        'f1_neg': f1[0], # F1 for class 0 (for info)\n",
    "    }\n",
    "\n",
    "# --- 5. Training Arguments ---\n",
    "print(\"Setting training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # Modifié (était BATCH_SIZE*2)\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    lr_scheduler_type=\"cosine\",  # <-- Ajouté\n",
    "    warmup_ratio=0.1,            # <-- Ajouté (10% des steps en warmup)\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_pos\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    report_to=\"wandb\" if \"wandb\" in locals() else \"none\",\n",
    "    fp16=False,  # Disabled FP16 to avoid gradient unscaling issues\n",
    "    bf16=True if torch.cuda.is_available() else False,  # Use BF16 instead if available\n",
    "    optim=\"adafactor\",  # Ajouté - Optimiseur plus léger\n",
    "    gradient_checkpointing=True,  # Ajouté - Économie de mémoire\n",
    "    #torch_compile=False,  # Ajouté - Acceleration PyTorch 2.x+\n",
    "    save_total_limit=2\n",
    "    # early_stopping_patience=EARLY_STOPPING_PATIENCE,  # Déjà géré par le callback\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# --- 6. Trainer ---\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,  # Utilisation du DataCollator\n",
    "    # Ajout du Callback pour Early Stopping\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)]\n",
    ")\n",
    "print(\"Trainer configured.\")\n",
    "\n",
    "# --- 7. Train ---\n",
    "print(\"Starting Training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")\n",
    "trainer.save_model(\"best_model_f1\") # Sauvegarde le meilleur modèle\n",
    "\n",
    "# Log training metrics\n",
    "# trainer.log_metrics(\"train\", train_result.metrics) # Décommenter si besoin\n",
    "# trainer.save_metrics(\"train\", train_result.metrics) # Décommenter si besoin\n",
    "# trainer.save_state() # Sauvegarde l'état du Trainer\n",
    "\n",
    "# --- 8. Evaluate on Dev Set (using the best model loaded) ---\n",
    "print(\"\\nEvaluating on Development Set (Best Model)...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\")\n",
    "print(eval_results)\n",
    "# trainer.log_metrics(\"eval\", eval_results) # Décommenter si besoin\n",
    "# trainer.save_metrics(\"eval\", eval_results) # Décommenter si besoin\n",
    "\n",
    "# --- 9. Detailed Evaluation and Submission File Generation ---\n",
    "print(\"\\nGenerating predictions and detailed metrics for Dev Set...\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predicted_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "# --- Threshold tuning ---\n",
    "print(\"\\nRunning threshold tuning on positive class...\")\n",
    "\n",
    "# Get predicted probabilities for class 1\n",
    "logits = torch.tensor(predictions.predictions)\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)[:, 1]  # Probabilities for class 1\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Baseline (argmax) F1\n",
    "baseline_f1 = f1_score(y_true, predicted_labels, pos_label=1)\n",
    "print(f\"Baseline F1 (argmax): {baseline_f1:.4f}\")\n",
    "\n",
    "# Search best threshold\n",
    "best_thresh = 0.5\n",
    "best_f1 = 0.0\n",
    "thresholds = np.linspace(0.1, 0.9, 81)\n",
    "\n",
    "for t in thresholds:\n",
    "    preds_thresh = (probs >= t).int()\n",
    "    f1 = f1_score(y_true, preds_thresh, pos_label=1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"Best threshold found: {best_thresh:.4f} → Tuned F1: {best_f1:.4f}\")\n",
    "\n",
    "# Save threshold to file\n",
    "threshold_path = os.path.join(OUTPUT_DIR, \"threshold.txt\")\n",
    "with open(threshold_path, \"w\") as f:\n",
    "    f.write(f\"{best_thresh:.4f}\")\n",
    "print(f\"Threshold saved to {threshold_path}\")\n",
    "\n",
    "# Add predictions to the dev dataframe (ensure index alignment if necessary)\n",
    "# If dev_df was filtered by dropna, indices might not match directly. Resetting index helps.\n",
    "dev_df_eval = dev_df.reset_index(drop=True)\n",
    "# Check lengths match before assigning\n",
    "if len(dev_df_eval) == len(predicted_labels):\n",
    "    dev_df_eval['predicted_label'] = predicted_labels\n",
    "else:\n",
    "    print(f\"Error: Length mismatch! Dev DF has {len(dev_df_eval)} rows, Predictions have {len(predicted_labels)} entries.\")\n",
    "    # Handle error appropriately, maybe skip detailed eval or investigate dropna impact\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Re-extract language (assuming ID format 'lang_...') - Be careful if IDs differ\n",
    "dev_df_eval[\"language\"] = dev_df_eval[\"id\"].apply(lambda x: str(x).split(\"_\")[0] if isinstance(x, str) and \"_\" in x else \"unknown\")\n",
    "\n",
    "# --- Calculate Per-Language and Overall Metrics ---\n",
    "languages = sorted(dev_df_eval['language'].unique())\n",
    "per_language_metrics = {}\n",
    "language_f1_scores = []\n",
    "all_true_labels_eval = []\n",
    "all_pred_labels_eval = []\n",
    "\n",
    "print(\"\\n--- Detailed Evaluation on Development Set ---\")\n",
    "wandb_logs = {} # Collect logs for wandb\n",
    "\n",
    "for lang in languages:\n",
    "    if lang == \"unknown\": continue # Skip if language couldn't be extracted\n",
    "    lang_mask = dev_df_eval['language'] == lang\n",
    "    y_true_lang = dev_df_eval.loc[lang_mask, 'label']\n",
    "    y_pred_lang = dev_df_eval.loc[lang_mask, 'predicted_label']\n",
    "\n",
    "    if len(y_true_lang) == 0: continue # Skip if no data for this language\n",
    "\n",
    "    all_true_labels_eval.extend(y_true_lang.tolist())\n",
    "    all_pred_labels_eval.extend(y_pred_lang.tolist())\n",
    "\n",
    "    # Utilisation de precision_recall_fscore_support (qui est maintenant importé)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_lang, y_pred_lang, average=None, labels=[0, 1], zero_division=0)\n",
    "    accuracy_lang = accuracy_score(y_true_lang, y_pred_lang) # Renommé pour éviter conflit avec la fonction accuracy_score\n",
    "\n",
    "    per_language_metrics[lang] = {'precision': precision[1], 'recall': recall[1], 'f1': f1[1], 'accuracy': accuracy_lang}\n",
    "    language_f1_scores.append(f1[1]) # On stocke le F1 de la classe positive (1)\n",
    "\n",
    "    print(f\"\\nMetrics for language: {lang.upper()}\")\n",
    "    print(f\"  Precision-{lang} (Pos): {precision[1]:.4f}\")\n",
    "    print(f\"  Recall-{lang}    (Pos): {recall[1]:.4f}\")\n",
    "    print(f\"  F1-{lang}        (Pos): {f1[1]:.4f}\")\n",
    "    print(f\"  Accuracy-{lang}:        {accuracy_lang:.4f}\")\n",
    "\n",
    "    # Prepare logs for wandb\n",
    "    wandb_logs[f\"{lang}/precision_pos\"] = precision[1]\n",
    "    wandb_logs[f\"{lang}/recall_pos\"] = recall[1]\n",
    "    wandb_logs[f\"{lang}/f1_pos\"] = f1[1]\n",
    "    wandb_logs[f\"{lang}/accuracy\"] = accuracy_lang\n",
    "\n",
    "\n",
    "# Calculate Overall Metrics (using the full dev set lists)\n",
    "cm_overall = confusion_matrix(all_true_labels_eval, all_pred_labels_eval, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm_overall.ravel() if cm_overall.size == 4 else (0, 0, 0, 0) # Handle cases with missing classes\n",
    "\n",
    "overall_precision_pos = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "overall_recall_pos = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "# Overall F1 (Primary Metric for Positive Class)\n",
    "overall_f1_pos = 2 * (overall_precision_pos * overall_recall_pos) / (overall_precision_pos + overall_recall_pos) if (overall_precision_pos + overall_recall_pos) > 0 else 0\n",
    "# Macro F1 (Average of per-language F1s for Positive Class)\n",
    "macro_f1_pos = np.mean(language_f1_scores) if language_f1_scores else 0\n",
    "# Overall Accuracy\n",
    "overall_accuracy = accuracy_score(all_true_labels_eval, all_pred_labels_eval)\n",
    "\n",
    "print(\"\\n--- Overall Evaluation Summary (Positive Class Focus) ---\")\n",
    "print(f\"F1-score across all languages (Positive Class): {overall_f1_pos:.4f}  <-- Primary Metric\")\n",
    "print(f\"Macro F1-score across all languages (Pos Class):{macro_f1_pos:.4f}\")\n",
    "print(f\"Overall Precision (Positive Class):             {overall_precision_pos:.4f}\")\n",
    "print(f\"Overall Recall (Positive Class):                {overall_recall_pos:.4f}\")\n",
    "print(f\"Overall Accuracy across all languages:          {overall_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nOverall Confusion Matrix (All Languages):\")\n",
    "print(f\"[[TN={tn}  FP={fp}]\")\n",
    "print(f\" [FN={fn}  TP={tp}]]\")\n",
    "\n",
    "# Add overall metrics to wandb logs\n",
    "wandb_logs[\"overall/f1_pos\"] = overall_f1_pos\n",
    "wandb_logs[\"overall/macro_f1_pos\"] = macro_f1_pos\n",
    "wandb_logs[\"overall/precision_pos\"] = overall_precision_pos\n",
    "wandb_logs[\"overall/recall_pos\"] = overall_recall_pos\n",
    "wandb_logs[\"overall/accuracy\"] = overall_accuracy\n",
    "wandb_logs[\"overall/TP\"] = tp\n",
    "wandb_logs[\"overall/FP\"] = fp\n",
    "wandb_logs[\"overall/FN\"] = fn\n",
    "wandb_logs[\"overall/TN\"] = tn\n",
    "\n",
    "# Log Confusion Matrix to wandb (optional)\n",
    "if \"wandb\" in locals() and tp+fp+fn+tn > 0:\n",
    "     try:\n",
    "        wandb_logs[\"confusion_matrix\"] = wandb.plot.confusion_matrix(\n",
    "             probs=None,\n",
    "             y_true=all_true_labels_eval,\n",
    "             preds=all_pred_labels_eval,\n",
    "             class_names=[\"Negative (0)\", \"Positive (1)\"]\n",
    "         )\n",
    "     except Exception as e:\n",
    "         print(f\"Could not log confusion matrix to wandb: {e}\")\n",
    "\n",
    "\n",
    "# Log all collected metrics to wandb (if initialized)\n",
    "if \"wandb\" in locals():\n",
    "    try:\n",
    "        wandb.log(wandb_logs)\n",
    "        print(\"Metrics logged to WandB.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not log metrics to wandb: {e}\")\n",
    "\n",
    "\n",
    "# --- 10. Save Submission File ---\n",
    "print(\"\\nSaving predictions for submission...\")\n",
    "# Ensure results directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Prepare submission dataframe\n",
    "submission_df = dev_df_eval[['id', 'predicted_label']]\n",
    "\n",
    "# Define CSV and ZIP paths\n",
    "csv_filename = \"predictions_task1.csv\"\n",
    "zip_filename = \"submission_task1.zip\"\n",
    "csv_path = os.path.join(OUTPUT_DIR, csv_filename)\n",
    "zip_path = os.path.join(OUTPUT_DIR, zip_filename)\n",
    "\n",
    "# Save CSV\n",
    "submission_df.to_csv(csv_path, index=False)\n",
    "print(f\"Predictions saved to {csv_path}\")\n",
    "\n",
    "# Zip the CSV file\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(csv_path, arcname=csv_filename)\n",
    "print(f\"{csv_filename} has been zipped into {zip_path}\")\n",
    "\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna XLM Roberta Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible. Utilisation de GPU: NVIDIA GeForce RTX 3070\n",
      "Loading data...\n",
      "Train data: 31187 rows, Dev data: 4625 rows\n",
      "Data loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925fed0eb9d840548bfbf29565c70568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/565 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ed803cc4394b33b402db6b334fca59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2204c4e5b43c40a88e5a60a703ee7cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 17:03:51,696] A new study created in RDB with name: ade_xlmr_large_opt_focused_v4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing class weights...\n",
      "Class Weights (pour classes [0 1]) sur cuda: tensor([0.5426, 6.3621], device='cuda:0')\n",
      "\n",
      "--- Starting Optuna Focused Study ---\n",
      "Optuna study using storage: sqlite:///optuna_cardiffnlp-twitter-roberta-base\\optuna_study.db\n",
      "Number of trials already completed in storage: 0\n",
      "\n",
      "--- Starting Trial 0 ---\n",
      "  Output Dir: optuna_cardiffnlp-twitter-roberta-base\\trial_0\n",
      "  Hyperparameters: {'learning_rate': 1.1275465620953943e-05, 'num_train_epochs': 7, 'weight_decay': 0.0639196365086843, 'warmup_ratio': 0.10986584841970365, 'lr_scheduler_type': 'cosine', 'hidden_dropout_prob': 0.2116167224336399, 'attention_probs_dropout_prob': 0.3732352291549871}\n",
      "Tokenizing data for trial...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047d08c4e52243399d732bf154965352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer:   0%|          | 0/31187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8c6bf13a0c4129aaa2222d453406ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer:   0%|          | 0/4625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Remaining cols: ['labels', 'input_ids', 'attention_mask']\n",
      "Loading model for trial...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ebae5007294894ac2fc8acabd0d46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_4732\\4256425422.py:198: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded (dtype: torch.bfloat16).\n",
      "Starting training for trial 0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c55d07576e49f3a3635834f82d2670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7800' max='13643' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7800/13643 29:55 < 22:25, 4.34 it/s, Epoch 4/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Pos</th>\n",
       "      <th>Precision Pos</th>\n",
       "      <th>Recall Pos</th>\n",
       "      <th>F1 Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.652300</td>\n",
       "      <td>0.631430</td>\n",
       "      <td>0.868108</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.236318</td>\n",
       "      <td>0.238693</td>\n",
       "      <td>0.927811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.530400</td>\n",
       "      <td>0.600862</td>\n",
       "      <td>0.771027</td>\n",
       "      <td>0.284943</td>\n",
       "      <td>0.194829</td>\n",
       "      <td>0.530151</td>\n",
       "      <td>0.863689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.571057</td>\n",
       "      <td>0.634811</td>\n",
       "      <td>0.265971</td>\n",
       "      <td>0.160799</td>\n",
       "      <td>0.768844</td>\n",
       "      <td>0.756943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.472100</td>\n",
       "      <td>0.573957</td>\n",
       "      <td>0.645838</td>\n",
       "      <td>0.272647</td>\n",
       "      <td>0.165588</td>\n",
       "      <td>0.771357</td>\n",
       "      <td>0.765933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 17:34:10,658] Trial 0 finished with value: 0.2849426063470628 and parameters: {'learning_rate': 1.1275465620953943e-05, 'num_train_epochs': 7, 'weight_decay': 0.0639196365086843, 'warmup_ratio': 0.10986584841970365, 'lr_scheduler_type': 'cosine', 'hidden_dropout_prob': 0.2116167224336399, 'attention_probs_dropout_prob': 0.3732352291549871}. Best is trial 0 with value: 0.2849426063470628.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for trial 0.\n",
      "Retrieving best metric for trial 0...\n",
      "Trial 0 - Best Validation F1_pos recorded: 0.2849\n",
      "\n",
      "--- Starting Trial 1 ---\n",
      "  Output Dir: optuna_cardiffnlp-twitter-roberta-base\\trial_1\n",
      "  Hyperparameters: {'learning_rate': 1.3877067474879668e-05, 'num_train_epochs': 6, 'weight_decay': 0.021235069657748146, 'warmup_ratio': 0.1469909852161994, 'lr_scheduler_type': 'cosine', 'hidden_dropout_prob': 0.23636499344142015, 'attention_probs_dropout_prob': 0.23668090197068678}\n",
      "Tokenizing data for trial...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ae2fae78654cd48e08d29e0e42ccc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer:   0%|          | 0/31187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae1fde90045484099befe099cfa8c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer:   0%|          | 0/4625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Remaining cols: ['labels', 'input_ids', 'attention_mask']\n",
      "Loading model for trial...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_4732\\4256425422.py:198: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded (dtype: torch.bfloat16).\n",
      "Starting training for trial 1...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11694' max='11694' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11694/11694 43:04, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Pos</th>\n",
       "      <th>Precision Pos</th>\n",
       "      <th>Recall Pos</th>\n",
       "      <th>F1 Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.636211</td>\n",
       "      <td>0.884973</td>\n",
       "      <td>0.273224</td>\n",
       "      <td>0.299401</td>\n",
       "      <td>0.251256</td>\n",
       "      <td>0.937544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.531800</td>\n",
       "      <td>0.752782</td>\n",
       "      <td>0.912432</td>\n",
       "      <td>0.201183</td>\n",
       "      <td>0.467890</td>\n",
       "      <td>0.128141</td>\n",
       "      <td>0.953677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.550574</td>\n",
       "      <td>0.760649</td>\n",
       "      <td>0.305956</td>\n",
       "      <td>0.203843</td>\n",
       "      <td>0.613065</td>\n",
       "      <td>0.855389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.479300</td>\n",
       "      <td>0.589865</td>\n",
       "      <td>0.848649</td>\n",
       "      <td>0.332061</td>\n",
       "      <td>0.267692</td>\n",
       "      <td>0.437186</td>\n",
       "      <td>0.914655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.476200</td>\n",
       "      <td>0.612825</td>\n",
       "      <td>0.862919</td>\n",
       "      <td>0.335430</td>\n",
       "      <td>0.287770</td>\n",
       "      <td>0.402010</td>\n",
       "      <td>0.923578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\trial\\_trial.py:497: UserWarning: The reported value is ignored because this `step` 5 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2025-04-11 18:17:19,594] Trial 1 finished with value: 0.33542976939203356 and parameters: {'learning_rate': 1.3877067474879668e-05, 'num_train_epochs': 6, 'weight_decay': 0.021235069657748146, 'warmup_ratio': 0.1469909852161994, 'lr_scheduler_type': 'cosine', 'hidden_dropout_prob': 0.23636499344142015, 'attention_probs_dropout_prob': 0.23668090197068678}. Best is trial 1 with value: 0.33542976939203356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for trial 1.\n",
      "Retrieving best metric for trial 1...\n",
      "Trial 1 - Best Validation F1_pos recorded: 0.3354\n",
      "\n",
      "--- Starting Trial 2 ---\n",
      "  Output Dir: optuna_cardiffnlp-twitter-roberta-base\\trial_2\n",
      "  Hyperparameters: {'learning_rate': 1.0572072866769706e-05, 'num_train_epochs': 6, 'weight_decay': 0.04591670111852694, 'warmup_ratio': 0.07912291401980419, 'lr_scheduler_type': 'cosine', 'hidden_dropout_prob': 0.2584289297070437, 'attention_probs_dropout_prob': 0.2732723686587384}\n",
      "Tokenizing data for trial...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5839c48da74ab19c1da4482ed8dafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer:   0%|          | 0/31187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- [Imports et configuration initiale comme avant] ---\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl\n",
    ")\n",
    "# import wandb # Décommente si utilisé\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import traceback\n",
    "from typing import Dict, Any\n",
    "\n",
    "# --- Configuration Initiale ---\n",
    "BASE_MODEL_NAME = \"cardiffnlp/twitter-roberta-base\"\n",
    "TRAIN_CSV = \"data/train_data_SMM4H_2025_Task_1.csv\" # Adapte le chemin si nécessaire\n",
    "DEV_CSV = \"data/dev_data_SMM4H_2025_Task_1.csv\"   # Adapte le chemin si nécessaire\n",
    "BASE_OUTPUT_DIR = \"optuna_cardiffnlp-twitter-roberta-base\" # Nouveau nom: recherche focalisée\n",
    "NUM_TRIALS = 25 # Peut-être moins d'essais suffisent avec des plages plus étroites\n",
    "N_BEST_MODELS = 5\n",
    "MAX_LENGTH = 512 #Gardé à 256 pour VRAM Optuna run\n",
    "BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "SEED = 42\n",
    "WANDB_PROJECT_NAME = \"ade-classification-optuna-focused-v4\"\n",
    "WANDB_LOGGING = False # Met à True si configuré\n",
    "\n",
    "# --- Seed & Setup GPU ---\n",
    "set_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    print(f\"CUDA disponible. Utilisation de GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    if not torch.cuda.is_bf16_supported():\n",
    "        print(\"Attention: BF16 non supporté par ce GPU. Utilisation de FP32.\")\n",
    "else:\n",
    "    print(\"CUDA non disponible. Utilisation de CPU.\")\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_CSV).dropna(subset=['text'])\n",
    "    dev_df = pd.read_csv(DEV_CSV).dropna(subset=['text'])\n",
    "    print(f\"Train data: {len(train_df)} rows, Dev data: {len(dev_df)} rows\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading CSV files: {e}. Vérifie les chemins TRAIN_CSV et DEV_CSV.\")\n",
    "    exit()\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "dataset_dict = DatasetDict({'train': train_dataset, 'validation': dev_dataset})\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "# --- 2. Tokenizer & tokenize_function ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=False, max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "# --- 3. Class Weights ---\n",
    "print(\"Computing class weights...\")\n",
    "labels_train = train_df['label'].values\n",
    "class_weights_tensor = None\n",
    "unique_labels = np.unique(labels_train)\n",
    "if len(unique_labels) > 1:\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=unique_labels, y=labels_train)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    print(f\"Class Weights (pour classes {unique_labels}) sur {device}: {class_weights_tensor}\")\n",
    "else:\n",
    "    print(f\"Warning: Only one class ({unique_labels[0]}) found. Cannot compute class weights.\")\n",
    "\n",
    "# --- 4. WeightedTrainer ---\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        if class_weights_tensor is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor.to(logits.device))\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --- 5. compute_metrics ---\n",
    "def compute_metrics(pred) -> Dict[str, float]:\n",
    "    labels = pred.label_ids\n",
    "    if isinstance(pred.predictions, tuple): logits = pred.predictions[0]\n",
    "    else: logits = pred.predictions\n",
    "    if logits is None: return {'f1_pos': 0.0}\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None, labels=[0, 1], zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1_pos': f1[1], 'precision_pos': precision[1], 'recall_pos': recall[1], 'f1_neg': f1[0]}\n",
    "\n",
    "# --- 6. Data Collator ---\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# --- 7. Callback Personnalisé pour le Pruning Optuna ---\n",
    "class OptunaPruningCallback(TrainerCallback):\n",
    "    def __init__(self, trial: optuna.Trial):\n",
    "        self.trial = trial\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics: Dict[str, float], **kwargs):\n",
    "        metric_to_report = args.metric_for_best_model\n",
    "        if not metric_to_report.startswith(\"eval_\"): metric_to_report = f\"eval_{metric_to_report}\"\n",
    "        metric_value = metrics.get(metric_to_report)\n",
    "        if metric_value is None: return\n",
    "        current_epoch = int(state.epoch) if state.epoch is not None else 0\n",
    "        self.trial.report(metric_value, step=current_epoch)\n",
    "        if self.trial.should_prune():\n",
    "            message = f\"Trial {self.trial.number} pruned at epoch {current_epoch}.\"\n",
    "            print(message)\n",
    "            raise optuna.exceptions.TrialPruned(message)\n",
    "\n",
    "# --- 8. Fonction Objective pour Optuna ---\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Hyperparamètres à optimiser (PLAGES RESTREINTES) ---\n",
    "    # *** MODIFICATIONS ICI ***\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 8e-6, 2e-5, log=True) # Autour de 1e-5\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 4, 7)             # Autour de 6\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.02, 0.08)             # Autour de 0.05\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.05, 0.15)             # Autour de 0.1\n",
    "    lr_scheduler_type = trial.suggest_categorical(\"lr_scheduler_type\", [\"cosine\", \"linear\"]) # Garde les deux options\n",
    "    hidden_dropout_prob = trial.suggest_float(\"hidden_dropout_prob\", 0.2, 0.4) # Autour de 0.3\n",
    "    attention_probs_dropout_prob = trial.suggest_float(\"attention_probs_dropout_prob\", 0.2, 0.4) # Autour de 0.3\n",
    "    optim = \"adafactor\" # Gardé fixe\n",
    "\n",
    "    # --- Configuration spécifique à l'essai ---\n",
    "    trial_run_name = f\"trial_{trial.number}_lr{learning_rate:.1e}_ep{num_train_epochs}_wd{weight_decay:.2f}\"\n",
    "    trial_output_dir = os.path.join(BASE_OUTPUT_DIR, f\"trial_{trial.number}\")\n",
    "    os.makedirs(trial_output_dir, exist_ok=True)\n",
    "    print(f\"\\n--- Starting Trial {trial.number} ---\")\n",
    "    print(f\"  Output Dir: {trial_output_dir}\")\n",
    "    print(f\"  Hyperparameters: {trial.params}\")\n",
    "\n",
    "    # --- Tokenization ---\n",
    "    print(\"Tokenizing data for trial...\")\n",
    "    try:\n",
    "        tokenized_datasets = dataset_dict.map(tokenize_function, batched=True, load_from_cache_file=False, desc=\"Running tokenizer\")\n",
    "        cols_before = tokenized_datasets['train'].column_names\n",
    "        cols_to_remove = [\"text\", \"id\", \"file_name\", \"origin\", \"language\", \"split\", \"type\"] + [c for c in cols_before if c.startswith(\"__\")]\n",
    "        actual_cols_to_remove = [col for col in cols_to_remove if col in cols_before]\n",
    "        tokenized_datasets = tokenized_datasets.remove_columns(actual_cols_to_remove)\n",
    "        tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "        tokenized_datasets.set_format(\"torch\")\n",
    "        print(f\"Tokenization complete. Remaining cols: {tokenized_datasets['train'].column_names}\")\n",
    "    except Exception as e: print(f\"Data processing error: {e}\"); return 0.0\n",
    "\n",
    "    # --- Chargement du modèle ---\n",
    "    print(\"Loading model for trial...\")\n",
    "    try:\n",
    "        model_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            BASE_MODEL_NAME, num_labels=2, hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob, torch_dtype=model_dtype,\n",
    "            ignore_mismatched_sizes=True )\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(f\"Model loaded (dtype: {model_dtype}).\")\n",
    "    except Exception as e: print(f\"Model loading error: {e}\"); traceback.print_exc(); return 0.0\n",
    "\n",
    "    # --- Training Arguments ---\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=trial_output_dir, num_train_epochs=num_train_epochs, learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay, lr_scheduler_type=lr_scheduler_type, warmup_ratio=warmup_ratio, optim=optim,\n",
    "        per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, fp16=False,\n",
    "        bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(), gradient_checkpointing=True,\n",
    "        eval_strategy=\"epoch\", save_strategy=\"epoch\", save_total_limit=1,\n",
    "        load_best_model_at_end=True, metric_for_best_model=\"f1_pos\", greater_is_better=True,\n",
    "        logging_dir=os.path.join(trial_output_dir, 'logs'), logging_strategy=\"epoch\",\n",
    "        report_to=\"wandb\" if WANDB_LOGGING else \"none\", seed=SEED, disable_tqdm=False )\n",
    "\n",
    "    # --- Callbacks ---\n",
    "    early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)\n",
    "    pruning_callback = OptunaPruningCallback(trial)\n",
    "\n",
    "    # --- Trainer ---\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model, args=training_args, train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"], tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics, data_collator=data_collator,\n",
    "        callbacks=[early_stopping_callback, pruning_callback] )\n",
    "\n",
    "    # --- W&B Run (Optionnel) ---\n",
    "    if WANDB_LOGGING:\n",
    "        try:\n",
    "            wandb.init(project=WANDB_PROJECT_NAME, name=trial_run_name, config=trial.params,\n",
    "                       reinit=True, group=\"Optuna Focused Search\", resume=\"allow\")\n",
    "        except Exception as e: print(f\"WandB init error trial {trial.number}: {e}\"); trainer.args.report_to = \"none\"\n",
    "\n",
    "    # --- Entraînement & Gestion Erreurs/Pruning ---\n",
    "    final_f1_score = 0.0\n",
    "    try:\n",
    "        print(f\"Starting training for trial {trial.number}...\")\n",
    "        train_result = trainer.train()\n",
    "        print(f\"Training finished for trial {trial.number}.\")\n",
    "        print(f\"Retrieving best metric for trial {trial.number}...\")\n",
    "        best_metric = trainer.state.best_metric\n",
    "        if best_metric is None and trainer.state.log_history:\n",
    "             # Essaye de trouver la meilleure métrique dans l'historique si state.best_metric est None\n",
    "             eval_logs = [log for log in trainer.state.log_history if 'eval_f1_pos' in log]\n",
    "             if eval_logs:\n",
    "                 final_f1_score = max(log['eval_f1_pos'] for log in eval_logs)\n",
    "             else: # Si toujours rien, évalue une dernière fois\n",
    "                 eval_results = trainer.evaluate()\n",
    "                 final_f1_score = eval_results.get(\"eval_f1_pos\", 0.0)\n",
    "        elif best_metric is not None:\n",
    "             final_f1_score = best_metric\n",
    "        else: # Cas où l'entraînement a échoué très tôt\n",
    "             final_f1_score = 0.0\n",
    "\n",
    "        print(f\"Trial {trial.number} - Best Validation F1_pos recorded: {final_f1_score:.4f}\")\n",
    "        if WANDB_LOGGING and wandb.run: wandb.log({\"best_eval_f1_pos\": final_f1_score})\n",
    "\n",
    "    except optuna.exceptions.TrialPruned as e:\n",
    "        print(f\"!!! {e}\")\n",
    "        if WANDB_LOGGING and wandb.run: wandb.log({\"status\": \"pruned\", \"eval_f1_pos\": 0.0})\n",
    "        final_f1_score = 0.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Trial {trial.number} failed with error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        if WANDB_LOGGING and wandb.run: wandb.log({\"error\": str(e), \"eval_f1_pos\": 0.0})\n",
    "        final_f1_score = 0.0\n",
    "\n",
    "    finally:\n",
    "        if WANDB_LOGGING and wandb.run:\n",
    "            try: wandb.finish()\n",
    "            except Exception as e: print(f\"WandB finish error: {e}\")\n",
    "        del model, trainer, tokenized_datasets\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return final_f1_score\n",
    "\n",
    "# --- Création et lancement de l'étude Optuna ---\n",
    "print(\"\\n--- Starting Optuna Focused Study ---\")\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0, interval_steps=1) # Warmup 0\n",
    "storage_name = f\"sqlite:///{os.path.join(BASE_OUTPUT_DIR, 'optuna_study.db')}\"\n",
    "study = optuna.create_study(\n",
    "    study_name=\"ade_xlmr_large_opt_focused_v4\", direction=\"maximize\", sampler=sampler, pruner=pruner,\n",
    "    storage=storage_name, load_if_exists=True )\n",
    "\n",
    "print(f\"Optuna study using storage: {storage_name}\")\n",
    "print(f\"Number of trials already completed in storage: {len(study.trials)}\")\n",
    "\n",
    "# Lancer l'optimisation\n",
    "try:\n",
    "    study.optimize(objective, n_trials=NUM_TRIALS, timeout=3600 * 12, gc_after_trial=True)\n",
    "except KeyboardInterrupt: print(\"\\nOptuna study interrupted.\")\n",
    "except Exception as e: print(f\"\\nOptuna study error: {e}\"); traceback.print_exc()\n",
    "\n",
    "# --- Analyse des résultats ---\n",
    "print(\"\\n--- Optuna Study Finished ---\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE and t.value is not None and t.value > 1e-6]\n",
    "if not completed_trials:\n",
    "    print(\"No trials completed successfully with a positive score.\")\n",
    "else:\n",
    "    best_trial = max(completed_trials, key=lambda t: t.value)\n",
    "    print(f\"\\nBest trial number (among completed): {best_trial.number}\")\n",
    "    print(f\"  Best Validation F1_pos: {best_trial.value:.4f}\")\n",
    "    print(\"  Best Hyperparameters:\"); [print(f\"    {k}: {v}\") for k, v in best_trial.params.items()]\n",
    "\n",
    "# --- Sauvegarder les N meilleurs modèles ---\n",
    "print(f\"\\n--- Saving Top {N_BEST_MODELS} Models ---\")\n",
    "top_models_dir = os.path.join(BASE_OUTPUT_DIR, \"top_models\")\n",
    "os.makedirs(top_models_dir, exist_ok=True)\n",
    "best_completed_trials = sorted(completed_trials, key=lambda t: t.value, reverse=True)\n",
    "saved_model_count = 0\n",
    "for rank, trial in enumerate(best_completed_trials):\n",
    "    if saved_model_count >= N_BEST_MODELS: break\n",
    "    trial_output_dir = os.path.join(BASE_OUTPUT_DIR, f\"trial_{trial.number}\")\n",
    "    source_model_dir = trial_output_dir\n",
    "    model_file = \"model.safetensors\" if os.path.exists(os.path.join(source_model_dir, \"model.safetensors\")) else \"pytorch_model.bin\"\n",
    "    required_files = [\"config.json\", model_file, \"tokenizer_config.json\", \"special_tokens_map.json\"]\n",
    "    if not all(os.path.exists(os.path.join(source_model_dir, f)) for f in required_files):\n",
    "         print(f\"!!! Warning: Missing files in {source_model_dir} for trial {trial.number}. Skipping save.\")\n",
    "         continue\n",
    "    destination_dir = os.path.join(top_models_dir, f\"rank_{rank+1}_trial_{trial.number}_f1_{trial.value:.4f}\")\n",
    "    try:\n",
    "        ignore_patterns = shutil.ignore_patterns('optimizer.pt', 'scheduler.pt', 'trainer_state.json', 'training_args.bin','logs', '*.log', 'rng_state.pth', 'events.out.tfevents.*','checkpoint-*')\n",
    "        shutil.copytree(source_model_dir, destination_dir, ignore=ignore_patterns, dirs_exist_ok=True)\n",
    "        trial_info = {\"rank\": rank + 1, \"trial_number\": trial.number, \"validation_f1_pos\": trial.value,\"hyperparameters\": trial.params}\n",
    "        with open(os.path.join(destination_dir, \"trial_info.json\"), \"w\") as f: json.dump(trial_info, f, indent=4)\n",
    "        print(f\"  Rank {rank+1}: Saved model from trial {trial.number} to {destination_dir} (F1: {trial.value:.4f})\")\n",
    "        saved_model_count += 1\n",
    "    except Exception as e: print(f\"!!! Error saving model for trial {trial.number}: {e}\"); traceback.print_exc()\n",
    "\n",
    "# --- Nettoyage Optionnel ---\n",
    "# Décommente pour supprimer les dossiers d'essais non conservés\n",
    "# print(\"\\n--- Cleaning up non-top trial directories ---\")\n",
    "# ...\n",
    "\n",
    "# --- Message Final ---\n",
    "print(f\"\\nScript finished. Optuna focused study completed.\")\n",
    "if saved_model_count > 0:\n",
    "    print(f\"Top {saved_model_count} models saved in '{top_models_dir}'.\")\n",
    "    print(\"You can now use these models for ensembling.\")\n",
    "else:\n",
    "    print(\"No models were saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback # Importer le callback pour l'arrêt précoce\n",
    ")\n",
    "# Assurez-vous que wandb est initialisé si vous l'utilisez, sinon commentez/supprimez les lignes wandb.log\n",
    "import wandb\n",
    "wandb.init(project=\"ade-classification-chatgptuamgneted_xlmr\") # Exemple d'initialisation\n",
    "\n",
    "# CORRECTION ICI: Ajout de precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "import zipfile # Pour la sauvegarde finale\n",
    "import os # Pour la sauvegarde finale\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"xlm-roberta-large\"  # Passer à la version Large\n",
    "TRAIN_CSV = \"data/train_data_SMM4H_2025_Task_1.csv\" # Utiliser le CSV d'entraînement augmenté\n",
    "DEV_CSV = \"data/dev_data_SMM4H_2025_Task_1.csv\"\n",
    "OUTPUT_DIR = \"easy_xlmr_large_256\"  # Répertoire de sortie pour le modèle et les résultats\n",
    "NUM_EPOCHS = 6  # Réduire légèrement les epochs\n",
    "BATCH_SIZE = 2  # Plus petit batch size\n",
    "LEARNING_RATE = 1e-5  # Learning rate plus bas pour le large\n",
    "GRADIENT_ACCUMULATION_STEPS = 16  # Accumulation plus importante\n",
    "EARLY_STOPPING_PATIENCE = 1  # Arrêt plus rapide si stagnation\n",
    "MAX_LENGTH = 512  # Réduire la longueur maximale des séquences\n",
    "WEIGHT_DECAY = 0.05  # Poids de régularisation\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_CSV).dropna(subset=['text'])\n",
    "    dev_df = pd.read_csv(DEV_CSV).dropna(subset=['text'])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading CSV files: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "dataset_dict = DatasetDict({'train': train_dataset, 'validation': dev_dataset})\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "# --- 2. Tokenization ---\n",
    "print(\"Tokenizing data...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- Modifications de la fonction de tokenization ---\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True  # Pour éviter de dépasser les limites du modèle\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# --- Clean up columns ---\n",
    "print(\"Cleaning dataset columns...\")\n",
    "print(\"Columns before removal:\", tokenized_datasets['train'].column_names)\n",
    "columns_to_remove = [\"text\", \"id\", \"file_name\", \"origin\", \"language\", \"split\", \"type\"]\n",
    "actual_columns_to_remove = [col for col in columns_to_remove if col in tokenized_datasets['train'].column_names]\n",
    "print(\"Removing columns:\", actual_columns_to_remove)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(actual_columns_to_remove)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Rename 'label' to 'labels'\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "print(\"Columns after cleaning and rename:\", tokenized_datasets['train'].column_names)\n",
    "\n",
    "# --- 3. Compute Class Weights ---\n",
    "print(\"Computing class weights...\")\n",
    "labels_train = train_df['label'].values\n",
    "if len(np.unique(labels_train)) > 1: # Ensure there are at least two classes\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels_train), y=labels_train)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Class Weights: {class_weights_tensor}\")\n",
    "else:\n",
    "    print(\"Warning: Only one class found in training data. Cannot compute class weights.\")\n",
    "    class_weights_tensor = None # Handle this case in the loss function if needed\n",
    "\n",
    "# --- Custom Trainer for Weighted Loss ---\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Use weights only if they were computed\n",
    "        if class_weights_tensor is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss() # Default unweighted loss\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --- 4. Model & Metrics ---\n",
    "print(\"Loading model...\")\n",
    "# --- Chargement du modèle avec optimisations ---\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# --- Increase dropout to help regularization ---\n",
    "model.config.hidden_dropout_prob = 0.3  # Dropout on fully connected layers\n",
    "model.config.attention_probs_dropout_prob = 0.3  # Dropout on attention probabilities\n",
    "print(\"Dropout increased: hidden_dropout=0.3, attention_dropout=0.3\")\n",
    "\n",
    "# Activer le gradient checkpointing pour économiser la mémoire\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Fonction compute_metrics qui utilise la fonction importée\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Utilisation de precision_recall_fscore_support\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None, labels=[0, 1], zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_pos': f1[1], # F1 for class 1\n",
    "        'precision_pos': precision[1],\n",
    "        'recall_pos': recall[1],\n",
    "        'f1_neg': f1[0], # F1 for class 0 (for info)\n",
    "    }\n",
    "\n",
    "# --- 5. Training Arguments ---\n",
    "print(\"Setting training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # Modifié (était BATCH_SIZE*2)\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    lr_scheduler_type=\"cosine\",  # <-- Ajouté\n",
    "    warmup_ratio=0.1,            # <-- Ajouté (10% des steps en warmup)\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_pos\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    report_to=\"wandb\" if \"wandb\" in locals() else \"none\",\n",
    "    fp16=False,  # Disabled FP16 to avoid gradient unscaling issues\n",
    "    bf16=True if torch.cuda.is_available() else False,  # Use BF16 instead if available\n",
    "    optim=\"adafactor\",  # Ajouté - Optimiseur plus léger\n",
    "    gradient_checkpointing=True,  # Ajouté - Économie de mémoire\n",
    "    #torch_compile=False,  # Ajouté - Acceleration PyTorch 2.x+\n",
    "    save_total_limit=2\n",
    "    # early_stopping_patience=EARLY_STOPPING_PATIENCE,  # Déjà géré par le callback\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# --- 6. Trainer ---\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,  # Utilisation du DataCollator\n",
    "    # Ajout du Callback pour Early Stopping\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)]\n",
    ")\n",
    "print(\"Trainer configured.\")\n",
    "\n",
    "# --- 7. Train ---\n",
    "print(\"Starting Training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")\n",
    "trainer.save_model(\"best_model_f1\") # Sauvegarde le meilleur modèle\n",
    "\n",
    "# Log training metrics\n",
    "# trainer.log_metrics(\"train\", train_result.metrics) # Décommenter si besoin\n",
    "# trainer.save_metrics(\"train\", train_result.metrics) # Décommenter si besoin\n",
    "# trainer.save_state() # Sauvegarde l'état du Trainer\n",
    "\n",
    "# --- 8. Evaluate on Dev Set (using the best model loaded) ---\n",
    "print(\"\\nEvaluating on Development Set (Best Model)...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\")\n",
    "print(eval_results)\n",
    "# trainer.log_metrics(\"eval\", eval_results) # Décommenter si besoin\n",
    "# trainer.save_metrics(\"eval\", eval_results) # Décommenter si besoin\n",
    "\n",
    "# --- 9. Detailed Evaluation and Submission File Generation ---\n",
    "print(\"\\nGenerating predictions and detailed metrics for Dev Set...\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predicted_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "# --- Threshold tuning ---\n",
    "print(\"\\nRunning threshold tuning on positive class...\")\n",
    "\n",
    "# Get predicted probabilities for class 1\n",
    "logits = torch.tensor(predictions.predictions)\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)[:, 1]  # Probabilities for class 1\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Baseline (argmax) F1\n",
    "baseline_f1 = f1_score(y_true, predicted_labels, pos_label=1)\n",
    "print(f\"Baseline F1 (argmax): {baseline_f1:.4f}\")\n",
    "\n",
    "# Search best threshold\n",
    "best_thresh = 0.5\n",
    "best_f1 = 0.0\n",
    "thresholds = np.linspace(0.1, 0.9, 81)\n",
    "\n",
    "for t in thresholds:\n",
    "    preds_thresh = (probs >= t).int()\n",
    "    f1 = f1_score(y_true, preds_thresh, pos_label=1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"Best threshold found: {best_thresh:.4f} → Tuned F1: {best_f1:.4f}\")\n",
    "\n",
    "# Save threshold to file\n",
    "threshold_path = os.path.join(OUTPUT_DIR, \"threshold.txt\")\n",
    "with open(threshold_path, \"w\") as f:\n",
    "    f.write(f\"{best_thresh:.4f}\")\n",
    "print(f\"Threshold saved to {threshold_path}\")\n",
    "\n",
    "# Add predictions to the dev dataframe (ensure index alignment if necessary)\n",
    "# If dev_df was filtered by dropna, indices might not match directly. Resetting index helps.\n",
    "dev_df_eval = dev_df.reset_index(drop=True)\n",
    "# Check lengths match before assigning\n",
    "if len(dev_df_eval) == len(predicted_labels):\n",
    "    dev_df_eval['predicted_label'] = predicted_labels\n",
    "else:\n",
    "    print(f\"Error: Length mismatch! Dev DF has {len(dev_df_eval)} rows, Predictions have {len(predicted_labels)} entries.\")\n",
    "    # Handle error appropriately, maybe skip detailed eval or investigate dropna impact\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Re-extract language (assuming ID format 'lang_...') - Be careful if IDs differ\n",
    "dev_df_eval[\"language\"] = dev_df_eval[\"id\"].apply(lambda x: str(x).split(\"_\")[0] if isinstance(x, str) and \"_\" in x else \"unknown\")\n",
    "\n",
    "# --- Calculate Per-Language and Overall Metrics ---\n",
    "languages = sorted(dev_df_eval['language'].unique())\n",
    "per_language_metrics = {}\n",
    "language_f1_scores = []\n",
    "all_true_labels_eval = []\n",
    "all_pred_labels_eval = []\n",
    "\n",
    "print(\"\\n--- Detailed Evaluation on Development Set ---\")\n",
    "wandb_logs = {} # Collect logs for wandb\n",
    "\n",
    "for lang in languages:\n",
    "    if lang == \"unknown\": continue # Skip if language couldn't be extracted\n",
    "    lang_mask = dev_df_eval['language'] == lang\n",
    "    y_true_lang = dev_df_eval.loc[lang_mask, 'label']\n",
    "    y_pred_lang = dev_df_eval.loc[lang_mask, 'predicted_label']\n",
    "\n",
    "    if len(y_true_lang) == 0: continue # Skip if no data for this language\n",
    "\n",
    "    all_true_labels_eval.extend(y_true_lang.tolist())\n",
    "    all_pred_labels_eval.extend(y_pred_lang.tolist())\n",
    "\n",
    "    # Utilisation de precision_recall_fscore_support (qui est maintenant importé)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_lang, y_pred_lang, average=None, labels=[0, 1], zero_division=0)\n",
    "    accuracy_lang = accuracy_score(y_true_lang, y_pred_lang) # Renommé pour éviter conflit avec la fonction accuracy_score\n",
    "\n",
    "    per_language_metrics[lang] = {'precision': precision[1], 'recall': recall[1], 'f1': f1[1], 'accuracy': accuracy_lang}\n",
    "    language_f1_scores.append(f1[1]) # On stocke le F1 de la classe positive (1)\n",
    "\n",
    "    print(f\"\\nMetrics for language: {lang.upper()}\")\n",
    "    print(f\"  Precision-{lang} (Pos): {precision[1]:.4f}\")\n",
    "    print(f\"  Recall-{lang}    (Pos): {recall[1]:.4f}\")\n",
    "    print(f\"  F1-{lang}        (Pos): {f1[1]:.4f}\")\n",
    "    print(f\"  Accuracy-{lang}:        {accuracy_lang:.4f}\")\n",
    "\n",
    "    # Prepare logs for wandb\n",
    "    wandb_logs[f\"{lang}/precision_pos\"] = precision[1]\n",
    "    wandb_logs[f\"{lang}/recall_pos\"] = recall[1]\n",
    "    wandb_logs[f\"{lang}/f1_pos\"] = f1[1]\n",
    "    wandb_logs[f\"{lang}/accuracy\"] = accuracy_lang\n",
    "\n",
    "\n",
    "# Calculate Overall Metrics (using the full dev set lists)\n",
    "cm_overall = confusion_matrix(all_true_labels_eval, all_pred_labels_eval, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm_overall.ravel() if cm_overall.size == 4 else (0, 0, 0, 0) # Handle cases with missing classes\n",
    "\n",
    "overall_precision_pos = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "overall_recall_pos = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "# Overall F1 (Primary Metric for Positive Class)\n",
    "overall_f1_pos = 2 * (overall_precision_pos * overall_recall_pos) / (overall_precision_pos + overall_recall_pos) if (overall_precision_pos + overall_recall_pos) > 0 else 0\n",
    "# Macro F1 (Average of per-language F1s for Positive Class)\n",
    "macro_f1_pos = np.mean(language_f1_scores) if language_f1_scores else 0\n",
    "# Overall Accuracy\n",
    "overall_accuracy = accuracy_score(all_true_labels_eval, all_pred_labels_eval)\n",
    "\n",
    "print(\"\\n--- Overall Evaluation Summary (Positive Class Focus) ---\")\n",
    "print(f\"F1-score across all languages (Positive Class): {overall_f1_pos:.4f}  <-- Primary Metric\")\n",
    "print(f\"Macro F1-score across all languages (Pos Class):{macro_f1_pos:.4f}\")\n",
    "print(f\"Overall Precision (Positive Class):             {overall_precision_pos:.4f}\")\n",
    "print(f\"Overall Recall (Positive Class):                {overall_recall_pos:.4f}\")\n",
    "print(f\"Overall Accuracy across all languages:          {overall_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nOverall Confusion Matrix (All Languages):\")\n",
    "print(f\"[[TN={tn}  FP={fp}]\")\n",
    "print(f\" [FN={fn}  TP={tp}]]\")\n",
    "\n",
    "# Add overall metrics to wandb logs\n",
    "wandb_logs[\"overall/f1_pos\"] = overall_f1_pos\n",
    "wandb_logs[\"overall/macro_f1_pos\"] = macro_f1_pos\n",
    "wandb_logs[\"overall/precision_pos\"] = overall_precision_pos\n",
    "wandb_logs[\"overall/recall_pos\"] = overall_recall_pos\n",
    "wandb_logs[\"overall/accuracy\"] = overall_accuracy\n",
    "wandb_logs[\"overall/TP\"] = tp\n",
    "wandb_logs[\"overall/FP\"] = fp\n",
    "wandb_logs[\"overall/FN\"] = fn\n",
    "wandb_logs[\"overall/TN\"] = tn\n",
    "\n",
    "# Log Confusion Matrix to wandb (optional)\n",
    "if \"wandb\" in locals() and tp+fp+fn+tn > 0:\n",
    "     try:\n",
    "        wandb_logs[\"confusion_matrix\"] = wandb.plot.confusion_matrix(\n",
    "             probs=None,\n",
    "             y_true=all_true_labels_eval,\n",
    "             preds=all_pred_labels_eval,\n",
    "             class_names=[\"Negative (0)\", \"Positive (1)\"]\n",
    "         )\n",
    "     except Exception as e:\n",
    "         print(f\"Could not log confusion matrix to wandb: {e}\")\n",
    "\n",
    "\n",
    "# Log all collected metrics to wandb (if initialized)\n",
    "if \"wandb\" in locals():\n",
    "    try:\n",
    "        wandb.log(wandb_logs)\n",
    "        print(\"Metrics logged to WandB.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not log metrics to wandb: {e}\")\n",
    "\n",
    "\n",
    "# --- 10. Save Submission File ---\n",
    "print(\"\\nSaving predictions for submission...\")\n",
    "# Ensure results directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Prepare submission dataframe\n",
    "submission_df = dev_df_eval[['id', 'predicted_label']]\n",
    "\n",
    "# Define CSV and ZIP paths\n",
    "csv_filename = \"predictions_task1.csv\"\n",
    "zip_filename = \"submission_task1.zip\"\n",
    "csv_path = os.path.join(OUTPUT_DIR, csv_filename)\n",
    "zip_path = os.path.join(OUTPUT_DIR, zip_filename)\n",
    "\n",
    "# Save CSV\n",
    "submission_df.to_csv(csv_path, index=False)\n",
    "print(f\"Predictions saved to {csv_path}\")\n",
    "\n",
    "# Zip the CSV file\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(csv_path, arcname=csv_filename)\n",
    "print(f\"{csv_filename} has been zipped into {zip_path}\")\n",
    "\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
