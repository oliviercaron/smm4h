Loading data...
Cleaning text data...
Data loaded. Train size: 31187, Dev size: 4624
Tokenizing data...
Tokenization complete.
Cleaning dataset columns...
Removing columns: ['text', 'file_name', 'origin', 'language', 'split', 'type']
Columns after cleaning and rename: ['id', 'labels', 'input_ids', 'attention_mask']
Computing class weights...
Class Weights (for CrossEntropy): tensor([0.5426, 6.3621], device='cuda:0')
Focal Loss Alpha (derived from class weights): tensor([0.5426, 6.3621], device='cuda:0')
Loading model...
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Model loaded with 2 labels.
Setting training arguments...
c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
C:\Users\Olivier\AppData\Local\Temp\ipykernel_32904\2125814163.py:293: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
Trainer configured.
Starting Training...
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
Loading data...
Cleaning text data...
Data loaded. Train size: 31187, Dev size: 4624
Tokenizing data...
Tokenization complete.
Cleaning dataset columns...
Removing columns: ['text', 'file_name', 'origin', 'language', 'split', 'type']
Columns after cleaning and rename: ['id', 'labels', 'input_ids', 'attention_mask']
Computing class weights...
Class Weights (for CrossEntropy): tensor([0.5426, 6.3621], device='cuda:0')
Focal Loss Alpha (derived from class weights): tensor([0.5426, 6.3621], device='cuda:0')
Loading model...
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Model loaded with 2 labels.
Setting training arguments...
c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
C:\Users\Olivier\AppData\Local\Temp\ipykernel_32904\1203867213.py:293: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
Trainer configured.
Starting Training...
