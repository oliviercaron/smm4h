Distribution des classes dans les données d'entraînement originales:
Classe positive: 153 (9.86%)
Classe négative: 1399 (90.14%)

Distribution des classes après rééquilibrage:
Classe positive: 699 (33.32%)
Classe négative: 1399 (66.68%)
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Distribution des classes dans les données d'entraînement originales:
Classe positive: 153 (9.86%)
Classe négative: 1399 (90.14%)

Distribution des classes après rééquilibrage:
Classe positive: 699 (33.32%)
Classe négative: 1399 (66.68%)
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\Olivier\AppData\Local\Temp\ipykernel_3416\935300630.py:168: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `PositiveF1Trainer.__init__`. Use `processing_class` instead.
  trainer = PositiveF1Trainer(
