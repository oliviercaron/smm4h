WandB initialized.
Loading data...
Applying text cleaning (config: APPLY_TEXT_CLEANING=True)...
Data loaded. Train size: 31187, Dev size: 4624
Train labels distribution:
label
0    0.92141
1    0.07859
Name: proportion, dtype: float64
Dev labels distribution:
label
0    0.913927
1    0.086073
Name: proportion, dtype: float64
Tokenizing data with max_length = 256...
c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
WandB initialized.
Loading data...
Applying text cleaning (config: APPLY_TEXT_CLEANING=True)...
Data loaded. Train size: 31187, Dev size: 4624
Train labels distribution:
label
0    0.92141
1    0.07859
Name: proportion, dtype: float64
Dev labels distribution:
label
0    0.913927
1    0.086073
Name: proportion, dtype: float64
Tokenizing data with max_length = 256...
c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Tokenization complete.
Cleaning dataset columns...
Removing columns: ['text', 'file_name', 'origin', 'language', 'split', 'type']
Columns after cleaning and rename: ['id', 'labels', 'input_ids', 'token_type_ids', 'attention_mask']
Setting up loss function...
Class Weights (for CrossEntropy): tensor([0.5426, 6.3621], device='cuda:0')
Loading model...
Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Model loaded with 2 labels.
Setting training arguments...
c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
C:\Users\Olivier\AppData\Local\Temp\ipykernel_29208\3826583536.py:381: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
Trainer configured.
Starting Training...
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
Training finished.

Evaluating on Development Set (Best Model loaded, using argmax)...
Evaluation Results (argmax):
{'eval_loss': 0.41951438784599304, 'eval_accuracy': 0.9398788927335641, 'eval_f1_pos': 0.6634382566585957, 'eval_precision_pos': 0.6401869158878505, 'eval_recall_pos': 0.6884422110552764, 'eval_f1_neg': 0.9669912134884825, 'eval_runtime': 47.0735, 'eval_samples_per_second': 98.229, 'eval_steps_per_second': 12.279, 'epoch': 5.999358727715788}

--- Performing Threshold Tuning on Validation Set ---
Optimizing threshold for Macro F1...
Best threshold found: 0.5169 (yielding Macro F1: 0.6653 on Dev)

--- Detailed Evaluation on Development Set (Using Optimal Threshold) ---
Final Threshold Applied: 0.5169

Metrics for language: DE
  Precision-de (Pos): 0.5476
  Recall-de    (Pos): 0.6571
  F1-de        (Pos): 0.5974
  Accuracy-de:        0.9511

Metrics for language: EN
  Precision-en (Pos): 0.6812
  Recall-en    (Pos): 0.7705
  F1-en        (Pos): 0.7231
  Accuracy-en:        0.9601

Metrics for language: FR
  Precision-fr (Pos): 0.5814
  Recall-fr    (Pos): 0.8333
  F1-fr        (Pos): 0.6849
  Accuracy-fr:        0.9450

Metrics for language: RU
  Precision-ru (Pos): 0.6533
  Recall-ru    (Pos): 0.6581
  F1-ru        (Pos): 0.6557
  Accuracy-ru:        0.9296

--- Overall Evaluation Summary (Final) ---
Macro F1-score across languages (Pos Class): 0.6653  <--- *** TARGET METRIC ***
F1-score across all languages (Positive Class): 0.6634
Overall Precision (Positive Class):             0.6402
Overall Recall (Positive Class):                0.6884
Overall Accuracy across all languages:          0.9399

Overall Confusion Matrix (All Languages - Final):
[[TN=4072  FP=154]
 [FN=124  TP=274]]
Final metrics logged to WandB.

Saving predictions for submission...
Predictions saved to results_mdeberta_256_tuned\predictions_task1_tuned.csv
predictions_task1_tuned.csv has been zipped into results_mdeberta_256_tuned\submission_task1_tuned.zip

Script finished.
