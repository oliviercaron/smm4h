Erreur avec le callback Wandb d'Optuna: module optuna.integration has no attribute WandbCallback. Lancement sans callback Wandb.
Trial 0: LR=9.37e-06, Accum=16, MaxLen=128, WD=0.031, Optim=adamw_torch
  -> Eval Steps: 487, Logging Steps: 97
Trial 0: Re-tokenisation avec max_length=128...
Chargement des données depuis data/train_data_SMM4H_2025_Task_1.csv et data/dev_data_SMM4H_2025_Task_1.csv...
Train shape: (31187, 8), Dev shape: (4625, 8)
Données chargées.
Tokenisation des données (max_length=128)...
Tokenisation terminée.
Nettoyage des colonnes inutiles...
Colonnes avant nettoyage: ['id', 'text', 'label', 'file_name', 'origin', 'type', 'language', 'split', 'input_ids', 'attention_mask']
Colonnes à supprimer: ['text', 'id', 'file_name', 'origin', 'language', 'split', 'type']
Colonnes après nettoyage: ['labels', 'input_ids', 'attention_mask']
Re-tokenisation terminée pour le trial.
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\Olivier\AppData\Local\Temp\ipykernel_24588\1816732936.py:163: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)

--- DÉBUT Trial 0 ---
ERREUR pendant l'entraînement du Trial 0: WeightedLossTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
