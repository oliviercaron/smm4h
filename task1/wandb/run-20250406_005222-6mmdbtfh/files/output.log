Loading data...
Loaded 31187 train and 4625 dev examples.
Data loaded.
Tokenizing data...
Using 6 processes for tokenization...
Tokenization complete.
Loading main tokenizer 'xlm-roberta-large' for the Trainer...
Cleaning dataset columns...
Columns after cleaning and rename: ['labels']
Computing class weights and determining label info...
Detected 2 distinct labels in training data: [0 1]
Model will be configured with num_labels=2
Class Weights (for classes [0, 1]): [0.5426468 6.3620973]

Setting up QLoRA configuration...
Loading large model 'xlm-roberta-large' with 4-bit quantization...
Initializing model with num_labels=2
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Large model loaded.
Preparing model for k-bit training...
Model prepared for k-bit training.

Inspecting classifier module parameters BEFORE manual casting:
  Parameter: dense.weight                   | dtype: torch.float32 | device: cuda:0 | requires_grad: False
  Parameter: dense.bias                     | dtype: torch.float32 | device: cuda:0 | requires_grad: False
  Parameter: out_proj.weight                | dtype: torch.float32 | device: cuda:0 | requires_grad: False
  Parameter: out_proj.bias                  | dtype: torch.float32 | device: cuda:0 | requires_grad: False

Manually casting classifier to float32...
Classifier casting attempted.

Inspecting classifier module parameters AFTER manual casting:
  Parameter: dense.weight                   | dtype: torch.float32 | device: cuda:0 | requires_grad: False
  Parameter: dense.bias                     | dtype: torch.float32 | device: cuda:0 | requires_grad: False
  Parameter: out_proj.weight                | dtype: torch.float32 | device: cuda:0 | requires_grad: False
  Parameter: out_proj.bias                  | dtype: torch.float32 | device: cuda:0 | requires_grad: False

Setting up LoRA configuration for XLM-R...

Applying LoRA adapter...
LoRA adapter applied successfully.
trainable params: 2,624,514 || all params: 562,516,996 || trainable%: 0.4666

Setting training arguments...
C:\Users\Olivier\AppData\Local\Temp\ipykernel_25148\315818492.py:361: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.
  trainer = WeightedTrainer(
No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Trainer configured.

Starting QLoRA Fine-tuning...

An error occurred during training: Caught ValueError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\data\_utils\worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
           ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\data\_utils\fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\data\data_collator.py", line 272, in __call__
    batch = pad_without_fast_tokenizer_warning(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\data\data_collator.py", line 67, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\tokenization_utils_base.py", line 3324, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['labels']
Traceback (most recent call last):
  File "C:\Users\Olivier\AppData\Local\Temp\ipykernel_25148\315818492.py", line 375, in <module>
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\trainer.py", line 2245, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\trainer.py", line 2514, in _inner_training_loop
    batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\trainer.py", line 5243, in get_batch_samples
    batch_samples.append(next(epoch_iterator))
                         ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\accelerate\data_loader.py", line 566, in __iter__
    current_batch = next(dataloader_iter)
                    ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\data\dataloader.py", line 708, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\data\dataloader.py", line 1480, in _next_data
    return self._process_data(data)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\data\dataloader.py", line 1505, in _process_data
    data.reraise()
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\_utils.py", line 733, in reraise
    raise exception
ValueError: Caught ValueError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\data\_utils\worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
           ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\data\_utils\fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\data\data_collator.py", line 272, in __call__
    batch = pad_without_fast_tokenizer_warning(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\data\data_collator.py", line 67, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\tokenization_utils_base.py", line 3324, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['labels']
