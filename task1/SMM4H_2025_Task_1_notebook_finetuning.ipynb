{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070f3c49",
   "metadata": {},
   "source": [
    "## XLM roberta large full finetuning BEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb40a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Olivier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: caron-olivier-80 (caron-olivier-80-universit-paris-dauphine-psl) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Olivier\\Documents\\GitHub\\ssmh4\\task1\\wandb\\run-20250407_090632-v1p69gu5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/caron-olivier-80-universit-paris-dauphine-psl/ade-classification-chatgptuamgneted_xlmr/runs/v1p69gu5' target=\"_blank\">pious-voice-4</a></strong> to <a href='https://wandb.ai/caron-olivier-80-universit-paris-dauphine-psl/ade-classification-chatgptuamgneted_xlmr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/caron-olivier-80-universit-paris-dauphine-psl/ade-classification-chatgptuamgneted_xlmr' target=\"_blank\">https://wandb.ai/caron-olivier-80-universit-paris-dauphine-psl/ade-classification-chatgptuamgneted_xlmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/caron-olivier-80-universit-paris-dauphine-psl/ade-classification-chatgptuamgneted_xlmr/runs/v1p69gu5' target=\"_blank\">https://wandb.ai/caron-olivier-80-universit-paris-dauphine-psl/ade-classification-chatgptuamgneted_xlmr/runs/v1p69gu5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "Tokenizing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baadd4d502f5419eb40df8be2eda16c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f38c0818bad4e5a91a4767390c7c2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n",
      "Cleaning dataset columns...\n",
      "Columns before removal: ['id', 'text', 'label', 'file_name', 'origin', 'type', 'language', 'split', 'input_ids', 'attention_mask']\n",
      "Removing columns: ['text', 'id', 'file_name', 'origin', 'language', 'split', 'type']\n",
      "Columns after cleaning and rename: ['labels', 'input_ids', 'attention_mask']\n",
      "Computing class weights...\n",
      "Class Weights: tensor([0.5426, 6.3621], device='cuda:0')\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Dropout increased: hidden_dropout=0.3, attention_dropout=0.3\n",
      "Setting training arguments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olivier\\AppData\\Local\\Temp\\ipykernel_20904\\1573148678.py:170: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer configured.\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5844' max='5844' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5844/5844 2:43:07, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Pos</th>\n",
       "      <th>Precision Pos</th>\n",
       "      <th>Recall Pos</th>\n",
       "      <th>F1 Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.237400</td>\n",
       "      <td>0.242307</td>\n",
       "      <td>0.946162</td>\n",
       "      <td>0.628912</td>\n",
       "      <td>0.772894</td>\n",
       "      <td>0.530151</td>\n",
       "      <td>0.970976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.196939</td>\n",
       "      <td>0.936216</td>\n",
       "      <td>0.674033</td>\n",
       "      <td>0.601578</td>\n",
       "      <td>0.766332</td>\n",
       "      <td>0.964649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.270457</td>\n",
       "      <td>0.936432</td>\n",
       "      <td>0.687898</td>\n",
       "      <td>0.595588</td>\n",
       "      <td>0.814070</td>\n",
       "      <td>0.964612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>0.277541</td>\n",
       "      <td>0.950270</td>\n",
       "      <td>0.718137</td>\n",
       "      <td>0.700957</td>\n",
       "      <td>0.736181</td>\n",
       "      <td>0.972729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.306390</td>\n",
       "      <td>0.953081</td>\n",
       "      <td>0.730435</td>\n",
       "      <td>0.722359</td>\n",
       "      <td>0.738693</td>\n",
       "      <td>0.974304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.305603</td>\n",
       "      <td>0.952865</td>\n",
       "      <td>0.731527</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.746231</td>\n",
       "      <td>0.974164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "\n",
      "Evaluating on Development Set (Best Model)...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "{'eval_loss': 0.3056026101112366, 'eval_accuracy': 0.9528648648648649, 'eval_f1_pos': 0.7315270935960592, 'eval_precision_pos': 0.717391304347826, 'eval_recall_pos': 0.7462311557788944, 'eval_f1_neg': 0.9741644939559138, 'eval_runtime': 47.3617, 'eval_samples_per_second': 97.653, 'eval_steps_per_second': 48.837, 'epoch': 5.999358727715788}\n",
      "\n",
      "Generating predictions and detailed metrics for Dev Set...\n",
      "\n",
      "Running threshold tuning on positive class...\n",
      "Baseline F1 (argmax): 0.7315\n",
      "Best threshold found: 0.6000 → Tuned F1: 0.7338\n",
      "Threshold saved to easy_xlmr_large_256\\threshold.txt\n",
      "\n",
      "--- Detailed Evaluation on Development Set ---\n",
      "\n",
      "Metrics for language: DE\n",
      "  Precision-de (Pos): 0.6053\n",
      "  Recall-de    (Pos): 0.6571\n",
      "  F1-de        (Pos): 0.6301\n",
      "  Accuracy-de:        0.9574\n",
      "\n",
      "Metrics for language: EN\n",
      "  Precision-en (Pos): 0.7727\n",
      "  Recall-en    (Pos): 0.8361\n",
      "  F1-en        (Pos): 0.8031\n",
      "  Accuracy-en:        0.9723\n",
      "\n",
      "Metrics for language: FR\n",
      "  Precision-fr (Pos): 0.7297\n",
      "  Recall-fr    (Pos): 0.9000\n",
      "  F1-fr        (Pos): 0.8060\n",
      "  Accuracy-fr:        0.9690\n",
      "\n",
      "Metrics for language: RU\n",
      "  Precision-ru (Pos): 0.7179\n",
      "  Recall-ru    (Pos): 0.7206\n",
      "  F1-ru        (Pos): 0.7193\n",
      "  Accuracy-ru:        0.9427\n",
      "\n",
      "--- Overall Evaluation Summary (Positive Class Focus) ---\n",
      "F1-score across all languages (Positive Class): 0.7315  <-- Primary Metric\n",
      "Macro F1-score across all languages (Pos Class):0.7396\n",
      "Overall Precision (Positive Class):             0.7174\n",
      "Overall Recall (Positive Class):                0.7462\n",
      "Overall Accuracy across all languages:          0.9529\n",
      "\n",
      "Overall Confusion Matrix (All Languages):\n",
      "[[TN=4110  FP=117]\n",
      " [FN=101  TP=297]]\n",
      "Metrics logged to WandB.\n",
      "\n",
      "Saving predictions for submission...\n",
      "Predictions saved to easy_xlmr_large_256\\predictions_task1.csv\n",
      "predictions_task1.csv has been zipped into easy_xlmr_large_256\\submission_task1.zip\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "import zipfile # Pour la sauvegarde finale\n",
    "import os # Pour la sauvegarde finale\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"xlm-roberta-large\"  \n",
    "TRAIN_CSV = \"data/train_data_SMM4H_2025_Task_1.csv\" \n",
    "DEV_CSV = \"data/dev_data_SMM4H_2025_Task_1.csv\"\n",
    "OUTPUT_DIR = \"easy_xlmr_large_256\"  \n",
    "NUM_EPOCHS = 6  \n",
    "BATCH_SIZE = 2  \n",
    "LEARNING_RATE = 1e-5  \n",
    "GRADIENT_ACCUMULATION_STEPS = 16  \n",
    "EARLY_STOPPING_PATIENCE = 1 \n",
    "MAX_LENGTH = 256 # I tried with 512 but it was not possible on my machine  \n",
    "WEIGHT_DECAY = 0.05\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_CSV).dropna(subset=['text'])\n",
    "    dev_df = pd.read_csv(DEV_CSV).dropna(subset=['text'])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading CSV files: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "dataset_dict = DatasetDict({'train': train_dataset, 'validation': dev_dataset})\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "# --- 2. Tokenization ---\n",
    "print(\"Tokenizing data...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# --- Modifications de la fonction de tokenization ---\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True  # Pour éviter de dépasser les limites du modèle\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# --- Clean up columns ---\n",
    "print(\"Cleaning dataset columns...\")\n",
    "print(\"Columns before removal:\", tokenized_datasets['train'].column_names)\n",
    "columns_to_remove = [\"text\", \"id\", \"file_name\", \"origin\", \"language\", \"split\", \"type\"]\n",
    "actual_columns_to_remove = [col for col in columns_to_remove if col in tokenized_datasets['train'].column_names]\n",
    "print(\"Removing columns:\", actual_columns_to_remove)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(actual_columns_to_remove)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Rename 'label' to 'labels'\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "print(\"Columns after cleaning and rename:\", tokenized_datasets['train'].column_names)\n",
    "\n",
    "# --- 3. Compute Class Weights ---\n",
    "print(\"Computing class weights...\")\n",
    "labels_train = train_df['label'].values\n",
    "if len(np.unique(labels_train)) > 1: # Ensure there are at least two classes\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels_train), y=labels_train)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Class Weights: {class_weights_tensor}\")\n",
    "else:\n",
    "    print(\"Warning: Only one class found in training data. Cannot compute class weights.\")\n",
    "    class_weights_tensor = None # Handle this case in the loss function if needed\n",
    "\n",
    "# --- Custom Trainer for Weighted Loss ---\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Use weights only if they were computed\n",
    "        if class_weights_tensor is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss() # Default unweighted loss\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --- 4. Model & Metrics ---\n",
    "print(\"Loading model...\")\n",
    "# --- Chargement du modèle avec optimisations ---\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# --- Increase dropout to help regularization ---\n",
    "model.config.hidden_dropout_prob = 0.3  # Dropout on fully connected layers\n",
    "model.config.attention_probs_dropout_prob = 0.3  # Dropout on attention probabilities\n",
    "print(\"Dropout increased: hidden_dropout=0.3, attention_dropout=0.3\")\n",
    "\n",
    "# Activer le gradient checkpointing pour économiser la mémoire\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Fonction compute_metrics qui utilise la fonction importée\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Utilisation de precision_recall_fscore_support\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None, labels=[0, 1], zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_pos': f1[1], # F1 for class 1\n",
    "        'precision_pos': precision[1],\n",
    "        'recall_pos': recall[1],\n",
    "        'f1_neg': f1[0], # F1 for class 0 (for info)\n",
    "    }\n",
    "\n",
    "# --- 5. Training Arguments ---\n",
    "print(\"Setting training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # Modifié (était BATCH_SIZE*2)\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    lr_scheduler_type=\"cosine\",  # <-- Ajouté\n",
    "    warmup_ratio=0.1,            # <-- Ajouté (10% des steps en warmup)\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_pos\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    report_to=\"wandb\" if \"wandb\" in locals() else \"none\",\n",
    "    fp16=False,  # Disabled FP16 to avoid gradient unscaling issues\n",
    "    bf16=True if torch.cuda.is_available() else False,  # Use BF16 instead if available\n",
    "    optim=\"adafactor\",  # Ajouté - Optimiseur plus léger\n",
    "    gradient_checkpointing=True,  # Ajouté - Économie de mémoire\n",
    "    #torch_compile=False,  # Ajouté - Acceleration PyTorch 2.x+\n",
    "    save_total_limit=2\n",
    "    # early_stopping_patience=EARLY_STOPPING_PATIENCE,  # Déjà géré par le callback\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# --- 6. Trainer ---\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,  # Utilisation du DataCollator\n",
    "    # Ajout du Callback pour Early Stopping\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)]\n",
    ")\n",
    "print(\"Trainer configured.\")\n",
    "\n",
    "# --- 7. Train ---\n",
    "print(\"Starting Training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")\n",
    "trainer.save_model(\"best_model_f1\") # Sauvegarde le meilleur modèle\n",
    "\n",
    "# Log training metrics\n",
    "# trainer.log_metrics(\"train\", train_result.metrics) # Décommenter si besoin\n",
    "# trainer.save_metrics(\"train\", train_result.metrics) # Décommenter si besoin\n",
    "# trainer.save_state() # Sauvegarde l'état du Trainer\n",
    "\n",
    "# --- 8. Evaluate on Dev Set (using the best model loaded) ---\n",
    "print(\"\\nEvaluating on Development Set (Best Model)...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\")\n",
    "print(eval_results)\n",
    "# trainer.log_metrics(\"eval\", eval_results) # Décommenter si besoin\n",
    "# trainer.save_metrics(\"eval\", eval_results) # Décommenter si besoin\n",
    "\n",
    "# --- 9. Detailed Evaluation and Submission File Generation ---\n",
    "print(\"\\nGenerating predictions and detailed metrics for Dev Set...\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predicted_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "# --- Threshold tuning ---\n",
    "print(\"\\nRunning threshold tuning on positive class...\")\n",
    "\n",
    "# Get predicted probabilities for class 1\n",
    "logits = torch.tensor(predictions.predictions)\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)[:, 1]  # Probabilities for class 1\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Baseline (argmax) F1\n",
    "baseline_f1 = f1_score(y_true, predicted_labels, pos_label=1)\n",
    "print(f\"Baseline F1 (argmax): {baseline_f1:.4f}\")\n",
    "\n",
    "# Search best threshold\n",
    "best_thresh = 0.5\n",
    "best_f1 = 0.0\n",
    "thresholds = np.linspace(0.1, 0.9, 81)\n",
    "\n",
    "for t in thresholds:\n",
    "    preds_thresh = (probs >= t).int()\n",
    "    f1 = f1_score(y_true, preds_thresh, pos_label=1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"Best threshold found: {best_thresh:.4f} → Tuned F1: {best_f1:.4f}\")\n",
    "\n",
    "# Save threshold to file\n",
    "threshold_path = os.path.join(OUTPUT_DIR, \"threshold.txt\")\n",
    "with open(threshold_path, \"w\") as f:\n",
    "    f.write(f\"{best_thresh:.4f}\")\n",
    "print(f\"Threshold saved to {threshold_path}\")\n",
    "\n",
    "# Add predictions to the dev dataframe (ensure index alignment if necessary)\n",
    "# If dev_df was filtered by dropna, indices might not match directly. Resetting index helps.\n",
    "dev_df_eval = dev_df.reset_index(drop=True)\n",
    "# Check lengths match before assigning\n",
    "if len(dev_df_eval) == len(predicted_labels):\n",
    "    dev_df_eval['predicted_label'] = predicted_labels\n",
    "else:\n",
    "    print(f\"Error: Length mismatch! Dev DF has {len(dev_df_eval)} rows, Predictions have {len(predicted_labels)} entries.\")\n",
    "    # Handle error appropriately, maybe skip detailed eval or investigate dropna impact\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Re-extract language (assuming ID format 'lang_...') - Be careful if IDs differ\n",
    "dev_df_eval[\"language\"] = dev_df_eval[\"id\"].apply(lambda x: str(x).split(\"_\")[0] if isinstance(x, str) and \"_\" in x else \"unknown\")\n",
    "\n",
    "# --- Calculate Per-Language and Overall Metrics ---\n",
    "languages = sorted(dev_df_eval['language'].unique())\n",
    "per_language_metrics = {}\n",
    "language_f1_scores = []\n",
    "all_true_labels_eval = []\n",
    "all_pred_labels_eval = []\n",
    "\n",
    "print(\"\\n--- Detailed Evaluation on Development Set ---\")\n",
    "wandb_logs = {} # Collect logs for wandb\n",
    "\n",
    "for lang in languages:\n",
    "    if lang == \"unknown\": continue # Skip if language couldn't be extracted\n",
    "    lang_mask = dev_df_eval['language'] == lang\n",
    "    y_true_lang = dev_df_eval.loc[lang_mask, 'label']\n",
    "    y_pred_lang = dev_df_eval.loc[lang_mask, 'predicted_label']\n",
    "\n",
    "    if len(y_true_lang) == 0: continue # Skip if no data for this language\n",
    "\n",
    "    all_true_labels_eval.extend(y_true_lang.tolist())\n",
    "    all_pred_labels_eval.extend(y_pred_lang.tolist())\n",
    "\n",
    "    # Utilisation de precision_recall_fscore_support (qui est maintenant importé)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_lang, y_pred_lang, average=None, labels=[0, 1], zero_division=0)\n",
    "    accuracy_lang = accuracy_score(y_true_lang, y_pred_lang) # Renommé pour éviter conflit avec la fonction accuracy_score\n",
    "\n",
    "    per_language_metrics[lang] = {'precision': precision[1], 'recall': recall[1], 'f1': f1[1], 'accuracy': accuracy_lang}\n",
    "    language_f1_scores.append(f1[1]) # On stocke le F1 de la classe positive (1)\n",
    "\n",
    "    print(f\"\\nMetrics for language: {lang.upper()}\")\n",
    "    print(f\"  Precision-{lang} (Pos): {precision[1]:.4f}\")\n",
    "    print(f\"  Recall-{lang}    (Pos): {recall[1]:.4f}\")\n",
    "    print(f\"  F1-{lang}        (Pos): {f1[1]:.4f}\")\n",
    "    print(f\"  Accuracy-{lang}:        {accuracy_lang:.4f}\")\n",
    "\n",
    "    # Prepare logs for wandb\n",
    "    wandb_logs[f\"{lang}/precision_pos\"] = precision[1]\n",
    "    wandb_logs[f\"{lang}/recall_pos\"] = recall[1]\n",
    "    wandb_logs[f\"{lang}/f1_pos\"] = f1[1]\n",
    "    wandb_logs[f\"{lang}/accuracy\"] = accuracy_lang\n",
    "\n",
    "\n",
    "# Calculate Overall Metrics (using the full dev set lists)\n",
    "cm_overall = confusion_matrix(all_true_labels_eval, all_pred_labels_eval, labels=[0, 1])\n",
    "tn, fp, fn, tp = cm_overall.ravel() if cm_overall.size == 4 else (0, 0, 0, 0) # Handle cases with missing classes\n",
    "\n",
    "overall_precision_pos = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "overall_recall_pos = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "# Overall F1 (Primary Metric for Positive Class)\n",
    "overall_f1_pos = 2 * (overall_precision_pos * overall_recall_pos) / (overall_precision_pos + overall_recall_pos) if (overall_precision_pos + overall_recall_pos) > 0 else 0\n",
    "# Macro F1 (Average of per-language F1s for Positive Class)\n",
    "macro_f1_pos = np.mean(language_f1_scores) if language_f1_scores else 0\n",
    "# Overall Accuracy\n",
    "overall_accuracy = accuracy_score(all_true_labels_eval, all_pred_labels_eval)\n",
    "\n",
    "print(\"\\n--- Overall Evaluation Summary (Positive Class Focus) ---\")\n",
    "print(f\"F1-score across all languages (Positive Class): {overall_f1_pos:.4f}  <-- Primary Metric\")\n",
    "print(f\"Macro F1-score across all languages (Pos Class):{macro_f1_pos:.4f}\")\n",
    "print(f\"Overall Precision (Positive Class):             {overall_precision_pos:.4f}\")\n",
    "print(f\"Overall Recall (Positive Class):                {overall_recall_pos:.4f}\")\n",
    "print(f\"Overall Accuracy across all languages:          {overall_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nOverall Confusion Matrix (All Languages):\")\n",
    "print(f\"[[TN={tn}  FP={fp}]\")\n",
    "print(f\" [FN={fn}  TP={tp}]]\")\n",
    "\n",
    "# Add overall metrics to wandb logs\n",
    "wandb_logs[\"overall/f1_pos\"] = overall_f1_pos\n",
    "wandb_logs[\"overall/macro_f1_pos\"] = macro_f1_pos\n",
    "wandb_logs[\"overall/precision_pos\"] = overall_precision_pos\n",
    "wandb_logs[\"overall/recall_pos\"] = overall_recall_pos\n",
    "wandb_logs[\"overall/accuracy\"] = overall_accuracy\n",
    "wandb_logs[\"overall/TP\"] = tp\n",
    "wandb_logs[\"overall/FP\"] = fp\n",
    "wandb_logs[\"overall/FN\"] = fn\n",
    "wandb_logs[\"overall/TN\"] = tn\n",
    "\n",
    "# Log Confusion Matrix to wandb (optional)\n",
    "if \"wandb\" in locals() and tp+fp+fn+tn > 0:\n",
    "     try:\n",
    "        wandb_logs[\"confusion_matrix\"] = wandb.plot.confusion_matrix(\n",
    "             probs=None,\n",
    "             y_true=all_true_labels_eval,\n",
    "             preds=all_pred_labels_eval,\n",
    "             class_names=[\"Negative (0)\", \"Positive (1)\"]\n",
    "         )\n",
    "     except Exception as e:\n",
    "         print(f\"Could not log confusion matrix to wandb: {e}\")\n",
    "\n",
    "\n",
    "# Log all collected metrics to wandb (if initialized)\n",
    "if \"wandb\" in locals():\n",
    "    try:\n",
    "        wandb.log(wandb_logs)\n",
    "        print(\"Metrics logged to WandB.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not log metrics to wandb: {e}\")\n",
    "\n",
    "\n",
    "# --- 10. Save Submission File ---\n",
    "print(\"\\nSaving predictions for submission...\")\n",
    "# Ensure results directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Prepare submission dataframe\n",
    "submission_df = dev_df_eval[['id', 'predicted_label']]\n",
    "\n",
    "# Define CSV and ZIP paths\n",
    "csv_filename = \"predictions_task1.csv\"\n",
    "zip_filename = \"submission_task1.zip\"\n",
    "csv_path = os.path.join(OUTPUT_DIR, csv_filename)\n",
    "zip_path = os.path.join(OUTPUT_DIR, zip_filename)\n",
    "\n",
    "# Save CSV\n",
    "submission_df.to_csv(csv_path, index=False)\n",
    "print(f\"Predictions saved to {csv_path}\")\n",
    "\n",
    "# Zip the CSV file\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(csv_path, arcname=csv_filename)\n",
    "print(f\"{csv_filename} has been zipped into {zip_path}\")\n",
    "\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
