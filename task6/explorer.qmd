---
title: "Untitled"
format: html
editor: visual
---

## Quarto

```{r}
# ---------------------------------------------------------
# Load required packages
# ---------------------------------------------------------
library(readr)       # For reading CSV
library(dplyr)       # For data manipulation
library(text2vec)    # For text processing (tokenization, tf-idf, etc.)
library(glmnet)      # For regularized logistic regression
library(yardstick)   # For F1 metric calculation
library(stringr)
library(tibble)      # For tibble data structures
library(ggthemr)

ggthemr("fresh")

theme_custom <- theme(
text = element_text(size = 12),
panel.grid = element_blank(),
panel.grid.major.y = element_line(colour = "#e3e1e1",
linetype = 2),
plot.title.position = 'plot',
legend.position = 'top',
legend.title = element_blank()
)
```

## Running Code

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}
# Charger les données
train_data <- read_csv("data/train.csv")
valid_data <- read_csv("data/valid.csv") 

# We'll rename columns for clarity if needed
colnames(train_data) <- c("id", "text", "label")
colnames(valid_data) <- c("id", "text", "label")

```

You can add options to executable code like this

```{r}

train_data <- train_data %>%
  mutate(nb_words = str_count(text, "\\w+"))

valid_data <- valid_data %>%
  mutate(nb_words = str_count(text, "\\w+"))

# Check distribution
table(train_data$label)
table(valid_data$label)

ggplot(train_data, aes(x = nb_words, fill = factor(label))) +
  geom_histogram(binwidth = 5, alpha = 0.7, position = "identity") +
  labs(title = "Distribution of Word Count by Label (Train Data)",
       x = "Number of Words",
       y = "Frequency",
       fill = "Label") +
  theme_custom

ggplot(valid_data, aes(x = nb_words, fill = factor(label))) +
  geom_histogram(binwidth = 5, alpha = 0.7, position = "identity") +
  labs(title = "Distribution of Word Count by Label (Valid Data)",
       x = "Number of Words",
       y = "Frequency",
       fill = "Label") +
  theme_custom
```

## Tfidf + regression logistique

The `echo: false` option disables the printing of code (only output is displayed).

```{r}
# ---------------------------------------------------------
# 3) Create an iterator for text tokenization
# ---------------------------------------------------------
# Define a simple tokenizer function
tokenize_fun <- function(texts) {
  # text2vec's word_tokenizer is decent; you can also do your own custom
  text2vec::word_tokenizer(tolower(texts))
}
```

Build vocabulary & Document-Term Matrices (DTM)

```{r}
# We'll use the train set to build the vocabulary, then apply to valid.

# Create an iterator over training text
train_it <- itoken(train_data$text, 
                   tokenizer = tokenize_fun, 
                   progressbar = FALSE)

# Build the vocabulary
vocab <- create_vocabulary(train_it, ngram = c(1L, 1L)) %>%
  prune_vocabulary(term_count_min = 5)  # keep terms appearing >=5 times

# Vectorizer (turn tokens into DTM)
vectorizer <- vocab_vectorizer(vocab)

# Create the DTM for train
train_dtm <- create_dtm(train_it, vectorizer)

# Create the DTM for valid
valid_it <- itoken(valid_data$text,
                   tokenizer = tokenize_fun,
                   progressbar = FALSE)
valid_dtm <- create_dtm(valid_it, vectorizer)
```

Train a logistic regression model with glmnet

```{r}
# glmnet expects a matrix (sparse is fine) and a numeric outcome (0/1)
train_label <- train_data$label

# We'll do a simple cross-validated glmnet
# family = "binomial" for logistic regression
# alpha=1 => L1 penalty (lasso), alpha=0 => ridge, or something in-between
cv_model <- cv.glmnet(
  x = train_dtm,
  y = train_label,
  family = "binomial",
  alpha = 0.5,         # elastic net, can tune if desired
  type.measure = "deviance",  # deviance is default for logistic
  nfolds = 5,          # 5-fold CV
  parallel = FALSE
)

# Get the best lambda from cross-validation
best_lambda <- cv_model$lambda.min
cat("Best lambda from CV:", best_lambda, "\n")
```

Evaluate on validation set

```{r}
valid_probs <- predict(cv_model, newx = valid_dtm, s = "lambda.min", type = "response")
# valid_probs is the probability of label=1

# By default, threshold = 0.5
valid_preds <- ifelse(valid_probs >= 0.5, 1, 0)

# We'll compute F1 for the positive class
# yardstick requires data frames with specific columns
results_df <- tibble(
  truth = factor(valid_data$label, levels = c(0,1)), 
  estimate = factor(valid_preds, levels = c(0,1))
)

# yardstick's f_meas() calculates F1; we specify positive="1"
f1_result <- f_meas(data = results_df, truth = truth, estimate = estimate, event_level = "second")
cat("F1-score (positive class) with threshold=0.5:", f1_result$.estimate, "\n")

```

Optional: Threshold tuning for best F1

```{r}
# Define thresholds and initialize best F1 and best threshold
thresholds <- seq(0, 1, by = 0.01)
best_f1 <- 0
best_thresh <- 0

# Loop over each threshold value
for (th in thresholds) {
  # Compute predictions based on the current threshold
  temp_pred <- ifelse(valid_probs >= th, 1, 0)
  
  # Create a tibble with truth and predicted estimates
  temp_df <- tibble(
    truth = factor(valid_data$label, levels = c(0,1)),
    estimate = factor(temp_pred, levels = c(0,1))
  )
  
  # Calculate the F1-score for the positive class
  this_f1 <- f_meas(data = temp_df, truth = truth, estimate = estimate, event_level = "second")$.estimate
  
  # If the F1-score is NA (e.g., no predicted positive events), set it to 0
  if (is.na(this_f1)) this_f1 <- 0
  
  # Update best F1 and threshold if current F1 is higher
  if (this_f1 > best_f1) {
    best_f1 <- this_f1
    best_thresh <- th
  }
}

cat("Best threshold =", best_thresh, "with F1 =", best_f1, "\n")
```

Final predictions on validation with the best threshold

```{r}
final_preds <- ifelse(valid_probs >= best_thresh, 1, 0)
final_df <- tibble(
  truth = factor(valid_data$label, levels = c(0,1)),
  estimate = factor(final_preds, levels = c(0,1))
)
final_f1 <- f_meas(data = final_df, truth = truth, estimate = estimate, event_level = "second")$.estimate
cat("Final F1 with tuned threshold =", final_f1, "\n")
```

Calculer le nombre de mauvaises classifications dans chaque classe

```{r}
# Compute final predictions using the best threshold
final_preds <- ifelse(valid_probs >= best_thresh, 1, 0)

# Merge predictions with the original validation data
valid_data_pred <- valid_data %>%
  mutate(pred = final_preds)

# Identify misclassified examples: where prediction != label
misclassified <- valid_data_pred %>% 
  filter(pred != label)

# Print the misclassified examples
print(misclassified)

# Optionally, separate false positives and false negatives:
false_positives <- misclassified %>% filter(label == 0)
false_negatives <- misclassified %>% filter(label == 1)

cat("Nombre de faux positifs :", nrow(false_positives), "\n")
cat("Nombre de faux négatifs :", nrow(false_negatives), "\n")
```

Voir les textes où la classification a échoué

```{r}
# Compute final predictions using the best threshold
final_preds <- ifelse(valid_probs >= best_thresh, 1, 0)

# Merge predictions with the original validation data
valid_data_pred <- valid_data %>%
  mutate(pred = final_preds)

# Identify misclassified examples: where prediction != label
misclassified <- valid_data_pred %>% 
  filter(pred != label)

# Select and display only the id, text, true label and predicted label
misclassified_texts <- misclassified %>% select(id, text, label, pred)
print(misclassified_texts)

```

## Modele xgboost sophistiqué

```{r}
# ---------------------------------------------
# Enhanced NLP pipeline in R using xgboost
# ---------------------------------------------
library(readr)
library(dplyr)
library(text2vec)
library(xgboost)
library(yardstick)
library(Matrix)
library(tm)

# 1) Read your datasets
train_data <- read_csv("data/train.csv")
valid_data <- read_csv("data/valid.csv")
colnames(train_data) <- c("id", "text", "label")
colnames(valid_data) <- c("id", "text", "label")

# 2) Text preprocessing and vectorization
prep_fun <- tolower
tok_fun <- word_tokenizer

it_train <- itoken(train_data$text, 
                   preprocessor = prep_fun, tokenizer = tok_fun, progressbar = FALSE)
it_valid <- itoken(valid_data$text, 
                   preprocessor = prep_fun, tokenizer = tok_fun, progressbar = FALSE)

vocab <- create_vocabulary(it_train, ngram = c(1L,3L), stopwords = stopwords("en")) %>%
  prune_vocabulary(term_count_min = 5)

vectorizer <- vocab_vectorizer(vocab)

# Use TF-IDF
tfidf_transformer <- TfIdf$new()
dtm_train <- create_dtm(it_train, vectorizer) %>% tfidf_transformer$fit_transform()
dtm_valid <- create_dtm(it_valid, vectorizer) %>% tfidf_transformer$transform()

# 3) Convert data into xgboost matrices
dtrain <- xgb.DMatrix(data = dtm_train, label = train_data$label)
dvalid <- xgb.DMatrix(data = dtm_valid, label = valid_data$label)

# 4) Train xgboost model
params <- list(
  objective = "binary:logistic",
  eval_metric = "logloss",
  max_depth = 6,
  eta = 0.1
)

model_xgb <- xgb.train(
  params = params,
  data = dtrain,
  verbose = 1,
  nrounds = 100,
  watchlist = list(train = dtrain, valid = dvalid),
  early_stopping_rounds = 10,
  print_every_n = 1
)

# 5) Predict probabilities on validation set
valid_probs <- predict(model_xgb, dvalid)

# 6) Find best threshold based on F1-score
thresholds <- seq(0, 1, by = 0.01)
best_f1 <- 0
best_thresh <- 0

for (th in thresholds) {
  preds <- ifelse(valid_probs >= th, 1, 0)
  temp_df <- tibble(
    truth = factor(valid_data$label, levels=c(0,1)),
    estimate = factor(preds, levels=c(0,1))
  )
  
  current_f1 <- f_meas(temp_df, truth, estimate, event_level = "second")$.estimate
  if(is.na(current_f1)) current_f1 <- 0
  
  if(current_f1 > best_f1){
    best_f1 <- current_f1
    best_thresh <- th
  }
}

cat("Best Threshold:", best_thresh, "F1-score:", best_f1, "\n")

# 7) Final predictions with best threshold
final_preds <- ifelse(valid_probs >= best_thresh, 1, 0)

# Identify misclassifications
misclassified <- valid_data %>%
  mutate(pred = final_preds) %>%
  filter(pred != label)

# Show examples of errors
misclassified %>% select(id, text, label, pred) %>% head(10) %>% print()
```

Y a-t-il des lignes vides ?

```{r}
# Vérifier lignes contenant au moins 1 NA ou chaîne vide ("")
problems <- train_data %>% filter(if_any(everything(), ~ is.na(.) | . == ""))

# Affichage
#print(problems)

# Vérification rapide du nombre de lignes concernées
cat("Nombre total de lignes problématiques dans train.csv:", nrow(problems), "\n")

# Vérifier lignes contenant au moins 1 NA ou chaîne vide ("")
problems <- valid_data %>% filter(if_any(everything(), ~ is.na(.) | . == ""))

# Affichage
#print(problems)

# Vérification rapide du nombre de lignes concernées
cat("Nombre total de lignes problématiques dans valid.csv:", nrow(problems), "\n")
```

## 5-Fold cross-validation tf-idf + XGBoost

```{r}
library(readr)
library(dplyr)
library(text2vec)
library(xgboost)
library(Matrix)
library(yardstick)
library(caret)   # pour createFolds
library(tm)

# Charger les données d'entraînement uniquement
train_data <- read_csv("data/train.csv")
colnames(train_data) <- c("id", "text", "label")

# Initialiser tokenizer
prep_fun <- tolower
tok_fun <- word_tokenizer

# Créer les folds stratifiés (par label)
set.seed(42)
folds <- createFolds(train_data$label, k = 5, list = TRUE, returnTrain = FALSE)

# Stocker les F1 scores pour chaque fold
f1_scores <- c()

# Boucle sur chaque fold
for (i in seq_along(folds)) {
  cat("\n🔁 Fold", i, "\n")
  
  # Séparer données de validation et d'entraînement pour ce fold
  valid_idx <- folds[[i]]
  train_fold <- train_data[-valid_idx, ]
  valid_fold <- train_data[valid_idx, ]
  
  # Créer les itérateurs text2vec
  it_train <- itoken(train_fold$text, preprocessor = prep_fun, tokenizer = tok_fun, progressbar = FALSE)
  it_valid <- itoken(valid_fold$text, preprocessor = prep_fun, tokenizer = tok_fun, progressbar = FALSE)
  
  # Créer vocabulaire (tu peux changer ngram = c(1L, 3L) si tu veux plus de contexte)
  vocab <- create_vocabulary(it_train, ngram = c(1L, 3L), stopwords = stopwords("en")) %>%
    prune_vocabulary(term_count_min = 5)
  
  vectorizer <- vocab_vectorizer(vocab)
  tfidf <- TfIdf$new()
  
  dtm_train <- create_dtm(it_train, vectorizer) %>% tfidf$fit_transform()
  dtm_valid <- create_dtm(it_valid, vectorizer) %>% tfidf$transform()
  
  dtrain <- xgb.DMatrix(data = dtm_train, label = train_fold$label)
  dvalid <- xgb.DMatrix(data = dtm_valid, label = valid_fold$label)
  
  # Paramètres XGBoost
  params <- list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    max_depth = 6,
    eta = 0.1
  )
  
  model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = 100,
    watchlist = list(valid = dvalid),
    early_stopping_rounds = 10,
    verbose = 0
  )
  
  # Prédictions
  valid_probs <- predict(model, dvalid)
  
  # Trouver le meilleur seuil pour F1 positif
  thresholds <- seq(0, 1, by = 0.01)
  best_f1 <- 0
  best_thresh <- 0
  
  for (th in thresholds) {
    preds <- ifelse(valid_probs >= th, 1, 0)
    temp_df <- tibble(
      truth = factor(valid_fold$label, levels = c(0,1)),
      estimate = factor(preds, levels = c(0,1))
    )
    f1 <- f_meas(temp_df, truth, estimate, event_level = "second")$.estimate
    if (is.na(f1)) f1 <- 0
    if (f1 > best_f1) {
      best_f1 <- f1
      best_thresh <- th
    }
  }
  
  cat("✅ Fold", i, "- Best F1:", round(best_f1, 4), " @ threshold =", best_thresh, "\n")
  f1_scores <- c(f1_scores, best_f1)
}

# Résumé
cat("\n📊 Moyenne F1 sur 5 folds :", round(mean(f1_scores), 4), "\n")
cat("📉 Écart-type :", round(sd(f1_scores), 4), "\n")

```

## Modèle BERT (2nd MODEL)

```{python}
import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt

# -------------------------------
# 1. Charger et préparer les données
# -------------------------------
train_df = pd.read_csv("data/train.csv")
valid_df = pd.read_csv("data/valid.csv")

# Renommer la colonne "labels" en "label"
train_df = train_df.rename(columns={"labels": "label"})
valid_df = valid_df.rename(columns={"labels": "label"})

# -------------------------------
# 2. Définir un Dataset personnalisé
# -------------------------------
class VaccineDataset(torch.utils.data.Dataset):
    def __init__(self, dataframe, tokenizer, max_length=256):
        self.texts = dataframe['text'].tolist()
        self.labels = dataframe['label'].tolist()
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        # On enlève la dimension du batch
        item = {key: encoding[key].squeeze(0) for key in encoding}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

# -------------------------------
# 3. Charger le tokenizer et le modèle
# -------------------------------
model_name = "roberta-base"  # Tu peux utiliser "bert-base-uncased" si souhaité
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Créer les datasets
train_dataset = VaccineDataset(train_df, tokenizer)
valid_dataset = VaccineDataset(valid_df, tokenizer)

# -------------------------------
# 4. Définir la fonction de calcul des métriques
# -------------------------------
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    f1 = f1_score(labels, preds, pos_label=1)
    return {"f1": f1}

# -------------------------------
# 5. Définir les arguments d'entraînement
# -------------------------------
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    logging_steps=50,
    seed=42,
    report_to=[]  # Désactive WandB et autres logs externes
)

# -------------------------------
# 6. Créer le Trainer et entraîner le modèle
# -------------------------------
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

# -------------------------------
# 7. Évaluation et tuning du seuil optimal
# -------------------------------
predictions = trainer.predict(valid_dataset)
logits = predictions.predictions
labels = predictions.label_ids

# -------------------------------
# 8. Courbes d'apprentissage : loss et F1
# -------------------------------
# Extract logs (Trainer stocke tout l'historique ici)
train_logs = [log for log in trainer.state.log_history if "loss" in log and "eval_loss" not in log]
eval_logs = [log for log in trainer.state.log_history if "eval_loss" in log]

# Epochs
train_epochs = [log["epoch"] for log in train_logs]
train_loss = [log["loss"] for log in train_logs]

eval_epochs = [log["epoch"] for log in eval_logs]
eval_loss = [log["eval_loss"] for log in eval_logs]
eval_f1 = [log["eval_f1"] for log in eval_logs]


# Convertir les logits en probabilités (pour la classe positive)
probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()

best_f1 = 0
best_thresh = 0
for thresh in np.arange(0, 1.01, 0.01):
    preds = (probs >= thresh).astype(int)
    current_f1 = f1_score(labels, preds, pos_label=1)
    if current_f1 > best_f1:
        best_f1 = current_f1
        best_thresh = thresh

print(f"\nBERT model - Best F1: {best_f1:.4f} at threshold = {best_thresh:.2f}")
```

### graphiques overfiting

```{python}
from sklearn.metrics import confusion_matrix
import seaborn as sns
# -------------------------------
# Plot: Training vs Validation Loss
# -------------------------------
plt.figure(figsize=(10, 4))
plt.plot(train_epochs, train_loss, marker='o', label="Training Loss")
plt.plot(eval_epochs, eval_loss, marker='o', label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss over Epochs")
plt.legend()
plt.grid(True)
plt.show()

# -------------------------------
# Plot: Validation F1 Score
# -------------------------------
plt.figure(figsize=(10, 4))
plt.plot(eval_epochs, eval_f1, marker='o', color='green', label="Validation F1")
plt.xlabel("Epoch")
plt.ylabel("F1 Score")
plt.title("Validation F1 over Epochs")
plt.legend()
plt.grid(True)
plt.show()

# Génère les prédictions sur le set de test (ou valid selon ton usage ici)
test_preds_proba = test_predictions.predictions
test_preds = np.argmax(test_preds_proba, axis=1)
test_labels = test_predictions.label_ids

# Matrice de confusion
cm = confusion_matrix(test_labels, test_preds)
labels = ["Class 0", "Class 1"]

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix (Best Threshold = 0.99)")
plt.tight_layout()
plt.show()

```

### voiir les textes en erreur :

```{python}
preds = (probs >= best_thresh).astype(int)
valid_df["prob"] = probs
valid_df["pred"] = preds
errors_df = valid_df[valid_df["pred"] != valid_df["label"]]
# Voir un aperçu des erreurs
print(errors_df[["text", "label", "pred", "prob"]].sample(10))
# Faux négatifs : label = 1, mais prédiction = 0
false_negatives = errors_df[(errors_df["label"] == 1) & (errors_df["pred"] == 0)]
print(false_negatives.sort_values("prob", ascending=False)[["text", "label", "pred", "prob"]].head(10))

# Faux positifs : label = 0, mais prédiction = 1
false_positives = errors_df[(errors_df["label"] == 0) & (errors_df["pred"] == 1)]
print(false_positives.sort_values("prob", ascending=True)[["text", "label", "pred", "prob"]].head(10))

errors_df.to_csv("bert_errors_24-03-2025.csv", index=False)

```

### on enregistre le modèle

```{python}
# 📦 Créer le dossier s'il n'existe pas
import os
os.makedirs("models", exist_ok=True)

# 💾 Sauvegarder le modèle et le tokenizer
model.save_pretrained("models/")
tokenizer.save_pretrained("models/")

print("✅ Modèle et tokenizer enregistrés dans le dossier 'models/'")
```

## meilleur modèle

```{python}
import pandas as pd
import numpy as np
import torch
import random
from sklearn.model_selection import train_test_split
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                          Trainer, TrainingArguments, EarlyStoppingCallback)
from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# -------------------------------
# 1. Load your two files
# -------------------------------
# train.csv = big dataset (2521 examples)
# valid.csv = final test set (786 examples) => We'll rename it test_df in code

train_full_df = pd.read_csv("data/train.csv")   # 2521 examples
test_df = pd.read_csv("data/valid.csv")         # 786 examples (becomes our sealed test)

# Rename "labels" to "label" for consistency
train_full_df = train_full_df.rename(columns={"labels": "label"})
test_df = test_df.rename(columns={"labels": "label"})

# -------------------------------
# 2. Split train.csv into train + valid
# -------------------------------
# We take 80% for actual training, 20% for validation
from sklearn.model_selection import train_test_split

train_df, valid_df = train_test_split(
    train_full_df,
    test_size=0.2,  
    stratify=train_full_df["label"],  # keep class distribution
    random_state=42
)

print("New train_df distribution:\n", train_df["label"].value_counts())
print("\nNew valid_df distribution:\n", valid_df["label"].value_counts())
print("\nTest set distribution (old valid.csv):\n", test_df["label"].value_counts())

# -------------------------------
# 3. Create a custom Dataset
# -------------------------------
class VaccineDataset(torch.utils.data.Dataset):
    def __init__(self, dataframe, tokenizer, max_length=512):
        self.texts = dataframe['text'].tolist()
        self.labels = dataframe['label'].tolist()
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        # Remove the batch dimension
        item = {key: val.squeeze(0) for key, val in encoding.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

# -------------------------------
# 4. Load tokenizer and model
# -------------------------------
model_name = "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Choose device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
print(f"✅ Using device: {device}")

# Create Datasets
train_dataset = VaccineDataset(train_df, tokenizer)
valid_dataset = VaccineDataset(valid_df, tokenizer)
test_dataset = VaccineDataset(test_df, tokenizer)

# -------------------------------
# 5. Define the compute_metrics function
# -------------------------------
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    f1 = f1_score(labels, preds, pos_label=1)
    return {"f1": f1}

# -------------------------------
# 6. Define TrainingArguments with EarlyStopping
# -------------------------------
from transformers import TrainingArguments, EarlyStoppingCallback

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",   # Evaluate on valid_dataset every epoch
    save_strategy="epoch",
    num_train_epochs=10,           # up to 10 epochs (EarlyStopping can stop earlier)
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    logging_steps=50,
    seed=42,
    report_to=[]  # no external logging
)

# -------------------------------
# 7. Create Trainer with EarlyStopping callback
# -------------------------------
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,  # used for early stopping and best model selection
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # stops if no improvement after 2 epochs
)

# Train
trainer.train()

# -------------------------------
# 7b. Tuning du meilleur threshold sur le valid_dataset
# -------------------------------
# Prédictions sur valid_df pour threshold tuning
valid_predictions = trainer.predict(valid_dataset)
valid_logits = valid_predictions.predictions
valid_labels = valid_predictions.label_ids

# Probabilités pour la classe positive
valid_probs = torch.softmax(torch.tensor(valid_logits), dim=1)[:, 1].numpy()

# Recherche du meilleur seuil
best_f1 = 0
best_thresh = 0.5  # Par défaut
for thresh in np.arange(0, 1.01, 0.01):
    preds = (valid_probs >= thresh).astype(int)
    score = f1_score(valid_labels, preds, pos_label=1)
    if score > best_f1:
        best_f1 = score
        best_thresh = thresh

print(f"\n✅ Best threshold found on validation set: {best_thresh:.2f} (F1 = {best_f1:.4f})")

# -------------------------------
# 8. Evaluate on the final test set (old valid.csv)
# -------------------------------
test_predictions = trainer.predict(test_dataset)
test_logits = test_predictions.predictions
test_labels = test_predictions.label_ids

test_preds = np.argmax(test_logits, axis=-1)
test_f1 = f1_score(test_labels, test_preds, pos_label=1)
print(f"\nFinal Test F1 on old valid.csv = {test_f1:.4f}")

# -------------------------------
# 8b. Confusion Matrix on Test
# -------------------------------
cm = confusion_matrix(test_labels, test_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap="Blues")
plt.title("Test Confusion Matrix")
plt.show()

# -------------------------------
# 8c. Classification Report (precision, recall, F1 per class)
# -------------------------------
report = classification_report(test_labels, test_preds, target_names=["Class 0", "Class 1"])
print("Classification Report:\n", report)

# -------------------------------
# 8d. Save logs to CSV
# -------------------------------
logs_df = pd.DataFrame(trainer.state.log_history)
logs_df.to_csv("training_log.csv", index=False)
print("\nTraining logs saved to training_log.csv")

# -------------------------------
# 9. Plot learning curves to check for overfitting
# -------------------------------
# Extract logs
train_logs = [log for log in trainer.state.log_history if "loss" in log and "eval_loss" not in log]
eval_logs = [log for log in trainer.state.log_history if "eval_loss" in log]

# Build lists of epochs, train loss, eval loss, eval f1
train_epochs = [log["epoch"] for log in train_logs]
train_loss = [log["loss"] for log in train_logs]

eval_epochs = [log["epoch"] for log in eval_logs]
eval_loss = [log["eval_loss"] for log in eval_logs]
eval_f1 = [log["eval_f1"] for log in eval_logs]

# Plot: Training vs Validation Loss
plt.figure(figsize=(8, 4))
plt.plot(train_epochs, train_loss, marker='o', label="Training Loss")
plt.plot(eval_epochs, eval_loss, marker='o', label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss over Epochs")
plt.legend()
plt.grid(True)
plt.show()

# Plot: Validation F1
plt.figure(figsize=(8, 4))
plt.plot(eval_epochs, eval_f1, marker='o', color='green', label="Validation F1")
plt.xlabel("Epoch")
plt.ylabel("F1 Score")
plt.title("Validation F1 over Epochs")
plt.legend()
plt.grid(True)
plt.show()
```

## Biomedroberta
