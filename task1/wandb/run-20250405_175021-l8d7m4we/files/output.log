Loading data...
Loaded 31187 train and 4625 dev examples.
Data loaded.
Tokenizing data...
Using 6 processes for tokenization...
Tokenization complete.
Loading tokenizer 'xlm-roberta-large' for the Trainer...
Cleaning dataset columns...
Columns after cleaning and rename: ['labels']
Computing class weights...
Detected 2 distinct labels in training data: [0 1]
Class Weights (for classes [0, 1]): [0.5426468 6.3620973]
Setting up QLoRA configuration...
Loading large model 'xlm-roberta-large' with 4-bit quantization...
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Large model loaded.
Model prepared for k-bit training.
Setting up LoRA configuration for XLM-R...
Applying LoRA adapter...
Error applying LoRA: Target modules {'q_proj', 'v_proj'} not found in the base model. Please check the target modules and try again.
Please double-check LoraConfig target_modules by inspecting the model structure (print(model)).
Setting training arguments...
c:\Users\Olivier\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
C:\Users\Olivier\AppData\Local\Temp\ipykernel_33604\1966703968.py:250: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.
  trainer = WeightedTrainer(
